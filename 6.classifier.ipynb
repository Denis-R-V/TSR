{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Denis-R-V/TSR/blob/main/sign_detector_augmentated_v1_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ8-zW_yJ279"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Denis-R-V/TSR/blob/main/sign_detector.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xtJ8o7tJ27-"
      },
      "source": [
        "# Система распознавания дорожных знаков на датасете RTSD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "colab = False\n",
        "\n",
        "if colab == True:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    !pip install kaggle\n",
        "    !mkdir ~/.kaggle\n",
        "    !cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json\n",
        "    !kaggle datasets download watchman/rtsd-dataset\n",
        "    !unzip rtsd-dataset.zip\n",
        "    !rm rtsd-dataset.zip\n",
        "    !cp -r rtsd-frames/rtsd-frames/ .\n",
        "    !rm -r rtsd-frames/rtsd-frames/\n",
        "    !pip install fiftyone\n",
        "if colab == True:\n",
        "    dataset_path = '.'\n",
        "    checkpoints_path = '../content/drive/MyDrive/TSR/checkpoints'\n",
        "else:\n",
        "    dataset_path = 'data'\n",
        "    checkpoints_path = 'checkpoints'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "#import matplotlib.patches as patches\n",
        "#%matplotlib inline\n",
        "\n",
        "#from pycocotools.coco import COCO\n",
        "#import fiftyone as fo\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "#import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "#from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "#from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models import resnet152\n",
        "#import cv2\n",
        "#PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k69EoY9zJ28Y"
      },
      "source": [
        "## Обучение модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRRoJ7Q4J28Z"
      },
      "source": [
        "### Загрузчик данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RTSD_dataset_classifier(Dataset):\n",
        "    def __init__(self, json_path, img_path, transforms):\n",
        "        self.json_path = json_path\n",
        "        self.img_path = img_path\n",
        "        self.transforms = transforms\n",
        "        \n",
        "        with open(json_path, 'r') as read_file:\n",
        "            self.anno = json.load(read_file)\n",
        "        read_file.close()\n",
        "\n",
        "        self.df_anno = pd.DataFrame(self.anno.get('annotations'))[['image_id','bbox','category_id']]\n",
        "        self.df_images = pd.DataFrame(self.anno.get('images'))[['id','file_name']]\n",
        "        self.df_dataset = self.df_anno.merge(self.df_images, left_on='image_id', right_on='id',)[['file_name','bbox','category_id']]\n",
        "        #self.labels = torch.eye(156)[self.df_dataset['category_id']]\n",
        "    def __len__(self):\n",
        "        return self.df_dataset.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_name = self.df_dataset.loc[index,'file_name']\n",
        "        bbox = self.df_dataset.loc[index,'bbox']\n",
        "        bbox = [bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]]\n",
        "        img = Image.open(os.path.join(self.img_path, img_name))\n",
        "        img = img.crop(bbox)\n",
        "        img = self.transforms(img)\n",
        "        label = torch.tensor(self.df_dataset.loc[index,'category_id'])\n",
        "        \n",
        "\n",
        "        #from PIL import ImageOps\n",
        "        #old_img = Image.open(image_path)\n",
        "        # создание нового изображения с белым фоном\n",
        "        #new_image = ImageOps.expand(old_img, border=25, fill=(255,255,255))\n",
        "\n",
        "\n",
        "\n",
        "        return {'images':img, 'labels':label}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_transform(train):\n",
        "    if train == True:\n",
        "        return transforms.Compose([transforms.Resize((224,224)),\n",
        "                                   transforms.ToTensor(),\n",
        "                                   transforms.RandomRotation(15),\n",
        "                                   transforms.Normalize([0.485, 0.456, 0.406],      # 1 496\n",
        "                                                        [0.229, 0.224, 0.225])\n",
        "                                   ])\n",
        "    else:\n",
        "        return transforms.Compose([transforms.Resize((224,224)),\n",
        "                                   transforms.ToTensor(),\n",
        "                                   transforms.Normalize([0.485, 0.456, 0.406],      # 1 496\n",
        "                                                        [0.229, 0.224, 0.225])\n",
        "                                   ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "test = RTSD_dataset_classifier(json_path = os.path.join(dataset_path, 'train_anno_reduced.json'),\n",
        "                               img_path = dataset_path,\n",
        "                               transforms = get_transform(train=True)\n",
        "                               )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'images': tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "          ...,\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
              " \n",
              "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "          ...,\n",
              "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
              " \n",
              "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "          ...,\n",
              "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]),\n",
              " 'labels': tensor(155)}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test.__getitem__(2324)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "72"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def plot_sign(dataset, index):\n",
        "    item = dataset.__getitem__(index)\n",
        "    img = item['images']\n",
        "    target = item['labels']\n",
        "    #img, target = test.__getitem__(index)\n",
        "    img = img.permute(1, 2, 0).detach().numpy()\n",
        "    img = img*255\n",
        "    img = img.astype(np.uint8)\n",
        "    fig, a = plt.subplots(1,1)\n",
        "    #fig.set_size_inches(10,10)\n",
        "    display(int(target.cpu().detach().numpy()))\n",
        "    a.imshow(img)\n",
        "    return None\n",
        "plot_sign(val_dataset, 13)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J90qxR2RJ28Z"
      },
      "source": [
        "### Гиперпараметры"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CI7vlkQPJ28Z"
      },
      "outputs": [],
      "source": [
        "device_id = 0\n",
        "if torch.cuda.is_available() == True:\n",
        "    device = f'cuda:{device_id}'\n",
        "elif torch.backends.mps.is_available() == True:\n",
        "    device = 'mps'\n",
        "else:\n",
        "    device = 'cpu'\n",
        "\n",
        "model_name = 'resnet152_with_rotate'\n",
        "last_epoch = 0\n",
        "n_epochs = 2\n",
        "batch_size = 32\n",
        "num_classes = 156"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HsbUmnfKyhlr",
        "outputId": "96e3d4b4-b23b-4653-972d-828936361e5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda:0'"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo40ASBXJ28Z"
      },
      "source": [
        "### Инициализация модели, задание оптимизатора и функции потерь"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TNQ4zYcHtHId"
      },
      "outputs": [],
      "source": [
        "def create_model(num_classes):\n",
        "    model = resnet152(weights='ResNet152_Weights.IMAGENET1K_V1')\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    \n",
        "    model.fc = nn.Sequential(nn.Linear(2048, 1024), nn.Linear(1024, num_classes))\n",
        "    #model.fc = nn.Sequential(nn.Linear(2048, num_classes))\n",
        "    for param in model.fc.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    #torchvision.models.detection.fasterrcnn_resnet50_fpn(weights='FasterRCNN_ResNet50_FPN_Weights.COCO_V1')\n",
        "    #in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    #model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint = torch.load(os.path.join(checkpoints_path, f'chkpt_classifier_{model_name}_{last_epoch}.pth'), map_location=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'losses_train', 'losses_val'])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checkpoint.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "242586dd62d544a39c122c7b1ed6efc9",
            "78cb1778f71b45b88ffdb20bfe0cd66e",
            "6d83c78727204f529982e56c3bfe32c1",
            "55191ea6a8eb4574aff8a9bc21fdd55a",
            "ae7f4e349b634abd8b36184774421f7a",
            "e012c54a3a9f48b38c500c17227f67b8",
            "91eb94ac4f6347c6b7d860b73e3742da",
            "4146164713eb4c2db70cf52335398a3e",
            "e66226838c994913b2d58fd0db3f4282",
            "b17d80f4da05457781be26c19a1dca17",
            "8e403bce502349d28da4553899f80c33"
          ]
        },
        "id": "ZdWxX5-PJ28Z",
        "outputId": "d572c5f8-341f-46c4-adb3-14866216da69"
      },
      "outputs": [],
      "source": [
        "model = create_model(num_classes).to(device)\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# parameters\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "#params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9, weight_decay=0.0005)\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.0005)\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
        "\n",
        "# Загрузка весов модели, состояния оптимизатора и шедулера\n",
        "if last_epoch is not None:\n",
        "    checkpoint = torch.load(os.path.join(checkpoints_path, f'chkpt_classifier_{model_name}_{last_epoch}.pth'), map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    #lr_scheduler.load_state_dict(checkpoint['lr_scheduler_state_dict'])\n",
        "\n",
        "#optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "    # and a learning rate scheduler\n",
        "#lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "#lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
        "train_dataset = RTSD_dataset_classifier(#json_path = os.path.join(dataset_path, 'train_anno_reduced.json'),\n",
        "                                        json_path = os.path.join(dataset_path, 'train_anno.json'),\n",
        "                                        img_path = dataset_path,\n",
        "                                        transforms = get_transform(train=True)\n",
        "                                        )\n",
        "val_dataset = RTSD_dataset_classifier(os.path.join(dataset_path, 'val_anno.json'),\n",
        "                                        dataset_path,\n",
        "                                          transforms = get_transform(train=False)\n",
        "                                          )\n",
        "\n",
        "train_data_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    #sampler=SubsetRandomSampler(),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    #num_workers=4,\n",
        "    #collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "val_data_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    #sampler=SubsetRandomSampler(),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    #num_workers=4,\n",
        "    #collate_fn=collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YimGoL3J28Z"
      },
      "source": [
        "### Трейн луп"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(train_dataloader, epoch): \n",
        "    len_dataloader = len(train_dataloader)\n",
        "\n",
        "    training_loss=0\n",
        "    correct = 0.0\n",
        "    total = 0.0\n",
        "\n",
        "    model.train()\n",
        "    running_loss = 0        # training_loss\n",
        "    for batch_idx, data in enumerate(train_dataloader):\n",
        "        \n",
        "        \n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        #images, targets = data[0], data[1]\n",
        "        #images = images.to(device)\n",
        "        #targets = targets.to(device)\n",
        "        images = data['images'].to(device)\n",
        "        targets = data['labels'].to(device)\n",
        "        output = model(images)\n",
        "        loss = loss_function(output, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss = running_loss + ((1/(batch_idx+1))*(loss.data-running_loss))\n",
        "        if batch_idx%20 == 0:\n",
        "            print(f\"Batch Id {batch_idx} is having training loss of {running_loss}\")\n",
        "            print(loss.item())\n",
        "\n",
        "        pred = output.data.max(1,keepdim=True)[1]\n",
        "        correct += np.sum(np.squeeze(pred.eq(targets.data.view_as(pred))).cpu().numpy())\n",
        "        total += images.size(0)\n",
        "        print(f\"Accuracy on batch {batch_idx} on Training is {(100*correct/total)}\")\n",
        "\n",
        "\n",
        "        #images = list(image.to(device) for image in images)\n",
        "        #targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        #loss_dict = model(images, targets)\n",
        "        #loss = sum(loss for loss in loss_dict.values())\n",
        "        #running_loss += loss.item()\n",
        "        #loss.backward()\n",
        "        #optimizer.step()\n",
        "        #if batch_idx % 50 == 0:\n",
        "        #    print(f\"\\tЭпоха {epoch}. Итерация {batch_idx}/{len_dataloader}. Loss: {loss}\")\n",
        "    train_loss = running_loss/len(train_dataloader.dataset)\n",
        "    return train_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def val (val_dataloader, epoch):\n",
        "    #len_dataloader = len(train_dataloader)\n",
        "\n",
        "    validation_loss=0\n",
        "    correct = 0.0\n",
        "    total = 0.0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for batch_idx, data in enumerate(val_dataloader):\n",
        "        \n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        #images, targets = data[0], data[1]\n",
        "        #images = images.to(device)\n",
        "        #targets = targets.to(device)\n",
        "        images = data['images'].to(device)\n",
        "        targets = data['labels'].to(device)\n",
        "        #with torch.no_grad():\n",
        "            \n",
        "        output = model(images)\n",
        "        loss = loss_function(output, targets)\n",
        "        \n",
        "        validation_loss = validation_loss + ((1/(batch_idx+1))*(loss.data-validation_loss))\n",
        "        #if batch_idx%20 == 0:\n",
        "        print(f\"Batch Id {batch_idx} is having validation loss of {validation_loss}\")\n",
        "        print(loss.item())\n",
        "\n",
        "        pred = output.data.max(1,keepdim=True)[1]\n",
        "        correct += np.sum(np.squeeze(pred.eq(targets.data.view_as(pred))).cpu().numpy())\n",
        "        total += images.size(0)\n",
        "        print(f\"Batch Id {batch_idx} is having validation accuracy of {(100*correct/total)}\")\n",
        "\n",
        "\n",
        "        #images = list(image.to(device) for image in images)\n",
        "        #targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        #loss_dict = model(images, targets)\n",
        "        #loss = sum(loss for loss in loss_dict.values())\n",
        "        #running_loss += loss.item()\n",
        "        #loss.backward()\n",
        "        #optimizer.step()\n",
        "        #if batch_idx % 50 == 0:\n",
        "        #    print(f\"\\tЭпоха {epoch}. Итерация {batch_idx}/{len_dataloader}. Loss: {loss}\")\n",
        "    val_loss = validation_loss/len(val_dataloader.dataset)\n",
        "    return val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch Id 0 is having training loss of 0.6684340834617615\n",
            "0.6684340834617615\n",
            "Accuracy on batch 0 on Training is 87.5\n",
            "Accuracy on batch 1 on Training is 84.375\n",
            "Accuracy on batch 2 on Training is 78.125\n",
            "Accuracy on batch 3 on Training is 78.90625\n",
            "Accuracy on batch 4 on Training is 79.375\n",
            "Accuracy on batch 5 on Training is 75.52083333333333\n",
            "Accuracy on batch 6 on Training is 73.66071428571429\n",
            "Accuracy on batch 7 on Training is 73.046875\n",
            "Accuracy on batch 8 on Training is 71.875\n",
            "Accuracy on batch 9 on Training is 71.875\n",
            "Accuracy on batch 10 on Training is 72.72727272727273\n",
            "Accuracy on batch 11 on Training is 72.91666666666667\n",
            "Accuracy on batch 12 on Training is 73.07692307692308\n",
            "Accuracy on batch 13 on Training is 73.4375\n",
            "Accuracy on batch 14 on Training is 74.375\n",
            "Accuracy on batch 15 on Training is 74.21875\n",
            "Accuracy on batch 16 on Training is 74.08088235294117\n",
            "Accuracy on batch 17 on Training is 74.30555555555556\n",
            "Accuracy on batch 18 on Training is 74.34210526315789\n",
            "Accuracy on batch 19 on Training is 74.375\n",
            "Batch Id 20 is having training loss of 0.8727125525474548\n",
            "0.7096356749534607\n",
            "Accuracy on batch 20 on Training is 74.70238095238095\n",
            "Accuracy on batch 21 on Training is 74.57386363636364\n",
            "Accuracy on batch 22 on Training is 75.0\n",
            "Accuracy on batch 23 on Training is 75.0\n",
            "Accuracy on batch 24 on Training is 75.0\n",
            "Accuracy on batch 25 on Training is 75.0\n",
            "Accuracy on batch 26 on Training is 75.46296296296296\n",
            "Accuracy on batch 27 on Training is 75.33482142857143\n",
            "Accuracy on batch 28 on Training is 75.32327586206897\n",
            "Accuracy on batch 29 on Training is 75.20833333333333\n",
            "Accuracy on batch 30 on Training is 74.59677419354838\n",
            "Accuracy on batch 31 on Training is 74.70703125\n",
            "Accuracy on batch 32 on Training is 75.0\n",
            "Accuracy on batch 33 on Training is 75.36764705882354\n",
            "Accuracy on batch 34 on Training is 75.35714285714286\n",
            "Accuracy on batch 35 on Training is 75.26041666666667\n",
            "Accuracy on batch 36 on Training is 75.33783783783784\n",
            "Accuracy on batch 37 on Training is 75.08223684210526\n",
            "Accuracy on batch 38 on Training is 75.24038461538461\n",
            "Accuracy on batch 39 on Training is 75.546875\n",
            "Batch Id 40 is having training loss of 0.8733981847763062\n",
            "0.8842421770095825\n",
            "Accuracy on batch 40 on Training is 75.3810975609756\n",
            "Accuracy on batch 41 on Training is 75.5952380952381\n",
            "Accuracy on batch 42 on Training is 75.43604651162791\n",
            "Accuracy on batch 43 on Training is 75.56818181818181\n",
            "Accuracy on batch 44 on Training is 75.55555555555556\n",
            "Accuracy on batch 45 on Training is 75.74728260869566\n",
            "Accuracy on batch 46 on Training is 75.46542553191489\n",
            "Accuracy on batch 47 on Training is 75.45572916666667\n",
            "Accuracy on batch 48 on Training is 75.51020408163265\n",
            "Accuracy on batch 49 on Training is 75.5\n",
            "Accuracy on batch 50 on Training is 75.42892156862744\n",
            "Accuracy on batch 51 on Training is 75.42067307692308\n",
            "Accuracy on batch 52 on Training is 75.47169811320755\n",
            "Accuracy on batch 53 on Training is 75.52083333333333\n",
            "Accuracy on batch 54 on Training is 75.625\n",
            "Accuracy on batch 55 on Training is 75.33482142857143\n",
            "Accuracy on batch 56 on Training is 75.32894736842105\n",
            "Accuracy on batch 57 on Training is 75.48491379310344\n",
            "Accuracy on batch 58 on Training is 75.47669491525424\n",
            "Accuracy on batch 59 on Training is 75.52083333333333\n",
            "Batch Id 60 is having training loss of 0.875573992729187\n",
            "1.1265398263931274\n",
            "Accuracy on batch 60 on Training is 75.40983606557377\n",
            "Accuracy on batch 61 on Training is 75.35282258064517\n",
            "Accuracy on batch 62 on Training is 75.44642857142857\n",
            "Accuracy on batch 63 on Training is 75.439453125\n",
            "Accuracy on batch 64 on Training is 75.625\n",
            "Accuracy on batch 65 on Training is 75.71022727272727\n",
            "Accuracy on batch 66 on Training is 75.60634328358209\n",
            "Accuracy on batch 67 on Training is 75.59742647058823\n",
            "Accuracy on batch 68 on Training is 75.58876811594203\n",
            "Accuracy on batch 69 on Training is 75.53571428571429\n",
            "Accuracy on batch 70 on Training is 75.35211267605634\n",
            "Accuracy on batch 71 on Training is 75.34722222222223\n",
            "Accuracy on batch 72 on Training is 75.21404109589041\n",
            "Accuracy on batch 73 on Training is 75.29560810810811\n",
            "Accuracy on batch 74 on Training is 75.375\n",
            "Accuracy on batch 75 on Training is 75.28782894736842\n",
            "Accuracy on batch 76 on Training is 75.44642857142857\n",
            "Accuracy on batch 77 on Training is 75.52083333333333\n",
            "Accuracy on batch 78 on Training is 75.4746835443038\n",
            "Accuracy on batch 79 on Training is 75.546875\n",
            "Batch Id 80 is having training loss of 0.8750990033149719\n",
            "0.9766258001327515\n",
            "Accuracy on batch 80 on Training is 75.42438271604938\n",
            "Accuracy on batch 81 on Training is 75.57164634146342\n",
            "Accuracy on batch 82 on Training is 75.56475903614458\n",
            "Accuracy on batch 83 on Training is 75.5952380952381\n",
            "Accuracy on batch 84 on Training is 75.625\n",
            "Accuracy on batch 85 on Training is 75.50872093023256\n",
            "Accuracy on batch 86 on Training is 75.61063218390805\n",
            "Accuracy on batch 87 on Training is 75.60369318181819\n",
            "Accuracy on batch 88 on Training is 75.66713483146067\n",
            "Accuracy on batch 89 on Training is 75.76388888888889\n",
            "Accuracy on batch 90 on Training is 75.82417582417582\n",
            "Accuracy on batch 91 on Training is 75.8491847826087\n",
            "Accuracy on batch 92 on Training is 75.77284946236558\n",
            "Accuracy on batch 93 on Training is 75.76462765957447\n",
            "Accuracy on batch 94 on Training is 75.72368421052632\n",
            "Accuracy on batch 95 on Training is 75.71614583333333\n",
            "Accuracy on batch 96 on Training is 75.7409793814433\n",
            "Accuracy on batch 97 on Training is 75.89285714285714\n",
            "Accuracy on batch 98 on Training is 75.88383838383838\n",
            "Accuracy on batch 99 on Training is 75.90625\n",
            "Batch Id 100 is having training loss of 0.8706642389297485\n",
            "0.8883853554725647\n",
            "Accuracy on batch 100 on Training is 75.83539603960396\n",
            "Accuracy on batch 101 on Training is 75.79656862745098\n",
            "Accuracy on batch 102 on Training is 75.8495145631068\n",
            "Accuracy on batch 103 on Training is 75.78125\n",
            "Accuracy on batch 104 on Training is 75.89285714285714\n",
            "Accuracy on batch 105 on Training is 75.91391509433963\n",
            "Accuracy on batch 106 on Training is 75.93457943925233\n",
            "Accuracy on batch 107 on Training is 75.86805555555556\n",
            "Accuracy on batch 108 on Training is 75.86009174311927\n",
            "Accuracy on batch 109 on Training is 75.9090909090909\n",
            "Accuracy on batch 110 on Training is 76.01351351351352\n",
            "Accuracy on batch 111 on Training is 76.06026785714286\n",
            "Accuracy on batch 112 on Training is 76.10619469026548\n",
            "Accuracy on batch 113 on Training is 76.12390350877193\n",
            "Accuracy on batch 114 on Training is 76.19565217391305\n",
            "Accuracy on batch 115 on Training is 76.23922413793103\n",
            "Accuracy on batch 116 on Training is 76.25534188034187\n",
            "Accuracy on batch 117 on Training is 76.32415254237289\n",
            "Accuracy on batch 118 on Training is 76.3655462184874\n",
            "Accuracy on batch 119 on Training is 76.38020833333333\n",
            "Batch Id 120 is having training loss of 0.8573887944221497\n",
            "0.7167584896087646\n",
            "Accuracy on batch 120 on Training is 76.49793388429752\n",
            "Accuracy on batch 121 on Training is 76.46004098360656\n",
            "Accuracy on batch 122 on Training is 76.57520325203252\n",
            "Accuracy on batch 123 on Training is 76.53729838709677\n",
            "Accuracy on batch 124 on Training is 76.475\n",
            "Accuracy on batch 125 on Training is 76.46329365079364\n",
            "Accuracy on batch 126 on Training is 76.47637795275591\n",
            "Accuracy on batch 127 on Training is 76.4404296875\n",
            "Accuracy on batch 128 on Training is 76.35658914728683\n",
            "Accuracy on batch 129 on Training is 76.32211538461539\n",
            "Accuracy on batch 130 on Training is 76.33587786259542\n",
            "Accuracy on batch 131 on Training is 76.32575757575758\n",
            "Accuracy on batch 132 on Training is 76.2922932330827\n",
            "Accuracy on batch 133 on Training is 76.32929104477611\n",
            "Accuracy on batch 134 on Training is 76.36574074074075\n",
            "Accuracy on batch 135 on Training is 76.37867647058823\n",
            "Accuracy on batch 136 on Training is 76.39142335766424\n",
            "Accuracy on batch 137 on Training is 76.3586956521739\n",
            "Accuracy on batch 138 on Training is 76.32643884892086\n",
            "Accuracy on batch 139 on Training is 76.47321428571429\n",
            "Batch Id 140 is having training loss of 0.863966166973114\n",
            "1.0827856063842773\n",
            "Accuracy on batch 140 on Training is 76.41843971631205\n",
            "Accuracy on batch 141 on Training is 76.43045774647888\n",
            "Accuracy on batch 142 on Training is 76.3548951048951\n",
            "Accuracy on batch 143 on Training is 76.38888888888889\n",
            "Accuracy on batch 144 on Training is 76.37931034482759\n",
            "Accuracy on batch 145 on Training is 76.36986301369863\n",
            "Accuracy on batch 146 on Training is 76.38180272108843\n",
            "Accuracy on batch 147 on Training is 76.37246621621621\n",
            "Accuracy on batch 148 on Training is 76.34228187919463\n",
            "Accuracy on batch 149 on Training is 76.33333333333333\n",
            "Accuracy on batch 150 on Training is 76.40728476821192\n",
            "Accuracy on batch 151 on Training is 76.43914473684211\n",
            "Accuracy on batch 152 on Training is 76.38888888888889\n",
            "Accuracy on batch 153 on Training is 76.33928571428571\n",
            "Accuracy on batch 154 on Training is 76.31048387096774\n",
            "Accuracy on batch 155 on Training is 76.24198717948718\n",
            "Accuracy on batch 156 on Training is 76.15445859872611\n",
            "Accuracy on batch 157 on Training is 76.14715189873418\n",
            "Accuracy on batch 158 on Training is 76.1006289308176\n",
            "Accuracy on batch 159 on Training is 76.11328125\n",
            "Batch Id 160 is having training loss of 0.8717268109321594\n",
            "0.955585777759552\n",
            "Accuracy on batch 160 on Training is 76.08695652173913\n",
            "Accuracy on batch 161 on Training is 76.09953703703704\n",
            "Accuracy on batch 162 on Training is 76.09279141104294\n",
            "Accuracy on batch 163 on Training is 76.10518292682927\n",
            "Accuracy on batch 164 on Training is 76.11742424242425\n",
            "Accuracy on batch 165 on Training is 76.11069277108433\n",
            "Accuracy on batch 166 on Training is 76.16017964071857\n",
            "Accuracy on batch 167 on Training is 76.171875\n",
            "Accuracy on batch 168 on Training is 76.22041420118343\n",
            "Accuracy on batch 169 on Training is 76.26838235294117\n",
            "Accuracy on batch 170 on Training is 76.29751461988305\n",
            "Accuracy on batch 171 on Training is 76.23546511627907\n",
            "Accuracy on batch 172 on Training is 76.22832369942196\n",
            "Accuracy on batch 173 on Training is 76.14942528735632\n",
            "Accuracy on batch 174 on Training is 76.14285714285714\n",
            "Accuracy on batch 175 on Training is 76.171875\n",
            "Accuracy on batch 176 on Training is 76.18290960451978\n",
            "Accuracy on batch 177 on Training is 76.15870786516854\n",
            "Accuracy on batch 178 on Training is 76.16969273743017\n",
            "Accuracy on batch 179 on Training is 76.14583333333333\n",
            "Batch Id 180 is having training loss of 0.8664383292198181\n",
            "0.9302794337272644\n",
            "Accuracy on batch 180 on Training is 76.17403314917127\n",
            "Accuracy on batch 181 on Training is 76.18475274725274\n",
            "Accuracy on batch 182 on Training is 76.26366120218579\n",
            "Accuracy on batch 183 on Training is 76.25679347826087\n",
            "Accuracy on batch 184 on Training is 76.26689189189189\n",
            "Accuracy on batch 185 on Training is 76.22647849462365\n",
            "Accuracy on batch 186 on Training is 76.16978609625669\n",
            "Accuracy on batch 187 on Training is 76.21343085106383\n",
            "Accuracy on batch 188 on Training is 76.25661375661376\n",
            "Accuracy on batch 189 on Training is 76.26644736842105\n",
            "Accuracy on batch 190 on Training is 76.2761780104712\n",
            "Accuracy on batch 191 on Training is 76.318359375\n",
            "Accuracy on batch 192 on Training is 76.34391191709845\n",
            "Accuracy on batch 193 on Training is 76.35309278350516\n",
            "Accuracy on batch 194 on Training is 76.34615384615384\n",
            "Accuracy on batch 195 on Training is 76.33928571428571\n",
            "Accuracy on batch 196 on Training is 76.39593908629442\n",
            "Accuracy on batch 197 on Training is 76.40467171717172\n",
            "Accuracy on batch 198 on Training is 76.44472361809045\n",
            "Accuracy on batch 199 on Training is 76.484375\n",
            "Batch Id 200 is having training loss of 0.8502790331840515\n",
            "0.2594282627105713\n",
            "Accuracy on batch 200 on Training is 76.55472636815921\n",
            "Accuracy on batch 201 on Training is 76.60891089108911\n",
            "Accuracy on batch 202 on Training is 76.64716748768473\n",
            "Accuracy on batch 203 on Training is 76.62377450980392\n",
            "Accuracy on batch 204 on Training is 76.6310975609756\n",
            "Accuracy on batch 205 on Training is 76.59283980582525\n",
            "Accuracy on batch 206 on Training is 76.57004830917874\n",
            "Accuracy on batch 207 on Training is 76.57752403846153\n",
            "Accuracy on batch 208 on Training is 76.59988038277513\n",
            "Accuracy on batch 209 on Training is 76.65178571428571\n",
            "Accuracy on batch 210 on Training is 76.67357819905213\n",
            "Accuracy on batch 211 on Training is 76.73938679245283\n",
            "Accuracy on batch 212 on Training is 76.7018779342723\n",
            "Accuracy on batch 213 on Training is 76.69392523364486\n",
            "Accuracy on batch 214 on Training is 76.71511627906976\n",
            "Accuracy on batch 215 on Training is 76.75057870370371\n",
            "Accuracy on batch 216 on Training is 76.75691244239631\n",
            "Accuracy on batch 217 on Training is 76.74885321100918\n",
            "Accuracy on batch 218 on Training is 76.75513698630137\n",
            "Accuracy on batch 219 on Training is 76.76136363636364\n",
            "Batch Id 220 is having training loss of 0.8417051434516907\n",
            "0.6240573525428772\n",
            "Accuracy on batch 220 on Training is 76.79581447963801\n",
            "Accuracy on batch 221 on Training is 76.77364864864865\n",
            "Accuracy on batch 222 on Training is 76.80773542600897\n",
            "Accuracy on batch 223 on Training is 76.79966517857143\n",
            "Accuracy on batch 224 on Training is 76.80555555555556\n",
            "Accuracy on batch 225 on Training is 76.79756637168141\n",
            "Accuracy on batch 226 on Training is 76.83094713656388\n",
            "Accuracy on batch 227 on Training is 76.87774122807018\n",
            "Accuracy on batch 228 on Training is 76.91048034934498\n",
            "Accuracy on batch 229 on Training is 76.9429347826087\n",
            "Accuracy on batch 230 on Training is 76.90746753246754\n",
            "Accuracy on batch 231 on Training is 76.92618534482759\n",
            "Accuracy on batch 232 on Training is 76.90450643776823\n",
            "Accuracy on batch 233 on Training is 76.90972222222223\n",
            "Accuracy on batch 234 on Training is 76.88829787234043\n",
            "Accuracy on batch 235 on Training is 76.89353813559322\n",
            "Accuracy on batch 236 on Training is 76.87236286919831\n",
            "Accuracy on batch 237 on Training is 76.90388655462185\n",
            "Accuracy on batch 238 on Training is 76.93514644351464\n",
            "Accuracy on batch 239 on Training is 76.96614583333333\n",
            "Batch Id 240 is having training loss of 0.8422996401786804\n",
            "0.6830151677131653\n",
            "Accuracy on batch 240 on Training is 76.95798755186722\n",
            "Accuracy on batch 241 on Training is 76.94989669421487\n",
            "Accuracy on batch 242 on Training is 76.91615226337449\n",
            "Accuracy on batch 243 on Training is 76.92110655737704\n",
            "Accuracy on batch 244 on Training is 76.875\n",
            "Accuracy on batch 245 on Training is 76.9308943089431\n",
            "Accuracy on batch 246 on Training is 76.93572874493927\n",
            "Accuracy on batch 247 on Training is 76.90272177419355\n",
            "Accuracy on batch 248 on Training is 76.86997991967871\n",
            "Accuracy on batch 249 on Training is 76.85\n",
            "Accuracy on batch 250 on Training is 76.81772908366534\n",
            "Accuracy on batch 251 on Training is 76.81051587301587\n",
            "Accuracy on batch 252 on Training is 76.82806324110672\n",
            "Accuracy on batch 253 on Training is 76.85777559055119\n",
            "Accuracy on batch 254 on Training is 76.83823529411765\n",
            "Accuracy on batch 255 on Training is 76.8310546875\n",
            "Accuracy on batch 256 on Training is 76.87256809338521\n",
            "Accuracy on batch 257 on Training is 76.9016472868217\n",
            "Accuracy on batch 258 on Training is 76.8219111969112\n",
            "Accuracy on batch 259 on Training is 76.80288461538461\n",
            "Batch Id 260 is having training loss of 0.8461413383483887\n",
            "0.8979265093803406\n",
            "Accuracy on batch 260 on Training is 76.80795019157088\n",
            "Accuracy on batch 261 on Training is 76.82490458015268\n",
            "Accuracy on batch 262 on Training is 76.81796577946768\n",
            "Accuracy on batch 263 on Training is 76.8465909090909\n",
            "Accuracy on batch 264 on Training is 76.82783018867924\n",
            "Accuracy on batch 265 on Training is 76.82095864661655\n",
            "Accuracy on batch 266 on Training is 76.82584269662921\n",
            "Accuracy on batch 267 on Training is 76.84235074626865\n",
            "Accuracy on batch 268 on Training is 76.88197026022304\n",
            "Accuracy on batch 269 on Training is 76.88657407407408\n",
            "Accuracy on batch 270 on Training is 76.91420664206642\n",
            "Accuracy on batch 271 on Training is 76.94163602941177\n",
            "Accuracy on batch 272 on Training is 76.93452380952381\n",
            "Accuracy on batch 273 on Training is 76.93886861313868\n",
            "Accuracy on batch 274 on Training is 76.92045454545455\n",
            "Accuracy on batch 275 on Training is 76.89085144927536\n",
            "Accuracy on batch 276 on Training is 76.83889891696751\n",
            "Accuracy on batch 277 on Training is 76.84352517985612\n",
            "Accuracy on batch 278 on Training is 76.87051971326164\n",
            "Accuracy on batch 279 on Training is 76.88616071428571\n",
            "Batch Id 280 is having training loss of 0.8441793918609619\n",
            "0.7840171456336975\n",
            "Accuracy on batch 280 on Training is 76.89056939501779\n",
            "Accuracy on batch 281 on Training is 76.8395390070922\n",
            "Accuracy on batch 282 on Training is 76.85512367491167\n",
            "Accuracy on batch 283 on Training is 76.8705985915493\n",
            "Accuracy on batch 284 on Training is 76.875\n",
            "Accuracy on batch 285 on Training is 76.85751748251748\n",
            "Accuracy on batch 286 on Training is 76.88371080139373\n",
            "Accuracy on batch 287 on Training is 76.89887152777777\n",
            "Accuracy on batch 288 on Training is 76.87067474048443\n",
            "Accuracy on batch 289 on Training is 76.86422413793103\n",
            "Accuracy on batch 290 on Training is 76.90077319587628\n",
            "Accuracy on batch 291 on Training is 76.86215753424658\n",
            "Accuracy on batch 292 on Training is 76.85580204778157\n",
            "Accuracy on batch 293 on Training is 76.89200680272108\n",
            "Accuracy on batch 294 on Training is 76.875\n",
            "Accuracy on batch 295 on Training is 76.86866554054055\n",
            "Accuracy on batch 296 on Training is 76.87289562289563\n",
            "Accuracy on batch 297 on Training is 76.88758389261746\n",
            "Accuracy on batch 298 on Training is 76.90217391304348\n",
            "Accuracy on batch 299 on Training is 76.89583333333333\n",
            "Batch Id 300 is having training loss of 0.8422650098800659\n",
            "0.5687918663024902\n",
            "Accuracy on batch 300 on Training is 76.89991694352159\n",
            "Accuracy on batch 301 on Training is 76.90397350993378\n",
            "Accuracy on batch 302 on Training is 76.8976897689769\n",
            "Accuracy on batch 303 on Training is 76.87088815789474\n",
            "Accuracy on batch 304 on Training is 76.85450819672131\n",
            "Accuracy on batch 305 on Training is 76.79738562091504\n",
            "Accuracy on batch 306 on Training is 76.78135179153094\n",
            "Accuracy on batch 307 on Training is 76.7958603896104\n",
            "Accuracy on batch 308 on Training is 76.7799352750809\n",
            "Accuracy on batch 309 on Training is 76.75403225806451\n",
            "Accuracy on batch 310 on Training is 76.78858520900322\n",
            "Accuracy on batch 311 on Training is 76.82291666666667\n",
            "Accuracy on batch 312 on Training is 76.81709265175719\n",
            "Accuracy on batch 313 on Training is 76.79140127388536\n",
            "Accuracy on batch 314 on Training is 76.76587301587301\n",
            "Accuracy on batch 315 on Training is 76.78995253164557\n",
            "Accuracy on batch 316 on Training is 76.78430599369085\n",
            "Accuracy on batch 317 on Training is 76.76886792452831\n",
            "Accuracy on batch 318 on Training is 76.78291536050156\n",
            "Accuracy on batch 319 on Training is 76.748046875\n",
            "Batch Id 320 is having training loss of 0.8473331332206726\n",
            "0.7930015921592712\n",
            "Accuracy on batch 320 on Training is 76.74260124610592\n",
            "Accuracy on batch 321 on Training is 76.75659937888199\n",
            "Accuracy on batch 322 on Training is 76.75116099071208\n",
            "Accuracy on batch 323 on Training is 76.73611111111111\n",
            "Accuracy on batch 324 on Training is 76.73076923076923\n",
            "Accuracy on batch 325 on Training is 76.71587423312883\n",
            "Accuracy on batch 326 on Training is 76.72974006116208\n",
            "Accuracy on batch 327 on Training is 76.73399390243902\n",
            "Accuracy on batch 328 on Training is 76.7572188449848\n",
            "Accuracy on batch 329 on Training is 76.74242424242425\n",
            "Accuracy on batch 330 on Training is 76.70883685800604\n",
            "Accuracy on batch 331 on Training is 76.72251506024097\n",
            "Accuracy on batch 332 on Training is 76.73611111111111\n",
            "Accuracy on batch 333 on Training is 76.7309131736527\n",
            "Accuracy on batch 334 on Training is 76.73507462686567\n",
            "Accuracy on batch 335 on Training is 76.7485119047619\n",
            "Accuracy on batch 336 on Training is 76.76186943620178\n",
            "Accuracy on batch 337 on Training is 76.77514792899409\n",
            "Accuracy on batch 338 on Training is 76.80678466076697\n",
            "Accuracy on batch 339 on Training is 76.81985294117646\n",
            "Batch Id 340 is having training loss of 0.8461736440658569\n",
            "0.8244341611862183\n",
            "Accuracy on batch 340 on Training is 76.81451612903226\n",
            "Accuracy on batch 341 on Training is 76.81834795321637\n",
            "Accuracy on batch 342 on Training is 76.78571428571429\n",
            "Accuracy on batch 343 on Training is 76.77143895348837\n",
            "Accuracy on batch 344 on Training is 76.78442028985508\n",
            "Accuracy on batch 345 on Training is 76.77023121387283\n",
            "Accuracy on batch 346 on Training is 76.75612391930835\n",
            "Accuracy on batch 347 on Training is 76.78699712643679\n",
            "Accuracy on batch 348 on Training is 76.81769340974212\n",
            "Accuracy on batch 349 on Training is 76.75\n",
            "Accuracy on batch 350 on Training is 76.73611111111111\n",
            "Accuracy on batch 351 on Training is 76.72230113636364\n",
            "Accuracy on batch 352 on Training is 76.67315864022663\n",
            "Accuracy on batch 353 on Training is 76.68608757062147\n",
            "Accuracy on batch 354 on Training is 76.68133802816901\n",
            "Accuracy on batch 355 on Training is 76.67661516853933\n",
            "Accuracy on batch 356 on Training is 76.69817927170868\n",
            "Accuracy on batch 357 on Training is 76.68470670391062\n",
            "Accuracy on batch 358 on Training is 76.66260445682451\n",
            "Accuracy on batch 359 on Training is 76.640625\n",
            "Batch Id 360 is having training loss of 0.8496361374855042\n",
            "1.0242412090301514\n",
            "Accuracy on batch 360 on Training is 76.62742382271468\n",
            "Accuracy on batch 361 on Training is 76.64019337016575\n",
            "Accuracy on batch 362 on Training is 76.65289256198348\n",
            "Accuracy on batch 363 on Training is 76.6826923076923\n",
            "Accuracy on batch 364 on Training is 76.71232876712328\n",
            "Accuracy on batch 365 on Training is 76.69911202185793\n",
            "Accuracy on batch 366 on Training is 76.66893732970027\n",
            "Accuracy on batch 367 on Training is 76.64741847826087\n",
            "Accuracy on batch 368 on Training is 76.67682926829268\n",
            "Accuracy on batch 369 on Training is 76.6554054054054\n",
            "Accuracy on batch 370 on Training is 76.68463611859838\n",
            "Accuracy on batch 371 on Training is 76.70530913978494\n",
            "Accuracy on batch 372 on Training is 76.71749329758713\n",
            "Accuracy on batch 373 on Training is 76.68783422459893\n",
            "Accuracy on batch 374 on Training is 76.70833333333333\n",
            "Accuracy on batch 375 on Training is 76.70378989361703\n",
            "Accuracy on batch 376 on Training is 76.71584880636605\n",
            "Accuracy on batch 377 on Training is 76.7526455026455\n",
            "Accuracy on batch 378 on Training is 76.73153034300792\n",
            "Accuracy on batch 379 on Training is 76.71052631578948\n",
            "Batch Id 380 is having training loss of 0.8427203297615051\n",
            "0.5755119919776917\n",
            "Accuracy on batch 380 on Training is 76.7224409448819\n",
            "Accuracy on batch 381 on Training is 76.73429319371728\n",
            "Accuracy on batch 382 on Training is 76.72976501305483\n",
            "Accuracy on batch 383 on Training is 76.71712239583333\n",
            "Accuracy on batch 384 on Training is 76.71266233766234\n",
            "Accuracy on batch 385 on Training is 76.73251295336787\n",
            "Accuracy on batch 386 on Training is 76.76841085271317\n",
            "Accuracy on batch 387 on Training is 76.7638530927835\n",
            "Accuracy on batch 388 on Training is 76.75931876606684\n",
            "Accuracy on batch 389 on Training is 76.77083333333333\n",
            "Accuracy on batch 390 on Training is 76.79028132992327\n",
            "Accuracy on batch 391 on Training is 76.78571428571429\n",
            "Accuracy on batch 392 on Training is 76.80502544529261\n",
            "Accuracy on batch 393 on Training is 76.79251269035532\n",
            "Accuracy on batch 394 on Training is 76.80379746835443\n",
            "Accuracy on batch 395 on Training is 76.82291666666667\n",
            "Accuracy on batch 396 on Training is 76.82619647355163\n",
            "Accuracy on batch 397 on Training is 76.7980527638191\n",
            "Accuracy on batch 398 on Training is 76.79354636591479\n",
            "Accuracy on batch 399 on Training is 76.7734375\n",
            "Batch Id 400 is having training loss of 0.8417499661445618\n",
            "0.3368046283721924\n",
            "Accuracy on batch 400 on Training is 76.80798004987531\n",
            "Accuracy on batch 401 on Training is 76.82680348258707\n",
            "Accuracy on batch 402 on Training is 76.79900744416874\n",
            "Accuracy on batch 403 on Training is 76.77908415841584\n",
            "Accuracy on batch 404 on Training is 76.75925925925925\n",
            "Accuracy on batch 405 on Training is 76.77032019704434\n",
            "Accuracy on batch 406 on Training is 76.76597051597052\n",
            "Accuracy on batch 407 on Training is 76.77696078431373\n",
            "Accuracy on batch 408 on Training is 76.77261613691931\n",
            "Accuracy on batch 409 on Training is 76.73780487804878\n",
            "Accuracy on batch 410 on Training is 76.7183698296837\n",
            "Accuracy on batch 411 on Training is 76.72178398058253\n",
            "Accuracy on batch 412 on Training is 76.71004842615012\n",
            "Accuracy on batch 413 on Training is 76.6832729468599\n",
            "Accuracy on batch 414 on Training is 76.70180722891567\n",
            "Accuracy on batch 415 on Training is 76.72776442307692\n",
            "Accuracy on batch 416 on Training is 76.72362110311751\n",
            "Accuracy on batch 417 on Training is 76.7494019138756\n",
            "Accuracy on batch 418 on Training is 76.75268496420048\n",
            "Accuracy on batch 419 on Training is 76.72619047619048\n",
            "Batch Id 420 is having training loss of 0.8428073525428772\n",
            "0.802029013633728\n",
            "Accuracy on batch 420 on Training is 76.74435866983373\n",
            "Accuracy on batch 421 on Training is 76.7550355450237\n",
            "Accuracy on batch 422 on Training is 76.73611111111111\n",
            "Accuracy on batch 423 on Training is 76.73201650943396\n",
            "Accuracy on batch 424 on Training is 76.72058823529412\n",
            "Accuracy on batch 425 on Training is 76.69454225352112\n",
            "Accuracy on batch 426 on Training is 76.70521077283372\n",
            "Accuracy on batch 427 on Training is 76.72313084112149\n",
            "Accuracy on batch 428 on Training is 76.71911421911422\n",
            "Accuracy on batch 429 on Training is 76.71511627906976\n",
            "Accuracy on batch 430 on Training is 76.72563805104409\n",
            "Accuracy on batch 431 on Training is 76.69994212962963\n",
            "Accuracy on batch 432 on Training is 76.68879907621248\n",
            "Accuracy on batch 433 on Training is 76.65610599078342\n",
            "Accuracy on batch 434 on Training is 76.64511494252874\n",
            "Accuracy on batch 435 on Training is 76.64134174311927\n",
            "Accuracy on batch 436 on Training is 76.66618993135012\n",
            "Accuracy on batch 437 on Training is 76.64098173515981\n",
            "Accuracy on batch 438 on Training is 76.60876993166288\n",
            "Accuracy on batch 439 on Training is 76.59801136363636\n",
            "Batch Id 440 is having training loss of 0.8488622307777405\n",
            "0.6097787618637085\n",
            "Accuracy on batch 440 on Training is 76.60856009070295\n",
            "Accuracy on batch 441 on Training is 76.61906108597285\n",
            "Accuracy on batch 442 on Training is 76.622460496614\n",
            "Accuracy on batch 443 on Training is 76.63992117117117\n",
            "Accuracy on batch 444 on Training is 76.6502808988764\n",
            "Accuracy on batch 445 on Training is 76.6535874439462\n",
            "Accuracy on batch 446 on Training is 76.64289709172259\n",
            "Accuracy on batch 447 on Training is 76.61830357142857\n",
            "Accuracy on batch 448 on Training is 76.62861915367483\n",
            "Accuracy on batch 449 on Training is 76.65277777777777\n",
            "Accuracy on batch 450 on Training is 76.66297117516629\n",
            "Accuracy on batch 451 on Training is 76.67311946902655\n",
            "Accuracy on batch 452 on Training is 76.69012141280353\n",
            "Accuracy on batch 453 on Training is 76.66574889867842\n",
            "Accuracy on batch 454 on Training is 76.64148351648352\n",
            "Accuracy on batch 455 on Training is 76.64473684210526\n",
            "Accuracy on batch 456 on Training is 76.60010940919037\n",
            "Accuracy on batch 457 on Training is 76.59661572052401\n",
            "Accuracy on batch 458 on Training is 76.57271241830065\n",
            "Accuracy on batch 459 on Training is 76.58967391304348\n",
            "Batch Id 460 is having training loss of 0.8512104749679565\n",
            "1.2123169898986816\n",
            "Accuracy on batch 460 on Training is 76.56588937093275\n",
            "Accuracy on batch 461 on Training is 76.58279220779221\n",
            "Accuracy on batch 462 on Training is 76.58612311015119\n",
            "Accuracy on batch 463 on Training is 76.5759698275862\n",
            "Accuracy on batch 464 on Training is 76.59274193548387\n",
            "Accuracy on batch 465 on Training is 76.61614806866953\n",
            "Accuracy on batch 466 on Training is 76.62607066381156\n",
            "Accuracy on batch 467 on Training is 76.5892094017094\n",
            "Accuracy on batch 468 on Training is 76.5658315565032\n",
            "Accuracy on batch 469 on Training is 76.5625\n",
            "Accuracy on batch 470 on Training is 76.55918259023355\n",
            "Accuracy on batch 471 on Training is 76.56912076271186\n",
            "Accuracy on batch 472 on Training is 76.57241014799155\n",
            "Accuracy on batch 473 on Training is 76.55590717299577\n",
            "Accuracy on batch 474 on Training is 76.5592105263158\n",
            "Accuracy on batch 475 on Training is 76.5625\n",
            "Accuracy on batch 476 on Training is 76.53957023060796\n",
            "Accuracy on batch 477 on Training is 76.5232740585774\n",
            "Accuracy on batch 478 on Training is 76.50704592901879\n",
            "Accuracy on batch 479 on Training is 76.51692708333333\n",
            "Batch Id 480 is having training loss of 0.8521056771278381\n",
            "0.4212549924850464\n",
            "Accuracy on batch 480 on Training is 76.54625779625779\n",
            "Accuracy on batch 481 on Training is 76.54304979253112\n",
            "Accuracy on batch 482 on Training is 76.5333850931677\n",
            "Accuracy on batch 483 on Training is 76.51084710743801\n",
            "Accuracy on batch 484 on Training is 76.5270618556701\n",
            "Accuracy on batch 485 on Training is 76.53677983539094\n",
            "Accuracy on batch 486 on Training is 76.54645790554414\n",
            "Accuracy on batch 487 on Training is 76.55609631147541\n",
            "Accuracy on batch 488 on Training is 76.53374233128834\n",
            "Accuracy on batch 489 on Training is 76.53698979591837\n",
            "Accuracy on batch 490 on Training is 76.5274949083503\n",
            "Accuracy on batch 491 on Training is 76.5307418699187\n",
            "Accuracy on batch 492 on Training is 76.54031440162272\n",
            "Accuracy on batch 493 on Training is 76.54984817813765\n",
            "Accuracy on batch 494 on Training is 76.55934343434343\n",
            "Accuracy on batch 495 on Training is 76.53729838709677\n",
            "Accuracy on batch 496 on Training is 76.54678068410463\n",
            "Accuracy on batch 497 on Training is 76.55622489959839\n",
            "Accuracy on batch 498 on Training is 76.55310621242485\n",
            "Accuracy on batch 499 on Training is 76.56875\n",
            "Batch Id 500 is having training loss of 0.8526291251182556\n",
            "0.8349935412406921\n",
            "Accuracy on batch 500 on Training is 76.57809381237524\n",
            "Accuracy on batch 501 on Training is 76.57495019920319\n",
            "Accuracy on batch 502 on Training is 76.57181908548708\n",
            "Accuracy on batch 503 on Training is 76.55629960317461\n",
            "Accuracy on batch 504 on Training is 76.55940594059406\n",
            "Accuracy on batch 505 on Training is 76.58102766798419\n",
            "Accuracy on batch 506 on Training is 76.59023668639053\n",
            "Accuracy on batch 507 on Training is 76.6240157480315\n",
            "Accuracy on batch 508 on Training is 76.60854616895874\n",
            "Accuracy on batch 509 on Training is 76.62377450980392\n",
            "Accuracy on batch 510 on Training is 76.60836594911937\n",
            "Accuracy on batch 511 on Training is 76.617431640625\n",
            "Accuracy on batch 512 on Training is 76.60818713450293\n",
            "Accuracy on batch 513 on Training is 76.60505836575875\n",
            "Accuracy on batch 514 on Training is 76.61407766990291\n",
            "Accuracy on batch 515 on Training is 76.60489341085271\n",
            "Accuracy on batch 516 on Training is 76.60178916827853\n",
            "Accuracy on batch 517 on Training is 76.60472972972973\n",
            "Accuracy on batch 518 on Training is 76.58959537572254\n",
            "Accuracy on batch 519 on Training is 76.55048076923077\n",
            "Batch Id 520 is having training loss of 0.8513138294219971\n",
            "0.7495734095573425\n",
            "Accuracy on batch 520 on Training is 76.5595009596929\n",
            "Accuracy on batch 521 on Training is 76.54454022988506\n",
            "Accuracy on batch 522 on Training is 76.54158699808795\n",
            "Accuracy on batch 523 on Training is 76.52671755725191\n",
            "Accuracy on batch 524 on Training is 76.5297619047619\n",
            "Accuracy on batch 525 on Training is 76.51497148288973\n",
            "Accuracy on batch 526 on Training is 76.52988614800759\n",
            "Accuracy on batch 527 on Training is 76.53882575757575\n",
            "Accuracy on batch 528 on Training is 76.52410207939508\n",
            "Accuracy on batch 529 on Training is 76.52122641509433\n",
            "Accuracy on batch 530 on Training is 76.54778719397363\n",
            "Accuracy on batch 531 on Training is 76.55662593984962\n",
            "Accuracy on batch 532 on Training is 76.55370544090056\n",
            "Accuracy on batch 533 on Training is 76.56835205992509\n",
            "Accuracy on batch 534 on Training is 76.54789719626169\n",
            "Accuracy on batch 535 on Training is 76.55083955223881\n",
            "Accuracy on batch 536 on Training is 76.55959031657356\n",
            "Accuracy on batch 537 on Training is 76.57411710037175\n",
            "Accuracy on batch 538 on Training is 76.58279220779221\n",
            "Accuracy on batch 539 on Training is 76.61458333333333\n",
            "Batch Id 540 is having training loss of 0.8488738536834717\n",
            "0.8690429925918579\n",
            "Accuracy on batch 540 on Training is 76.6115988909427\n",
            "Accuracy on batch 541 on Training is 76.60285977859779\n",
            "Accuracy on batch 542 on Training is 76.5999079189687\n",
            "Accuracy on batch 543 on Training is 76.59696691176471\n",
            "Accuracy on batch 544 on Training is 76.6112385321101\n",
            "Accuracy on batch 545 on Training is 76.59684065934066\n",
            "Accuracy on batch 546 on Training is 76.59963436928702\n",
            "Accuracy on batch 547 on Training is 76.61952554744525\n",
            "Accuracy on batch 548 on Training is 76.60519125683061\n",
            "Accuracy on batch 549 on Training is 76.61931818181819\n",
            "Accuracy on batch 550 on Training is 76.63906533575317\n",
            "Accuracy on batch 551 on Training is 76.64741847826087\n",
            "Accuracy on batch 552 on Training is 76.6500904159132\n",
            "Accuracy on batch 553 on Training is 76.64147111913357\n",
            "Accuracy on batch 554 on Training is 76.66103603603604\n",
            "Accuracy on batch 555 on Training is 76.64118705035972\n",
            "Accuracy on batch 556 on Training is 76.64946140035907\n",
            "Accuracy on batch 557 on Training is 76.62410394265233\n",
            "Accuracy on batch 558 on Training is 76.61560822898032\n",
            "Accuracy on batch 559 on Training is 76.62946428571429\n",
            "Batch Id 560 is having training loss of 0.8507880568504333\n",
            "0.8693220615386963\n",
            "Accuracy on batch 560 on Training is 76.62098930481284\n",
            "Accuracy on batch 561 on Training is 76.6181049822064\n",
            "Accuracy on batch 562 on Training is 76.60968028419182\n",
            "Accuracy on batch 563 on Training is 76.6123670212766\n",
            "Accuracy on batch 564 on Training is 76.62057522123894\n",
            "Accuracy on batch 565 on Training is 76.6232332155477\n",
            "Accuracy on batch 566 on Training is 76.64241622574956\n",
            "Accuracy on batch 567 on Training is 76.6450264084507\n",
            "Accuracy on batch 568 on Training is 76.65311950790861\n",
            "Accuracy on batch 569 on Training is 76.66666666666667\n",
            "Accuracy on batch 570 on Training is 76.67469352014011\n",
            "Accuracy on batch 571 on Training is 76.69908216783217\n",
            "Accuracy on batch 572 on Training is 76.66884816753927\n",
            "Accuracy on batch 573 on Training is 76.69316202090593\n",
            "Accuracy on batch 574 on Training is 76.67391304347827\n",
            "Accuracy on batch 575 on Training is 76.66558159722223\n",
            "Accuracy on batch 576 on Training is 76.67894280762565\n",
            "Accuracy on batch 577 on Training is 76.69766435986159\n",
            "Accuracy on batch 578 on Training is 76.70552677029362\n",
            "Accuracy on batch 579 on Training is 76.67564655172414\n",
            "Batch Id 580 is having training loss of 0.8484155535697937\n",
            "0.6482073664665222\n",
            "Accuracy on batch 580 on Training is 76.68351979345955\n",
            "Accuracy on batch 581 on Training is 76.6698883161512\n",
            "Accuracy on batch 582 on Training is 76.67238421955403\n",
            "Accuracy on batch 583 on Training is 76.65346746575342\n",
            "Accuracy on batch 584 on Training is 76.66132478632478\n",
            "Accuracy on batch 585 on Training is 76.69048634812286\n",
            "Accuracy on batch 586 on Training is 76.71422487223168\n",
            "Accuracy on batch 587 on Training is 76.72725340136054\n",
            "Accuracy on batch 588 on Training is 76.7243208828523\n",
            "Accuracy on batch 589 on Training is 76.73728813559322\n",
            "Accuracy on batch 590 on Training is 76.707910321489\n",
            "Accuracy on batch 591 on Training is 76.71030405405405\n",
            "Accuracy on batch 592 on Training is 76.72322934232714\n",
            "Accuracy on batch 593 on Training is 76.72558922558923\n",
            "Accuracy on batch 594 on Training is 76.72268907563026\n",
            "Accuracy on batch 595 on Training is 76.70406879194631\n",
            "Accuracy on batch 596 on Training is 76.69597989949749\n",
            "Accuracy on batch 597 on Training is 76.70359531772576\n",
            "Accuracy on batch 598 on Training is 76.71118530884809\n",
            "Accuracy on batch 599 on Training is 76.72395833333333\n",
            "Batch Id 600 is having training loss of 0.8452194929122925\n",
            "0.8235543966293335\n",
            "Accuracy on batch 600 on Training is 76.72108985024958\n",
            "Accuracy on batch 601 on Training is 76.73380398671097\n",
            "Accuracy on batch 602 on Training is 76.7516583747927\n",
            "Accuracy on batch 603 on Training is 76.77462748344371\n",
            "Accuracy on batch 604 on Training is 76.79235537190083\n",
            "Accuracy on batch 605 on Training is 76.80486798679868\n",
            "Accuracy on batch 606 on Training is 76.80704283360791\n",
            "Accuracy on batch 607 on Training is 76.83490953947368\n",
            "Accuracy on batch 608 on Training is 76.84215927750411\n",
            "Accuracy on batch 609 on Training is 76.84426229508196\n",
            "Accuracy on batch 610 on Training is 76.8310147299509\n",
            "Accuracy on batch 611 on Training is 76.81270424836602\n",
            "Accuracy on batch 612 on Training is 76.78935562805873\n",
            "Accuracy on batch 613 on Training is 76.76608306188925\n",
            "Accuracy on batch 614 on Training is 76.7530487804878\n",
            "Accuracy on batch 615 on Training is 76.75020292207792\n",
            "Accuracy on batch 616 on Training is 76.742301458671\n",
            "Accuracy on batch 617 on Training is 76.73442556634305\n",
            "Accuracy on batch 618 on Training is 76.74172051696284\n",
            "Accuracy on batch 619 on Training is 76.73891129032258\n",
            "Batch Id 620 is having training loss of 0.8454310297966003\n",
            "0.8213701844215393\n",
            "Accuracy on batch 620 on Training is 76.74114331723027\n",
            "Accuracy on batch 621 on Training is 76.76346463022509\n",
            "Accuracy on batch 622 on Training is 76.77066613162118\n",
            "Accuracy on batch 623 on Training is 76.79286858974359\n",
            "Accuracy on batch 624 on Training is 76.79\n",
            "Accuracy on batch 625 on Training is 76.79213258785943\n",
            "Accuracy on batch 626 on Training is 76.78429027113238\n",
            "Accuracy on batch 627 on Training is 76.77149681528662\n",
            "Accuracy on batch 628 on Training is 76.77364864864865\n",
            "Accuracy on batch 629 on Training is 76.78571428571429\n",
            "Accuracy on batch 630 on Training is 76.78783676703645\n",
            "Accuracy on batch 631 on Training is 76.76028481012658\n",
            "Accuracy on batch 632 on Training is 76.75750394944708\n",
            "Accuracy on batch 633 on Training is 76.75473186119874\n",
            "Accuracy on batch 634 on Training is 76.75688976377953\n",
            "Accuracy on batch 635 on Training is 76.76395440251572\n",
            "Accuracy on batch 636 on Training is 76.7611852433281\n",
            "Accuracy on batch 637 on Training is 76.74862852664577\n",
            "Accuracy on batch 638 on Training is 76.74589201877934\n",
            "Accuracy on batch 639 on Training is 76.73828125\n",
            "Batch Id 640 is having training loss of 0.8436591029167175\n",
            "0.7934876084327698\n",
            "Accuracy on batch 640 on Training is 76.7306942277691\n",
            "Accuracy on batch 641 on Training is 76.70366043613707\n",
            "Accuracy on batch 642 on Training is 76.69615085536547\n",
            "Accuracy on batch 643 on Training is 76.69836956521739\n",
            "Accuracy on batch 644 on Training is 76.71511627906976\n",
            "Accuracy on batch 645 on Training is 76.72697368421052\n",
            "Accuracy on batch 646 on Training is 76.73396445131375\n",
            "Accuracy on batch 647 on Training is 76.73128858024691\n",
            "Accuracy on batch 648 on Training is 76.70936055469954\n",
            "Accuracy on batch 649 on Training is 76.71634615384616\n",
            "Accuracy on batch 650 on Training is 76.72331029185868\n",
            "Accuracy on batch 651 on Training is 76.73504601226993\n",
            "Accuracy on batch 652 on Training is 76.72281776416538\n",
            "Accuracy on batch 653 on Training is 76.73451834862385\n",
            "Accuracy on batch 654 on Training is 76.73664122137404\n",
            "Accuracy on batch 655 on Training is 76.7578125\n",
            "Accuracy on batch 656 on Training is 76.74562404870625\n",
            "Accuracy on batch 657 on Training is 76.75246960486322\n",
            "Accuracy on batch 658 on Training is 76.75455235204856\n",
            "Accuracy on batch 659 on Training is 76.76136363636364\n",
            "Batch Id 660 is having training loss of 0.842764139175415\n",
            "1.1560908555984497\n",
            "Accuracy on batch 660 on Training is 76.74924357034796\n",
            "Accuracy on batch 661 on Training is 76.76076283987915\n",
            "Accuracy on batch 662 on Training is 76.77696078431373\n",
            "Accuracy on batch 663 on Training is 76.7507530120482\n",
            "Accuracy on batch 664 on Training is 76.75751879699249\n",
            "Accuracy on batch 665 on Training is 76.77364864864865\n",
            "Accuracy on batch 666 on Training is 76.78035982008996\n",
            "Accuracy on batch 667 on Training is 76.77301646706587\n",
            "Accuracy on batch 668 on Training is 76.77970852017937\n",
            "Accuracy on batch 669 on Training is 76.77705223880596\n",
            "Accuracy on batch 670 on Training is 76.75577496274218\n",
            "Accuracy on batch 671 on Training is 76.7578125\n",
            "Accuracy on batch 672 on Training is 76.76448736998515\n",
            "Accuracy on batch 673 on Training is 76.76186943620178\n",
            "Accuracy on batch 674 on Training is 76.74537037037037\n",
            "Accuracy on batch 675 on Training is 76.75203402366864\n",
            "Accuracy on batch 676 on Training is 76.74021418020679\n",
            "Accuracy on batch 677 on Training is 76.74225663716814\n",
            "Accuracy on batch 678 on Training is 76.735088365243\n",
            "Accuracy on batch 679 on Training is 76.74172794117646\n",
            "Batch Id 680 is having training loss of 0.8433749675750732\n",
            "0.7702663540840149\n",
            "Accuracy on batch 680 on Training is 76.74375917767988\n",
            "Accuracy on batch 681 on Training is 76.73203812316716\n",
            "Accuracy on batch 682 on Training is 76.73865300146413\n",
            "Accuracy on batch 683 on Training is 76.73154239766082\n",
            "Accuracy on batch 684 on Training is 76.73357664233576\n",
            "Accuracy on batch 685 on Training is 76.72649416909621\n",
            "Accuracy on batch 686 on Training is 76.72852983988355\n",
            "Accuracy on batch 687 on Training is 76.71693313953489\n",
            "Accuracy on batch 688 on Training is 76.72351233671988\n",
            "Accuracy on batch 689 on Training is 76.72554347826087\n",
            "Accuracy on batch 690 on Training is 76.72756874095514\n",
            "Accuracy on batch 691 on Training is 76.71604046242774\n",
            "Accuracy on batch 692 on Training is 76.71356421356421\n",
            "Accuracy on batch 693 on Training is 76.73360951008645\n",
            "Accuracy on batch 694 on Training is 76.73561151079137\n",
            "Accuracy on batch 695 on Training is 76.73311781609195\n",
            "Accuracy on batch 696 on Training is 76.73959827833572\n",
            "Accuracy on batch 697 on Training is 76.73262893982807\n",
            "Accuracy on batch 698 on Training is 76.73909155937054\n",
            "Accuracy on batch 699 on Training is 76.75892857142857\n",
            "Batch Id 700 is having training loss of 0.8433830142021179\n",
            "1.350630283355713\n",
            "Accuracy on batch 700 on Training is 76.73858773181169\n",
            "Accuracy on batch 701 on Training is 76.74501424501425\n",
            "Accuracy on batch 702 on Training is 76.76920341394026\n",
            "Accuracy on batch 703 on Training is 76.77556818181819\n",
            "Accuracy on batch 704 on Training is 76.73758865248227\n",
            "Accuracy on batch 705 on Training is 76.7528328611898\n",
            "Accuracy on batch 706 on Training is 76.74593352192362\n",
            "Accuracy on batch 707 on Training is 76.74788135593221\n",
            "Accuracy on batch 708 on Training is 76.74100846262341\n",
            "Accuracy on batch 709 on Training is 76.74735915492958\n",
            "Accuracy on batch 710 on Training is 76.74050632911393\n",
            "Accuracy on batch 711 on Training is 76.7380617977528\n",
            "Accuracy on batch 712 on Training is 76.74000701262273\n",
            "Accuracy on batch 713 on Training is 76.7375700280112\n",
            "Accuracy on batch 714 on Training is 76.74388111888112\n",
            "Accuracy on batch 715 on Training is 76.73271648044692\n",
            "Accuracy on batch 716 on Training is 76.71286610878661\n",
            "Accuracy on batch 717 on Training is 76.6974233983287\n",
            "Accuracy on batch 718 on Training is 76.69506258692628\n",
            "Accuracy on batch 719 on Training is 76.68836805555556\n",
            "Batch Id 720 is having training loss of 0.8439735770225525\n",
            "0.7883535027503967\n",
            "Accuracy on batch 720 on Training is 76.69036061026353\n",
            "Accuracy on batch 721 on Training is 76.70100415512465\n",
            "Accuracy on batch 722 on Training is 76.69000691562933\n",
            "Accuracy on batch 723 on Training is 76.68335635359117\n",
            "Accuracy on batch 724 on Training is 76.69396551724138\n",
            "Accuracy on batch 725 on Training is 76.70454545454545\n",
            "Accuracy on batch 726 on Training is 76.70220082530949\n",
            "Accuracy on batch 727 on Training is 76.69986263736264\n",
            "Accuracy on batch 728 on Training is 76.68895747599451\n",
            "Accuracy on batch 729 on Training is 76.70804794520548\n",
            "Accuracy on batch 730 on Training is 76.70571135430916\n",
            "Accuracy on batch 731 on Training is 76.69484289617486\n",
            "Accuracy on batch 732 on Training is 76.68826739427013\n",
            "Accuracy on batch 733 on Training is 76.69873978201635\n",
            "Accuracy on batch 734 on Training is 76.70493197278911\n",
            "Accuracy on batch 735 on Training is 76.71110733695652\n",
            "Accuracy on batch 736 on Training is 76.72150610583446\n",
            "Accuracy on batch 737 on Training is 76.74034552845528\n",
            "Accuracy on batch 738 on Training is 76.74221921515561\n",
            "Accuracy on batch 739 on Training is 76.75253378378379\n",
            "Batch Id 740 is having training loss of 0.8432872295379639\n",
            "0.8990734815597534\n",
            "Accuracy on batch 740 on Training is 76.75016869095816\n",
            "Accuracy on batch 741 on Training is 76.74780997304582\n",
            "Accuracy on batch 742 on Training is 76.73704576043069\n",
            "Accuracy on batch 743 on Training is 76.74311155913979\n",
            "Accuracy on batch 744 on Training is 76.73238255033557\n",
            "Accuracy on batch 745 on Training is 76.74262734584451\n",
            "Accuracy on batch 746 on Training is 76.73611111111111\n",
            "Accuracy on batch 747 on Training is 76.73379010695187\n",
            "Accuracy on batch 748 on Training is 76.72313084112149\n",
            "Accuracy on batch 749 on Training is 76.72916666666667\n",
            "Accuracy on batch 750 on Training is 76.72270306258322\n",
            "Accuracy on batch 751 on Training is 76.71210106382979\n",
            "Accuracy on batch 752 on Training is 76.70567729083665\n",
            "Accuracy on batch 753 on Training is 76.69927055702918\n",
            "Accuracy on batch 754 on Training is 76.6887417218543\n",
            "Accuracy on batch 755 on Training is 76.69477513227513\n",
            "Accuracy on batch 756 on Training is 76.68015191545575\n",
            "Accuracy on batch 757 on Training is 76.67381266490766\n",
            "Accuracy on batch 758 on Training is 76.67160737812912\n",
            "Accuracy on batch 759 on Training is 76.67763157894737\n",
            "Batch Id 760 is having training loss of 0.8441323041915894\n",
            "0.599545419216156\n",
            "Accuracy on batch 760 on Training is 76.67132063074901\n",
            "Accuracy on batch 761 on Training is 76.67732939632546\n",
            "Accuracy on batch 762 on Training is 76.67922673656619\n",
            "Accuracy on batch 763 on Training is 76.66475785340315\n",
            "Accuracy on batch 764 on Training is 76.65849673202614\n",
            "Accuracy on batch 765 on Training is 76.6563315926893\n",
            "Accuracy on batch 766 on Training is 76.64194915254237\n",
            "Accuracy on batch 767 on Training is 76.63981119791667\n",
            "Accuracy on batch 768 on Training is 76.64580624187256\n",
            "Accuracy on batch 769 on Training is 76.64366883116882\n",
            "Accuracy on batch 770 on Training is 76.62937743190662\n",
            "Accuracy on batch 771 on Training is 76.63131476683938\n",
            "Accuracy on batch 772 on Training is 76.62920439844761\n",
            "Accuracy on batch 773 on Training is 76.64324935400516\n",
            "Accuracy on batch 774 on Training is 76.63709677419355\n",
            "Accuracy on batch 775 on Training is 76.63901417525773\n",
            "Accuracy on batch 776 on Training is 76.63288288288288\n",
            "Accuracy on batch 777 on Training is 76.60668380462725\n",
            "Accuracy on batch 778 on Training is 76.60462130937098\n",
            "Accuracy on batch 779 on Training is 76.61858974358974\n",
            "Batch Id 780 is having training loss of 0.8443081378936768\n",
            "0.5948331356048584\n",
            "Accuracy on batch 780 on Training is 76.61651728553137\n",
            "Accuracy on batch 781 on Training is 76.61045396419438\n",
            "Accuracy on batch 782 on Training is 76.61238825031928\n",
            "Accuracy on batch 783 on Training is 76.61431760204081\n",
            "Accuracy on batch 784 on Training is 76.61226114649682\n",
            "Accuracy on batch 785 on Training is 76.61418575063614\n",
            "Accuracy on batch 786 on Training is 76.61213468869123\n",
            "Accuracy on batch 787 on Training is 76.59819162436548\n",
            "Accuracy on batch 788 on Training is 76.60012674271229\n",
            "Accuracy on batch 789 on Training is 76.60996835443038\n",
            "Accuracy on batch 790 on Training is 76.60793299620734\n",
            "Accuracy on batch 791 on Training is 76.62168560606061\n",
            "Accuracy on batch 792 on Training is 76.61569987389659\n",
            "Accuracy on batch 793 on Training is 76.6294080604534\n",
            "Accuracy on batch 794 on Training is 76.61949685534591\n",
            "Accuracy on batch 795 on Training is 76.62531407035176\n",
            "Accuracy on batch 796 on Training is 76.62719573400251\n",
            "Accuracy on batch 797 on Training is 76.63690476190476\n",
            "Accuracy on batch 798 on Training is 76.65832290362954\n",
            "Accuracy on batch 799 on Training is 76.6484375\n",
            "Batch Id 800 is having training loss of 0.8447369933128357\n",
            "1.243957281112671\n",
            "Accuracy on batch 800 on Training is 76.63857677902622\n",
            "Accuracy on batch 801 on Training is 76.6209476309227\n",
            "Accuracy on batch 802 on Training is 76.61114570361146\n",
            "Accuracy on batch 803 on Training is 76.61302860696517\n",
            "Accuracy on batch 804 on Training is 76.62267080745342\n",
            "Accuracy on batch 805 on Training is 76.62453473945409\n",
            "Accuracy on batch 806 on Training is 76.6263940520446\n",
            "Accuracy on batch 807 on Training is 76.63985148514851\n",
            "Accuracy on batch 808 on Training is 76.63396168108777\n",
            "Accuracy on batch 809 on Training is 76.62422839506173\n",
            "Accuracy on batch 810 on Training is 76.62993218249075\n",
            "Accuracy on batch 811 on Training is 76.63177339901478\n",
            "Accuracy on batch 812 on Training is 76.64129766297663\n",
            "Accuracy on batch 813 on Training is 76.65847665847666\n",
            "Accuracy on batch 814 on Training is 76.66411042944786\n",
            "Accuracy on batch 815 on Training is 76.6888786764706\n",
            "Accuracy on batch 816 on Training is 76.69446144430844\n",
            "Accuracy on batch 817 on Training is 76.7000305623472\n",
            "Accuracy on batch 818 on Training is 76.69795482295483\n",
            "Accuracy on batch 819 on Training is 76.67682926829268\n",
            "Batch Id 820 is having training loss of 0.8453958034515381\n",
            "0.9130425453186035\n",
            "Accuracy on batch 820 on Training is 76.67478684531059\n",
            "Accuracy on batch 821 on Training is 76.67274939172749\n",
            "Accuracy on batch 822 on Training is 76.67071688942892\n",
            "Accuracy on batch 823 on Training is 76.68006674757281\n",
            "Accuracy on batch 824 on Training is 76.68181818181819\n",
            "Accuracy on batch 825 on Training is 76.66843220338983\n",
            "Accuracy on batch 826 on Training is 76.68152962515114\n",
            "Accuracy on batch 827 on Training is 76.67572463768116\n",
            "Accuracy on batch 828 on Training is 76.68124246079614\n",
            "Accuracy on batch 829 on Training is 76.67545180722891\n",
            "Accuracy on batch 830 on Training is 76.68095667870035\n",
            "Accuracy on batch 831 on Training is 76.67142427884616\n",
            "Accuracy on batch 832 on Training is 76.6656662665066\n",
            "Accuracy on batch 833 on Training is 76.66741606714628\n",
            "Accuracy on batch 834 on Training is 76.66541916167665\n",
            "Accuracy on batch 835 on Training is 76.6596889952153\n",
            "Accuracy on batch 836 on Training is 76.65023894862604\n",
            "Accuracy on batch 837 on Training is 76.64454057279237\n",
            "Accuracy on batch 838 on Training is 76.64258045292014\n",
            "Accuracy on batch 839 on Training is 76.640625\n",
            "Batch Id 840 is having training loss of 0.8459886908531189\n",
            "0.472652792930603\n",
            "Accuracy on batch 840 on Training is 76.64982164090368\n",
            "Accuracy on batch 841 on Training is 76.64043942992875\n",
            "Accuracy on batch 842 on Training is 76.64220047449585\n",
            "Accuracy on batch 843 on Training is 76.64765995260663\n",
            "Accuracy on batch 844 on Training is 76.64940828402366\n",
            "Accuracy on batch 845 on Training is 76.64376477541371\n",
            "Accuracy on batch 846 on Training is 76.65289256198348\n",
            "Accuracy on batch 847 on Training is 76.66199882075472\n",
            "Accuracy on batch 848 on Training is 76.66372202591283\n",
            "Accuracy on batch 849 on Training is 76.66911764705883\n",
            "Accuracy on batch 850 on Training is 76.67817273795535\n",
            "Accuracy on batch 851 on Training is 76.68353873239437\n",
            "Accuracy on batch 852 on Training is 76.68889214536928\n",
            "Accuracy on batch 853 on Training is 76.69423302107728\n",
            "Accuracy on batch 854 on Training is 76.703216374269\n",
            "Accuracy on batch 855 on Training is 76.71217873831776\n",
            "Accuracy on batch 856 on Training is 76.71382730455076\n",
            "Accuracy on batch 857 on Training is 76.72275641025641\n",
            "Accuracy on batch 858 on Training is 76.7243888242142\n",
            "Accuracy on batch 859 on Training is 76.7296511627907\n",
            "Batch Id 860 is having training loss of 0.8434375524520874\n",
            "0.7091617584228516\n",
            "Accuracy on batch 860 on Training is 76.74216027874564\n",
            "Accuracy on batch 861 on Training is 76.73288863109049\n",
            "Accuracy on batch 862 on Training is 76.72725955967555\n",
            "Accuracy on batch 863 on Training is 76.73611111111111\n",
            "Accuracy on batch 864 on Training is 76.73049132947978\n",
            "Accuracy on batch 865 on Training is 76.73210161662817\n",
            "Accuracy on batch 866 on Training is 76.7409169550173\n",
            "Accuracy on batch 867 on Training is 76.74971198156682\n",
            "Accuracy on batch 868 on Training is 76.75848676639816\n",
            "Accuracy on batch 869 on Training is 76.74209770114942\n",
            "Accuracy on batch 870 on Training is 76.74727324913891\n",
            "Accuracy on batch 871 on Training is 76.7416857798165\n",
            "Accuracy on batch 872 on Training is 76.71821305841924\n",
            "Accuracy on batch 873 on Training is 76.71982265446225\n",
            "Accuracy on batch 874 on Training is 76.72142857142858\n",
            "Accuracy on batch 875 on Training is 76.7230308219178\n",
            "Accuracy on batch 876 on Training is 76.73175598631698\n",
            "Accuracy on batch 877 on Training is 76.73334282460137\n",
            "Accuracy on batch 878 on Training is 76.73137087599545\n",
            "Accuracy on batch 879 on Training is 76.74360795454545\n",
            "Batch Id 880 is having training loss of 0.8438143134117126\n",
            "0.9480767250061035\n",
            "Accuracy on batch 880 on Training is 76.73098751418843\n",
            "Accuracy on batch 881 on Training is 76.71839569160997\n",
            "Accuracy on batch 882 on Training is 76.71644960362401\n",
            "Accuracy on batch 883 on Training is 76.71097285067873\n",
            "Accuracy on batch 884 on Training is 76.7090395480226\n",
            "Accuracy on batch 885 on Training is 76.69300225733635\n",
            "Accuracy on batch 886 on Training is 76.69109357384441\n",
            "Accuracy on batch 887 on Training is 76.69974662162163\n",
            "Accuracy on batch 888 on Training is 76.70134983127109\n",
            "Accuracy on batch 889 on Training is 76.71348314606742\n",
            "Accuracy on batch 890 on Training is 76.69402356902357\n",
            "Accuracy on batch 891 on Training is 76.69913116591928\n",
            "Accuracy on batch 892 on Training is 76.6832306830907\n",
            "Accuracy on batch 893 on Training is 76.68484340044743\n",
            "Accuracy on batch 894 on Training is 76.68994413407822\n",
            "Accuracy on batch 895 on Training is 76.67759486607143\n",
            "Accuracy on batch 896 on Training is 76.66527313266444\n",
            "Accuracy on batch 897 on Training is 76.67385857461025\n",
            "Accuracy on batch 898 on Training is 76.67547274749722\n",
            "Accuracy on batch 899 on Training is 76.67708333333333\n",
            "Batch Id 900 is having training loss of 0.8440033793449402\n",
            "0.6956122517585754\n",
            "Accuracy on batch 900 on Training is 76.68562708102108\n",
            "Accuracy on batch 901 on Training is 76.69415188470066\n",
            "Accuracy on batch 902 on Training is 76.69573643410853\n",
            "Accuracy on batch 903 on Training is 76.69731747787611\n",
            "Accuracy on batch 904 on Training is 76.68853591160222\n",
            "Accuracy on batch 905 on Training is 76.6832229580574\n",
            "Accuracy on batch 906 on Training is 76.66069459757442\n",
            "Accuracy on batch 907 on Training is 76.65198237885463\n",
            "Accuracy on batch 908 on Training is 76.64672717271728\n",
            "Accuracy on batch 909 on Training is 76.64835164835165\n",
            "Accuracy on batch 910 on Training is 76.64997255762898\n",
            "Accuracy on batch 911 on Training is 76.64473684210526\n",
            "Accuracy on batch 912 on Training is 76.64978094194961\n",
            "Accuracy on batch 913 on Training is 76.64797592997812\n",
            "Accuracy on batch 914 on Training is 76.63592896174863\n",
            "Accuracy on batch 915 on Training is 76.63073144104804\n",
            "Accuracy on batch 916 on Training is 76.62895310796074\n",
            "Accuracy on batch 917 on Training is 76.63739106753813\n",
            "Accuracy on batch 918 on Training is 76.62540805223068\n",
            "Accuracy on batch 919 on Training is 76.63722826086956\n",
            "Batch Id 920 is having training loss of 0.8439152836799622\n",
            "0.644182562828064\n",
            "Accuracy on batch 920 on Training is 76.64223669923996\n",
            "Accuracy on batch 921 on Training is 76.64723427331887\n",
            "Accuracy on batch 922 on Training is 76.64544962080173\n",
            "Accuracy on batch 923 on Training is 76.65381493506493\n",
            "Accuracy on batch 924 on Training is 76.65878378378379\n",
            "Accuracy on batch 925 on Training is 76.66711663066954\n",
            "Accuracy on batch 926 on Training is 76.66531823085221\n",
            "Accuracy on batch 927 on Training is 76.65342133620689\n",
            "Accuracy on batch 928 on Training is 76.66509687836383\n",
            "Accuracy on batch 929 on Training is 76.67338709677419\n",
            "Accuracy on batch 930 on Training is 76.66823308270676\n",
            "Accuracy on batch 931 on Training is 76.66979613733906\n",
            "Accuracy on batch 932 on Training is 76.67135584137192\n",
            "Accuracy on batch 933 on Training is 76.66956638115632\n",
            "Accuracy on batch 934 on Training is 76.66109625668449\n",
            "Accuracy on batch 935 on Training is 76.66266025641026\n",
            "Accuracy on batch 936 on Training is 76.66422091782285\n",
            "Accuracy on batch 937 on Training is 76.66577825159915\n",
            "Accuracy on batch 938 on Training is 76.6706602768903\n",
            "Accuracy on batch 939 on Training is 76.67220744680851\n",
            "Batch Id 940 is having training loss of 0.8432552218437195\n",
            "0.9748069643974304\n",
            "Accuracy on batch 940 on Training is 76.66710945802338\n",
            "Accuracy on batch 941 on Training is 76.66202229299363\n",
            "Accuracy on batch 942 on Training is 76.66025980911984\n",
            "Accuracy on batch 943 on Training is 76.67505296610169\n",
            "Accuracy on batch 944 on Training is 76.68981481481481\n",
            "Accuracy on batch 945 on Training is 76.6946353065539\n",
            "Accuracy on batch 946 on Training is 76.68624604012672\n",
            "Accuracy on batch 947 on Training is 76.67457805907173\n",
            "Accuracy on batch 948 on Training is 76.67610642781875\n",
            "Accuracy on batch 949 on Training is 76.67105263157895\n",
            "Accuracy on batch 950 on Training is 76.68243953732913\n",
            "Accuracy on batch 951 on Training is 76.68067226890756\n",
            "Accuracy on batch 952 on Training is 76.68218782791186\n",
            "Accuracy on batch 953 on Training is 76.69680293501048\n",
            "Accuracy on batch 954 on Training is 76.70484293193718\n",
            "Accuracy on batch 955 on Training is 76.70632845188284\n",
            "Accuracy on batch 956 on Training is 76.71434169278997\n",
            "Accuracy on batch 957 on Training is 76.71907620041753\n",
            "Accuracy on batch 958 on Training is 76.7238008342023\n",
            "Accuracy on batch 959 on Training is 76.728515625\n",
            "Batch Id 960 is having training loss of 0.8418935537338257\n",
            "0.5692824125289917\n",
            "Accuracy on batch 960 on Training is 76.72996878251821\n",
            "Accuracy on batch 961 on Training is 76.73466735966736\n",
            "Accuracy on batch 962 on Training is 76.74584631360332\n",
            "Accuracy on batch 963 on Training is 76.73755186721992\n",
            "Accuracy on batch 964 on Training is 76.75194300518135\n",
            "Accuracy on batch 965 on Training is 76.7468944099379\n",
            "Accuracy on batch 966 on Training is 76.75801447776628\n",
            "Accuracy on batch 967 on Training is 76.74651342975207\n",
            "Accuracy on batch 968 on Training is 76.73826109391125\n",
            "Accuracy on batch 969 on Training is 76.75257731958763\n",
            "Accuracy on batch 970 on Training is 76.75077239958806\n",
            "Accuracy on batch 971 on Training is 76.74897119341564\n",
            "Accuracy on batch 972 on Training is 76.74717368961973\n",
            "Accuracy on batch 973 on Training is 76.75179671457906\n",
            "Accuracy on batch 974 on Training is 76.75961538461539\n",
            "Accuracy on batch 975 on Training is 76.76101434426229\n",
            "Accuracy on batch 976 on Training is 76.76560900716478\n",
            "Accuracy on batch 977 on Training is 76.77658486707567\n",
            "Accuracy on batch 978 on Training is 76.77157814096016\n",
            "Accuracy on batch 979 on Training is 76.77614795918367\n",
            "Batch Id 980 is having training loss of 0.841184139251709\n",
            "1.1144969463348389\n",
            "Accuracy on batch 980 on Training is 76.77115188583079\n",
            "Accuracy on batch 981 on Training is 76.75980142566192\n",
            "Accuracy on batch 982 on Training is 76.77072736520854\n",
            "Accuracy on batch 983 on Training is 76.75940040650407\n",
            "Accuracy on batch 984 on Training is 76.75444162436548\n",
            "Accuracy on batch 985 on Training is 76.74632352941177\n",
            "Accuracy on batch 986 on Training is 76.74455420466059\n",
            "Accuracy on batch 987 on Training is 76.74911437246963\n",
            "Accuracy on batch 988 on Training is 76.75366531850354\n",
            "Accuracy on batch 989 on Training is 76.7550505050505\n",
            "Accuracy on batch 990 on Training is 76.75012613521696\n",
            "Accuracy on batch 991 on Training is 76.76411290322581\n",
            "Accuracy on batch 992 on Training is 76.77492447129909\n",
            "Accuracy on batch 993 on Training is 76.76999496981891\n",
            "Accuracy on batch 994 on Training is 76.77449748743719\n",
            "Accuracy on batch 995 on Training is 76.77585341365462\n",
            "Accuracy on batch 996 on Training is 76.77720661985958\n",
            "Accuracy on batch 997 on Training is 76.76916332665331\n",
            "Accuracy on batch 998 on Training is 76.77990490490491\n",
            "Accuracy on batch 999 on Training is 76.778125\n",
            "Batch Id 1000 is having training loss of 0.8409057259559631\n",
            "0.46151649951934814\n",
            "Accuracy on batch 1000 on Training is 76.78571428571429\n",
            "Accuracy on batch 1001 on Training is 76.78705089820359\n",
            "Accuracy on batch 1002 on Training is 76.78215353938185\n",
            "Accuracy on batch 1003 on Training is 76.78349103585657\n",
            "Accuracy on batch 1004 on Training is 76.7723880597015\n",
            "Accuracy on batch 1005 on Training is 76.76441351888668\n",
            "Accuracy on batch 1006 on Training is 76.75955809334657\n",
            "Accuracy on batch 1007 on Training is 76.7578125\n",
            "Accuracy on batch 1008 on Training is 76.7529732408325\n",
            "Accuracy on batch 1009 on Training is 76.75433168316832\n",
            "Accuracy on batch 1010 on Training is 76.75259643916914\n",
            "Accuracy on batch 1011 on Training is 76.73542490118577\n",
            "Accuracy on batch 1012 on Training is 76.72445705824285\n",
            "Accuracy on batch 1013 on Training is 76.71659270216962\n",
            "Accuracy on batch 1014 on Training is 76.7179802955665\n",
            "Accuracy on batch 1015 on Training is 76.72551673228347\n",
            "Accuracy on batch 1016 on Training is 76.72689282202556\n",
            "Accuracy on batch 1017 on Training is 76.73133595284872\n",
            "Accuracy on batch 1018 on Training is 76.72657016683023\n",
            "Accuracy on batch 1019 on Training is 76.73406862745098\n",
            "Batch Id 1020 is having training loss of 0.8425810933113098\n",
            "0.7395481467247009\n",
            "Accuracy on batch 1020 on Training is 76.72930950048972\n",
            "Accuracy on batch 1021 on Training is 76.72761741682974\n",
            "Accuracy on batch 1022 on Training is 76.7289833822092\n",
            "Accuracy on batch 1023 on Training is 76.727294921875\n",
            "Accuracy on batch 1024 on Training is 76.71341463414635\n",
            "Accuracy on batch 1025 on Training is 76.70869883040936\n",
            "Accuracy on batch 1026 on Training is 76.70703505355404\n",
            "Accuracy on batch 1027 on Training is 76.71449416342412\n",
            "Accuracy on batch 1028 on Training is 76.72497570456754\n",
            "Accuracy on batch 1029 on Training is 76.72330097087378\n",
            "Accuracy on batch 1030 on Training is 76.71556741028128\n",
            "Accuracy on batch 1031 on Training is 76.72298934108527\n",
            "Accuracy on batch 1032 on Training is 76.72132139399807\n",
            "Accuracy on batch 1033 on Training is 76.73476789168278\n",
            "Accuracy on batch 1034 on Training is 76.73611111111111\n",
            "Accuracy on batch 1035 on Training is 76.73443532818533\n",
            "Accuracy on batch 1036 on Training is 76.73878977820637\n",
            "Accuracy on batch 1037 on Training is 76.72808285163777\n",
            "Accuracy on batch 1038 on Training is 76.72641963426372\n",
            "Accuracy on batch 1039 on Training is 76.72475961538461\n",
            "Batch Id 1040 is having training loss of 0.8428199291229248\n",
            "0.7992459535598755\n",
            "Accuracy on batch 1040 on Training is 76.72610470701248\n",
            "Accuracy on batch 1041 on Training is 76.7274472168906\n",
            "Accuracy on batch 1042 on Training is 76.72878715244487\n",
            "Accuracy on batch 1043 on Training is 76.73611111111111\n",
            "Accuracy on batch 1044 on Training is 76.73744019138756\n",
            "Accuracy on batch 1045 on Training is 76.73876673040154\n",
            "Accuracy on batch 1046 on Training is 76.73412129894938\n",
            "Accuracy on batch 1047 on Training is 76.74737595419847\n",
            "Accuracy on batch 1048 on Training is 76.7516682554814\n",
            "Accuracy on batch 1049 on Training is 76.75297619047619\n",
            "Accuracy on batch 1050 on Training is 76.76022835394862\n",
            "Accuracy on batch 1051 on Training is 76.76152566539923\n",
            "Accuracy on batch 1052 on Training is 76.7687559354226\n",
            "Accuracy on batch 1053 on Training is 76.78783206831119\n",
            "Accuracy on batch 1054 on Training is 76.77132701421802\n",
            "Accuracy on batch 1055 on Training is 76.77556818181819\n",
            "Accuracy on batch 1056 on Training is 76.77980132450331\n",
            "Accuracy on batch 1057 on Training is 76.78402646502836\n",
            "Accuracy on batch 1058 on Training is 76.79119452313503\n",
            "Accuracy on batch 1059 on Training is 76.79834905660377\n",
            "Batch Id 1060 is having training loss of 0.8404365181922913\n",
            "0.9786983728408813\n",
            "Accuracy on batch 1060 on Training is 76.79665409990575\n",
            "Accuracy on batch 1061 on Training is 76.79790489642184\n",
            "Accuracy on batch 1062 on Training is 76.78739416745061\n",
            "Accuracy on batch 1063 on Training is 76.80333646616542\n",
            "Accuracy on batch 1064 on Training is 76.81338028169014\n",
            "Accuracy on batch 1065 on Training is 76.81754221388368\n",
            "Accuracy on batch 1066 on Training is 76.81583880037488\n",
            "Accuracy on batch 1067 on Training is 76.81999063670412\n",
            "Accuracy on batch 1068 on Training is 76.82121141253508\n",
            "Accuracy on batch 1069 on Training is 76.81658878504673\n",
            "Accuracy on batch 1070 on Training is 76.79738562091504\n",
            "Accuracy on batch 1071 on Training is 76.80736940298507\n",
            "Accuracy on batch 1072 on Training is 76.81150978564771\n",
            "Accuracy on batch 1073 on Training is 76.82146182495345\n",
            "Accuracy on batch 1074 on Training is 76.81395348837209\n",
            "Accuracy on batch 1075 on Training is 76.82678903345725\n",
            "Accuracy on batch 1076 on Training is 76.82799442896936\n",
            "Accuracy on batch 1077 on Training is 76.81470315398887\n",
            "Accuracy on batch 1078 on Training is 76.82750231696015\n",
            "Accuracy on batch 1079 on Training is 76.82870370370371\n",
            "Batch Id 1080 is having training loss of 0.8397611379623413\n",
            "1.2622743844985962\n",
            "Accuracy on batch 1080 on Training is 76.8154486586494\n",
            "Accuracy on batch 1081 on Training is 76.81665896487985\n",
            "Accuracy on batch 1082 on Training is 76.82075253924285\n",
            "Accuracy on batch 1083 on Training is 76.82772140221402\n",
            "Accuracy on batch 1084 on Training is 76.83179723502305\n",
            "Accuracy on batch 1085 on Training is 76.83586556169429\n",
            "Accuracy on batch 1086 on Training is 76.83992640294389\n",
            "Accuracy on batch 1087 on Training is 76.84397977941177\n",
            "Accuracy on batch 1088 on Training is 76.83941689623508\n",
            "Accuracy on batch 1089 on Training is 76.8520642201835\n",
            "Accuracy on batch 1090 on Training is 76.85036663611366\n",
            "Accuracy on batch 1091 on Training is 76.86011904761905\n",
            "Accuracy on batch 1092 on Training is 76.87271271729186\n",
            "Accuracy on batch 1093 on Training is 76.87671389396709\n",
            "Accuracy on batch 1094 on Training is 76.86929223744292\n",
            "Accuracy on batch 1095 on Training is 76.87328923357664\n",
            "Accuracy on batch 1096 on Training is 76.8630355515041\n",
            "Accuracy on batch 1097 on Training is 76.8528005464481\n",
            "Accuracy on batch 1098 on Training is 76.848271155596\n",
            "Accuracy on batch 1099 on Training is 76.86363636363636\n",
            "Batch Id 1100 is having training loss of 0.8388364911079407\n",
            "0.6709930300712585\n",
            "Accuracy on batch 1100 on Training is 76.87045867393279\n",
            "Accuracy on batch 1101 on Training is 76.86025408348458\n",
            "Accuracy on batch 1102 on Training is 76.85856754306437\n",
            "Accuracy on batch 1103 on Training is 76.85688405797102\n",
            "Accuracy on batch 1104 on Training is 76.85237556561086\n",
            "Accuracy on batch 1105 on Training is 76.85070072332731\n",
            "Accuracy on batch 1106 on Training is 76.85185185185185\n",
            "Accuracy on batch 1107 on Training is 76.86428249097473\n",
            "Accuracy on batch 1108 on Training is 76.86260144274121\n",
            "Accuracy on batch 1109 on Training is 76.86092342342343\n",
            "Accuracy on batch 1110 on Training is 76.86768676867686\n",
            "Accuracy on batch 1111 on Training is 76.86881744604317\n",
            "Accuracy on batch 1112 on Training is 76.87275381850854\n",
            "Accuracy on batch 1113 on Training is 76.88229353680431\n",
            "Accuracy on batch 1114 on Training is 76.88621076233184\n",
            "Accuracy on batch 1115 on Training is 76.89012096774194\n",
            "Accuracy on batch 1116 on Training is 76.89122649955237\n",
            "Accuracy on batch 1117 on Training is 76.89792039355993\n",
            "Accuracy on batch 1118 on Training is 76.90180965147454\n",
            "Accuracy on batch 1119 on Training is 76.89453125\n",
            "Batch Id 1120 is having training loss of 0.8386631011962891\n",
            "1.0763593912124634\n",
            "Accuracy on batch 1120 on Training is 76.90120428189117\n",
            "Accuracy on batch 1121 on Training is 76.90229500891266\n",
            "Accuracy on batch 1122 on Training is 76.90338379341051\n",
            "Accuracy on batch 1123 on Training is 76.89891014234875\n",
            "Accuracy on batch 1124 on Training is 76.90277777777777\n",
            "Accuracy on batch 1125 on Training is 76.89831261101243\n",
            "Accuracy on batch 1126 on Training is 76.90771960958297\n",
            "Accuracy on batch 1127 on Training is 76.91710992907801\n",
            "Accuracy on batch 1128 on Training is 76.91541186891054\n",
            "Accuracy on batch 1129 on Training is 76.91924778761062\n",
            "Accuracy on batch 1130 on Training is 76.92307692307692\n",
            "Accuracy on batch 1131 on Training is 76.9241386925795\n",
            "Accuracy on batch 1132 on Training is 76.93623124448366\n",
            "Accuracy on batch 1133 on Training is 76.9372795414462\n",
            "Accuracy on batch 1134 on Training is 76.93832599118943\n",
            "Accuracy on batch 1135 on Training is 76.94762323943662\n",
            "Accuracy on batch 1136 on Training is 76.94316182937555\n",
            "Accuracy on batch 1137 on Training is 76.94145430579965\n",
            "Accuracy on batch 1138 on Training is 76.94249341527656\n",
            "Accuracy on batch 1139 on Training is 76.95175438596492\n",
            "Batch Id 1140 is having training loss of 0.8362147212028503\n",
            "0.5324621200561523\n",
            "Accuracy on batch 1140 on Training is 76.95826029798422\n",
            "Accuracy on batch 1141 on Training is 76.9592819614711\n",
            "Accuracy on batch 1142 on Training is 76.96030183727034\n",
            "Accuracy on batch 1143 on Training is 76.96678321678321\n",
            "Accuracy on batch 1144 on Training is 76.9650655021834\n",
            "Accuracy on batch 1145 on Training is 76.96062390924956\n",
            "Accuracy on batch 1146 on Training is 76.97253705318221\n",
            "Accuracy on batch 1147 on Training is 76.97081881533101\n",
            "Accuracy on batch 1148 on Training is 76.96910356832028\n",
            "Accuracy on batch 1149 on Training is 76.97010869565217\n",
            "Accuracy on batch 1150 on Training is 76.96568201563858\n",
            "Accuracy on batch 1151 on Training is 76.96397569444444\n",
            "Accuracy on batch 1152 on Training is 76.95956201214224\n",
            "Accuracy on batch 1153 on Training is 76.94974003466204\n",
            "Accuracy on batch 1154 on Training is 76.94534632034632\n",
            "Accuracy on batch 1155 on Training is 76.95177335640139\n",
            "Accuracy on batch 1156 on Training is 76.95278738115817\n",
            "Accuracy on batch 1157 on Training is 76.95379965457686\n",
            "Accuracy on batch 1158 on Training is 76.96289905090596\n",
            "Accuracy on batch 1159 on Training is 76.953125\n",
            "Batch Id 1160 is having training loss of 0.8360865712165833\n",
            "1.2958381175994873\n",
            "Accuracy on batch 1160 on Training is 76.93798449612403\n",
            "Accuracy on batch 1161 on Training is 76.94169535283993\n",
            "Accuracy on batch 1162 on Training is 76.95346087704213\n",
            "Accuracy on batch 1163 on Training is 76.95715206185567\n",
            "Accuracy on batch 1164 on Training is 76.9554721030043\n",
            "Accuracy on batch 1165 on Training is 76.95647512864494\n",
            "Accuracy on batch 1166 on Training is 76.9521208226221\n",
            "Accuracy on batch 1167 on Training is 76.95044948630137\n",
            "Accuracy on batch 1168 on Training is 76.95412745936699\n",
            "Accuracy on batch 1169 on Training is 76.95245726495726\n",
            "Accuracy on batch 1170 on Training is 76.95078992314261\n",
            "Accuracy on batch 1171 on Training is 76.95179180887372\n",
            "Accuracy on batch 1172 on Training is 76.95545609548167\n",
            "Accuracy on batch 1173 on Training is 76.9484667802385\n",
            "Accuracy on batch 1174 on Training is 76.94680851063829\n",
            "Accuracy on batch 1175 on Training is 76.953125\n",
            "Accuracy on batch 1176 on Training is 76.95677570093459\n",
            "Accuracy on batch 1177 on Training is 76.96042020373514\n",
            "Accuracy on batch 1178 on Training is 76.97466072943172\n",
            "Accuracy on batch 1179 on Training is 76.96239406779661\n",
            "Batch Id 1180 is having training loss of 0.8355010747909546\n",
            "0.8216878175735474\n",
            "Accuracy on batch 1180 on Training is 76.96602455546147\n",
            "Accuracy on batch 1181 on Training is 76.97229272419628\n",
            "Accuracy on batch 1182 on Training is 76.97855029585799\n",
            "Accuracy on batch 1183 on Training is 76.9821579391892\n",
            "Accuracy on batch 1184 on Training is 76.98048523206751\n",
            "Accuracy on batch 1185 on Training is 76.97881534569983\n",
            "Accuracy on batch 1186 on Training is 76.97714827295704\n",
            "Accuracy on batch 1187 on Training is 76.98074494949495\n",
            "Accuracy on batch 1188 on Training is 76.98433557611438\n",
            "Accuracy on batch 1189 on Training is 76.98004201680672\n",
            "Accuracy on batch 1190 on Training is 76.97837951301427\n",
            "Accuracy on batch 1191 on Training is 76.99244966442953\n",
            "Accuracy on batch 1192 on Training is 76.99339899413243\n",
            "Accuracy on batch 1193 on Training is 77.00219849246231\n",
            "Accuracy on batch 1194 on Training is 76.9979079497908\n",
            "Accuracy on batch 1195 on Training is 77.0066889632107\n",
            "Accuracy on batch 1196 on Training is 77.00501253132832\n",
            "Accuracy on batch 1197 on Training is 77.0033388981636\n",
            "Accuracy on batch 1198 on Training is 76.99384904086739\n",
            "Accuracy on batch 1199 on Training is 76.99739583333333\n",
            "Batch Id 1200 is having training loss of 0.8350135684013367\n",
            "1.5297282934188843\n",
            "Accuracy on batch 1200 on Training is 76.98272273105745\n",
            "Accuracy on batch 1201 on Training is 76.97847337770382\n",
            "Accuracy on batch 1202 on Training is 76.9690357439734\n",
            "Accuracy on batch 1203 on Training is 76.97518687707641\n",
            "Accuracy on batch 1204 on Training is 76.96836099585062\n",
            "Accuracy on batch 1205 on Training is 76.97450248756219\n",
            "Accuracy on batch 1206 on Training is 76.96768848384424\n",
            "Accuracy on batch 1207 on Training is 76.9738203642384\n",
            "Accuracy on batch 1208 on Training is 76.97477253928867\n",
            "Accuracy on batch 1209 on Training is 76.97572314049587\n",
            "Accuracy on batch 1210 on Training is 76.9740916597853\n",
            "Accuracy on batch 1211 on Training is 76.98277640264027\n",
            "Accuracy on batch 1212 on Training is 76.98371805441056\n",
            "Accuracy on batch 1213 on Training is 76.99238056013179\n",
            "Accuracy on batch 1214 on Training is 76.98559670781893\n",
            "Accuracy on batch 1215 on Training is 76.98396381578948\n",
            "Accuracy on batch 1216 on Training is 76.98490139687756\n",
            "Accuracy on batch 1217 on Training is 76.99096880131363\n",
            "Accuracy on batch 1218 on Training is 76.99702625102543\n",
            "Accuracy on batch 1219 on Training is 77.00307377049181\n",
            "Batch Id 1220 is having training loss of 0.8333421349525452\n",
            "0.6910742521286011\n",
            "Accuracy on batch 1220 on Training is 77.00399262899263\n",
            "Accuracy on batch 1221 on Training is 77.002352700491\n",
            "Accuracy on batch 1222 on Training is 77.00071545380213\n",
            "Accuracy on batch 1223 on Training is 76.99908088235294\n",
            "Accuracy on batch 1224 on Training is 77.0\n",
            "Accuracy on batch 1225 on Training is 77.00856443719412\n",
            "Accuracy on batch 1226 on Training is 77.01202118989406\n",
            "Accuracy on batch 1227 on Training is 77.01292752442997\n",
            "Accuracy on batch 1228 on Training is 77.0163751017087\n",
            "Accuracy on batch 1229 on Training is 77.01473577235772\n",
            "Accuracy on batch 1230 on Training is 77.01563769293257\n",
            "Accuracy on batch 1231 on Training is 77.0216112012987\n",
            "Accuracy on batch 1232 on Training is 77.02250608272506\n",
            "Accuracy on batch 1233 on Training is 77.02086709886548\n",
            "Accuracy on batch 1234 on Training is 77.02429149797571\n",
            "Accuracy on batch 1235 on Training is 77.03276699029126\n",
            "Accuracy on batch 1236 on Training is 77.03364995957963\n",
            "Accuracy on batch 1237 on Training is 77.03705573505654\n",
            "Accuracy on batch 1238 on Training is 77.04045601291364\n",
            "Accuracy on batch 1239 on Training is 77.0539314516129\n",
            "Batch Id 1240 is having training loss of 0.830947995185852\n",
            "0.7643166780471802\n",
            "Accuracy on batch 1240 on Training is 77.05479452054794\n",
            "Accuracy on batch 1241 on Training is 77.05062399355877\n",
            "Accuracy on batch 1242 on Training is 77.06405872888173\n",
            "Accuracy on batch 1243 on Training is 77.06742363344051\n",
            "Accuracy on batch 1244 on Training is 77.06576305220884\n",
            "Accuracy on batch 1245 on Training is 77.07162921348315\n",
            "Accuracy on batch 1246 on Training is 77.0674619085806\n",
            "Accuracy on batch 1247 on Training is 77.05829326923077\n",
            "Accuracy on batch 1248 on Training is 77.04913931144915\n",
            "Accuracy on batch 1249 on Training is 77.0625\n",
            "Accuracy on batch 1250 on Training is 77.06085131894484\n",
            "Accuracy on batch 1251 on Training is 77.05421325878594\n",
            "Accuracy on batch 1252 on Training is 77.05756185155626\n",
            "Accuracy on batch 1253 on Training is 77.04346092503987\n",
            "Accuracy on batch 1254 on Training is 77.04930278884463\n",
            "Accuracy on batch 1255 on Training is 77.03523089171975\n",
            "Accuracy on batch 1256 on Training is 77.04355608591885\n",
            "Accuracy on batch 1257 on Training is 77.04193163751987\n",
            "Accuracy on batch 1258 on Training is 77.03782764098491\n",
            "Accuracy on batch 1259 on Training is 77.03373015873017\n",
            "Batch Id 1260 is having training loss of 0.8305259943008423\n",
            "0.802418053150177\n",
            "Accuracy on batch 1260 on Training is 77.02963917525773\n",
            "Accuracy on batch 1261 on Training is 77.03298335974644\n",
            "Accuracy on batch 1262 on Training is 77.03384798099762\n",
            "Accuracy on batch 1263 on Training is 77.04460047468355\n",
            "Accuracy on batch 1264 on Training is 77.05286561264822\n",
            "Accuracy on batch 1265 on Training is 77.05864928909952\n",
            "Accuracy on batch 1266 on Training is 77.06442383583267\n",
            "Accuracy on batch 1267 on Training is 77.06279574132492\n",
            "Accuracy on batch 1268 on Training is 77.06363278171789\n",
            "Accuracy on batch 1269 on Training is 77.05462598425197\n",
            "Accuracy on batch 1270 on Training is 77.06038552321007\n",
            "Accuracy on batch 1271 on Training is 77.06367924528301\n",
            "Accuracy on batch 1272 on Training is 77.06942262372348\n",
            "Accuracy on batch 1273 on Training is 77.07025117739404\n",
            "Accuracy on batch 1274 on Training is 77.06617647058823\n",
            "Accuracy on batch 1275 on Training is 77.06210815047022\n",
            "Accuracy on batch 1276 on Training is 77.05315191855912\n",
            "Accuracy on batch 1277 on Training is 77.05154538341158\n",
            "Accuracy on batch 1278 on Training is 77.04994136043784\n",
            "Accuracy on batch 1279 on Training is 77.0556640625\n",
            "Batch Id 1280 is having training loss of 0.8292434215545654\n",
            "0.6045032739639282\n",
            "Accuracy on batch 1280 on Training is 77.06137782982046\n",
            "Accuracy on batch 1281 on Training is 77.07195787831513\n",
            "Accuracy on batch 1282 on Training is 77.07765003897116\n",
            "Accuracy on batch 1283 on Training is 77.07603193146417\n",
            "Accuracy on batch 1284 on Training is 77.07441634241245\n",
            "Accuracy on batch 1285 on Training is 77.0703732503888\n",
            "Accuracy on batch 1286 on Training is 77.06148018648018\n",
            "Accuracy on batch 1287 on Training is 77.06715838509317\n",
            "Accuracy on batch 1288 on Training is 77.06797905352987\n",
            "Accuracy on batch 1289 on Training is 77.07848837209302\n",
            "Accuracy on batch 1290 on Training is 77.08898140975988\n",
            "Accuracy on batch 1291 on Training is 77.08494582043343\n",
            "Accuracy on batch 1292 on Training is 77.0857501933488\n",
            "Accuracy on batch 1293 on Training is 77.07447836166924\n",
            "Accuracy on batch 1294 on Training is 77.08011583011583\n",
            "Accuracy on batch 1295 on Training is 77.08092206790124\n",
            "Accuracy on batch 1296 on Training is 77.08895528141866\n",
            "Accuracy on batch 1297 on Training is 77.0945685670262\n",
            "Accuracy on batch 1298 on Training is 77.09055042340262\n",
            "Accuracy on batch 1299 on Training is 77.08173076923077\n",
            "Batch Id 1300 is having training loss of 0.8275025486946106\n",
            "0.8086199164390564\n",
            "Accuracy on batch 1300 on Training is 77.0825326671791\n",
            "Accuracy on batch 1301 on Training is 77.088133640553\n",
            "Accuracy on batch 1302 on Training is 77.08892939370683\n",
            "Accuracy on batch 1303 on Training is 77.08732745398773\n",
            "Accuracy on batch 1304 on Training is 77.08572796934865\n",
            "Accuracy on batch 1305 on Training is 77.08652373660031\n",
            "Accuracy on batch 1306 on Training is 77.09688217291507\n",
            "Accuracy on batch 1307 on Training is 77.10005733944953\n",
            "Accuracy on batch 1308 on Training is 77.10561497326204\n",
            "Accuracy on batch 1309 on Training is 77.09446564885496\n",
            "Accuracy on batch 1310 on Training is 77.09286803966438\n",
            "Accuracy on batch 1311 on Training is 77.09127286585365\n",
            "Accuracy on batch 1312 on Training is 77.0944402132521\n",
            "Accuracy on batch 1313 on Training is 77.0880898021309\n",
            "Accuracy on batch 1314 on Training is 77.0888783269962\n",
            "Accuracy on batch 1315 on Training is 77.08491641337386\n",
            "Accuracy on batch 1316 on Training is 77.09282460136674\n",
            "Accuracy on batch 1317 on Training is 77.10072078907436\n",
            "Accuracy on batch 1318 on Training is 77.10386656557999\n",
            "Accuracy on batch 1319 on Training is 77.11174242424242\n",
            "Batch Id 1320 is having training loss of 0.8264040350914001\n",
            "0.7477662563323975\n",
            "Accuracy on batch 1320 on Training is 77.1125094625284\n",
            "Accuracy on batch 1321 on Training is 77.11091149773071\n",
            "Accuracy on batch 1322 on Training is 77.11404006046862\n",
            "Accuracy on batch 1323 on Training is 77.11952416918429\n",
            "Accuracy on batch 1324 on Training is 77.12735849056604\n",
            "Accuracy on batch 1325 on Training is 77.13518099547511\n",
            "Accuracy on batch 1326 on Training is 77.13592690278824\n",
            "Accuracy on batch 1327 on Training is 77.1437311746988\n",
            "Accuracy on batch 1328 on Training is 77.14917231000753\n",
            "Accuracy on batch 1329 on Training is 77.13815789473684\n",
            "Accuracy on batch 1330 on Training is 77.13655146506386\n",
            "Accuracy on batch 1331 on Training is 77.13494744744744\n",
            "Accuracy on batch 1332 on Training is 77.13100150037509\n",
            "Accuracy on batch 1333 on Training is 77.13643178410794\n",
            "Accuracy on batch 1334 on Training is 77.15121722846442\n",
            "Accuracy on batch 1335 on Training is 77.15194610778443\n",
            "Accuracy on batch 1336 on Training is 77.16436050860135\n",
            "Accuracy on batch 1337 on Training is 77.16274289985053\n",
            "Accuracy on batch 1338 on Training is 77.16579536967886\n",
            "Accuracy on batch 1339 on Training is 77.15951492537313\n",
            "Batch Id 1340 is having training loss of 0.825610339641571\n",
            "0.7092026472091675\n",
            "Accuracy on batch 1340 on Training is 77.16489560029828\n",
            "Accuracy on batch 1341 on Training is 77.17259687034277\n",
            "Accuracy on batch 1342 on Training is 77.16632539091586\n",
            "Accuracy on batch 1343 on Training is 77.15308779761905\n",
            "Accuracy on batch 1344 on Training is 77.14219330855019\n",
            "Accuracy on batch 1345 on Training is 77.14292347696879\n",
            "Accuracy on batch 1346 on Training is 77.14133259094284\n",
            "Accuracy on batch 1347 on Training is 77.14438056379822\n",
            "Accuracy on batch 1348 on Training is 77.14742401779095\n",
            "Accuracy on batch 1349 on Training is 77.15277777777777\n",
            "Accuracy on batch 1350 on Training is 77.15118430792006\n",
            "Accuracy on batch 1351 on Training is 77.14728180473372\n",
            "Accuracy on batch 1352 on Training is 77.1480044345898\n",
            "Accuracy on batch 1353 on Training is 77.15795790251107\n",
            "Accuracy on batch 1354 on Training is 77.15405904059041\n",
            "Accuracy on batch 1355 on Training is 77.15938421828909\n",
            "Accuracy on batch 1356 on Training is 77.16470154753132\n",
            "Accuracy on batch 1357 on Training is 77.16080633284241\n",
            "Accuracy on batch 1358 on Training is 77.1638153053716\n",
            "Accuracy on batch 1359 on Training is 77.16452205882354\n",
            "Batch Id 1360 is having training loss of 0.8257545828819275\n",
            "0.37416914105415344\n",
            "Accuracy on batch 1360 on Training is 77.17211609110947\n",
            "Accuracy on batch 1361 on Training is 77.1751101321586\n",
            "Accuracy on batch 1362 on Training is 77.17351430667645\n",
            "Accuracy on batch 1363 on Training is 77.16733870967742\n",
            "Accuracy on batch 1364 on Training is 77.17032967032966\n",
            "Accuracy on batch 1365 on Training is 77.17102855051245\n",
            "Accuracy on batch 1366 on Training is 77.18315654718361\n",
            "Accuracy on batch 1367 on Training is 77.1952668128655\n",
            "Accuracy on batch 1368 on Training is 77.19822863403944\n",
            "Accuracy on batch 1369 on Training is 77.19662408759125\n",
            "Accuracy on batch 1370 on Training is 77.19958059810358\n",
            "Accuracy on batch 1371 on Training is 77.19797740524781\n",
            "Accuracy on batch 1372 on Training is 77.19410050983248\n",
            "Accuracy on batch 1373 on Training is 77.19250363901018\n",
            "Accuracy on batch 1374 on Training is 77.19318181818181\n",
            "Accuracy on batch 1375 on Training is 77.1984011627907\n",
            "Accuracy on batch 1376 on Training is 77.19226579520698\n",
            "Accuracy on batch 1377 on Training is 77.18840711175616\n",
            "Accuracy on batch 1378 on Training is 77.18908629441624\n",
            "Accuracy on batch 1379 on Training is 77.18976449275362\n",
            "Batch Id 1380 is having training loss of 0.8256925344467163\n",
            "0.9424909353256226\n",
            "Accuracy on batch 1380 on Training is 77.18365314989138\n",
            "Accuracy on batch 1381 on Training is 77.19111794500724\n",
            "Accuracy on batch 1382 on Training is 77.19179320318149\n",
            "Accuracy on batch 1383 on Training is 77.19472543352602\n",
            "Accuracy on batch 1384 on Training is 77.19539711191335\n",
            "Accuracy on batch 1385 on Training is 77.2005772005772\n",
            "Accuracy on batch 1386 on Training is 77.20800288392213\n",
            "Accuracy on batch 1387 on Training is 77.20866354466858\n",
            "Accuracy on batch 1388 on Training is 77.21157307415407\n",
            "Accuracy on batch 1389 on Training is 77.2054856115108\n",
            "Accuracy on batch 1390 on Training is 77.20390007189073\n",
            "Accuracy on batch 1391 on Training is 77.20231681034483\n",
            "Accuracy on batch 1392 on Training is 77.20746590093324\n",
            "Accuracy on batch 1393 on Training is 77.2148493543759\n",
            "Accuracy on batch 1394 on Training is 77.2155017921147\n",
            "Accuracy on batch 1395 on Training is 77.22734598853869\n",
            "Accuracy on batch 1396 on Training is 77.22351467430208\n",
            "Accuracy on batch 1397 on Training is 77.21968884120172\n",
            "Accuracy on batch 1398 on Training is 77.21363473909936\n",
            "Accuracy on batch 1399 on Training is 77.22321428571429\n",
            "Batch Id 1400 is having training loss of 0.8248929977416992\n",
            "0.45479947328567505\n",
            "Accuracy on batch 1400 on Training is 77.22831905781585\n",
            "Accuracy on batch 1401 on Training is 77.23118758915834\n",
            "Accuracy on batch 1402 on Training is 77.23627940128297\n",
            "Accuracy on batch 1403 on Training is 77.2346866096866\n",
            "Accuracy on batch 1404 on Training is 77.230871886121\n",
            "Accuracy on batch 1405 on Training is 77.2248399715505\n",
            "Accuracy on batch 1406 on Training is 77.22103766879886\n",
            "Accuracy on batch 1407 on Training is 77.22611860795455\n",
            "Accuracy on batch 1408 on Training is 77.22897444996451\n",
            "Accuracy on batch 1409 on Training is 77.22739361702128\n",
            "Accuracy on batch 1410 on Training is 77.22360028348689\n",
            "Accuracy on batch 1411 on Training is 77.22645184135978\n",
            "Accuracy on batch 1412 on Training is 77.23372257607926\n",
            "Accuracy on batch 1413 on Training is 77.24319306930693\n",
            "Accuracy on batch 1414 on Training is 77.24381625441696\n",
            "Accuracy on batch 1415 on Training is 77.246645480226\n",
            "Accuracy on batch 1416 on Training is 77.25388143966126\n",
            "Accuracy on batch 1417 on Training is 77.25008815232722\n",
            "Accuracy on batch 1418 on Training is 77.25070472163496\n",
            "Accuracy on batch 1419 on Training is 77.24251760563381\n",
            "Batch Id 1420 is having training loss of 0.8248744010925293\n",
            "0.8719276785850525\n",
            "Accuracy on batch 1420 on Training is 77.23214285714286\n",
            "Accuracy on batch 1421 on Training is 77.23936357243319\n",
            "Accuracy on batch 1422 on Training is 77.22680955727337\n",
            "Accuracy on batch 1423 on Training is 77.22524578651685\n",
            "Accuracy on batch 1424 on Training is 77.23026315789474\n",
            "Accuracy on batch 1425 on Training is 77.22869915848527\n",
            "Accuracy on batch 1426 on Training is 77.22932725998598\n",
            "Accuracy on batch 1427 on Training is 77.23214285714286\n",
            "Accuracy on batch 1428 on Training is 77.2349545136459\n",
            "Accuracy on batch 1429 on Training is 77.24213286713287\n",
            "Accuracy on batch 1430 on Training is 77.24930118798044\n",
            "Accuracy on batch 1431 on Training is 77.24554818435755\n",
            "Accuracy on batch 1432 on Training is 77.25052337752966\n",
            "Accuracy on batch 1433 on Training is 77.25549163179916\n",
            "Accuracy on batch 1434 on Training is 77.2560975609756\n",
            "Accuracy on batch 1435 on Training is 77.25017409470752\n",
            "Accuracy on batch 1436 on Training is 77.2464335421016\n",
            "Accuracy on batch 1437 on Training is 77.24704450625869\n",
            "Accuracy on batch 1438 on Training is 77.2454829742877\n",
            "Accuracy on batch 1439 on Training is 77.23741319444444\n",
            "Batch Id 1440 is having training loss of 0.8243831992149353\n",
            "0.5444878935813904\n",
            "Accuracy on batch 1440 on Training is 77.24236641221374\n",
            "Accuracy on batch 1441 on Training is 77.24081137309292\n",
            "Accuracy on batch 1442 on Training is 77.23492723492724\n",
            "Accuracy on batch 1443 on Training is 77.23554362880887\n",
            "Accuracy on batch 1444 on Training is 77.2318339100346\n",
            "Accuracy on batch 1445 on Training is 77.22812932226833\n",
            "Accuracy on batch 1446 on Training is 77.23090877677954\n",
            "Accuracy on batch 1447 on Training is 77.23584254143647\n",
            "Accuracy on batch 1448 on Training is 77.24723947550035\n",
            "Accuracy on batch 1449 on Training is 77.2521551724138\n",
            "Accuracy on batch 1450 on Training is 77.25706409372846\n",
            "Accuracy on batch 1451 on Training is 77.26411845730027\n",
            "Accuracy on batch 1452 on Training is 77.26901238816242\n",
            "Accuracy on batch 1453 on Training is 77.25885488308116\n",
            "Accuracy on batch 1454 on Training is 77.26159793814433\n",
            "Accuracy on batch 1455 on Training is 77.26219093406593\n",
            "Accuracy on batch 1456 on Training is 77.2670727522306\n",
            "Accuracy on batch 1457 on Training is 77.26766117969822\n",
            "Accuracy on batch 1458 on Training is 77.25968128855381\n",
            "Accuracy on batch 1459 on Training is 77.27097602739725\n",
            "Batch Id 1460 is having training loss of 0.8232608437538147\n",
            "0.9210287928581238\n",
            "Accuracy on batch 1460 on Training is 77.27583846680356\n",
            "Accuracy on batch 1461 on Training is 77.28710670314638\n",
            "Accuracy on batch 1462 on Training is 77.29408749145591\n",
            "Accuracy on batch 1463 on Training is 77.30105874316939\n",
            "Accuracy on batch 1464 on Training is 77.30162116040955\n",
            "Accuracy on batch 1465 on Training is 77.30218281036835\n",
            "Accuracy on batch 1466 on Training is 77.29848329925017\n",
            "Accuracy on batch 1467 on Training is 77.29691757493188\n",
            "Accuracy on batch 1468 on Training is 77.29960857726344\n",
            "Accuracy on batch 1469 on Training is 77.29591836734694\n",
            "Accuracy on batch 1470 on Training is 77.29860639021074\n",
            "Accuracy on batch 1471 on Training is 77.30553668478261\n",
            "Accuracy on batch 1472 on Training is 77.29760692464359\n",
            "Accuracy on batch 1473 on Training is 77.29604816824965\n",
            "Accuracy on batch 1474 on Training is 77.29661016949153\n",
            "Accuracy on batch 1475 on Training is 77.29293699186992\n",
            "Accuracy on batch 1476 on Training is 77.29350033852404\n",
            "Accuracy on batch 1477 on Training is 77.29406292286875\n",
            "Accuracy on batch 1478 on Training is 77.29251183231914\n",
            "Accuracy on batch 1479 on Training is 77.29307432432432\n",
            "Batch Id 1480 is having training loss of 0.8224738240242004\n",
            "0.8747618198394775\n",
            "Accuracy on batch 1480 on Training is 77.28519581363943\n",
            "Accuracy on batch 1481 on Training is 77.28787112010797\n",
            "Accuracy on batch 1482 on Training is 77.28000674308834\n",
            "Accuracy on batch 1483 on Training is 77.27215296495957\n",
            "Accuracy on batch 1484 on Training is 77.27062289562289\n",
            "Accuracy on batch 1485 on Training is 77.26909488559892\n",
            "Accuracy on batch 1486 on Training is 77.27387357094821\n",
            "Accuracy on batch 1487 on Training is 77.27444556451613\n",
            "Accuracy on batch 1488 on Training is 77.27291806581599\n",
            "Accuracy on batch 1489 on Training is 77.27978187919463\n",
            "Accuracy on batch 1490 on Training is 77.2845405767941\n",
            "Accuracy on batch 1491 on Training is 77.28510388739946\n",
            "Accuracy on batch 1492 on Training is 77.28566644340255\n",
            "Accuracy on batch 1493 on Training is 77.28413654618474\n",
            "Accuracy on batch 1494 on Training is 77.28051839464882\n",
            "Accuracy on batch 1495 on Training is 77.28734959893048\n",
            "Accuracy on batch 1496 on Training is 77.27747160988643\n",
            "Accuracy on batch 1497 on Training is 77.28012349799732\n",
            "Accuracy on batch 1498 on Training is 77.2827718478986\n",
            "Accuracy on batch 1499 on Training is 77.28958333333334\n",
            "Batch Id 1500 is having training loss of 0.8232457637786865\n",
            "0.45742687582969666\n",
            "Accuracy on batch 1500 on Training is 77.29638574283811\n",
            "Accuracy on batch 1501 on Training is 77.30109853528629\n",
            "Accuracy on batch 1502 on Training is 77.30372588157019\n",
            "Accuracy on batch 1503 on Training is 77.31050531914893\n",
            "Accuracy on batch 1504 on Training is 77.3172757475083\n",
            "Accuracy on batch 1505 on Training is 77.30951195219123\n",
            "Accuracy on batch 1506 on Training is 77.31627405441274\n",
            "Accuracy on batch 1507 on Training is 77.31888262599469\n",
            "Accuracy on batch 1508 on Training is 77.31113320079523\n",
            "Accuracy on batch 1509 on Training is 77.31167218543047\n",
            "Accuracy on batch 1510 on Training is 77.31014228987425\n",
            "Accuracy on batch 1511 on Training is 77.30448082010582\n",
            "Accuracy on batch 1512 on Training is 77.29676140118968\n",
            "Accuracy on batch 1513 on Training is 77.29318031704095\n",
            "Accuracy on batch 1514 on Training is 77.30404290429043\n",
            "Accuracy on batch 1515 on Training is 77.29221635883906\n",
            "Accuracy on batch 1516 on Training is 77.294825313118\n",
            "Accuracy on batch 1517 on Training is 77.29331357048748\n",
            "Accuracy on batch 1518 on Training is 77.2897465437788\n",
            "Accuracy on batch 1519 on Training is 77.2923519736842\n",
            "Batch Id 1520 is having training loss of 0.8223806619644165\n",
            "0.5790011882781982\n",
            "Accuracy on batch 1520 on Training is 77.2990631163708\n",
            "Accuracy on batch 1521 on Training is 77.30371222076215\n",
            "Accuracy on batch 1522 on Training is 77.30425147734734\n",
            "Accuracy on batch 1523 on Training is 77.31504265091864\n",
            "Accuracy on batch 1524 on Training is 77.31352459016394\n",
            "Accuracy on batch 1525 on Training is 77.30996068152031\n",
            "Accuracy on batch 1526 on Training is 77.31254092992796\n",
            "Accuracy on batch 1527 on Training is 77.31307264397905\n",
            "Accuracy on batch 1528 on Training is 77.30747220405493\n",
            "Accuracy on batch 1529 on Training is 77.30596405228758\n",
            "Accuracy on batch 1530 on Training is 77.30854016982364\n",
            "Accuracy on batch 1531 on Training is 77.30091383812011\n",
            "Accuracy on batch 1532 on Training is 77.29941291585128\n",
            "Accuracy on batch 1533 on Training is 77.30402542372882\n",
            "Accuracy on batch 1534 on Training is 77.3086319218241\n",
            "Accuracy on batch 1535 on Training is 77.32340494791667\n",
            "Accuracy on batch 1536 on Training is 77.32392648015615\n",
            "Accuracy on batch 1537 on Training is 77.32647919375813\n",
            "Accuracy on batch 1538 on Training is 77.3290285899935\n",
            "Accuracy on batch 1539 on Training is 77.33766233766234\n",
            "Batch Id 1540 is having training loss of 0.8209647536277771\n",
            "0.6775064468383789\n",
            "Accuracy on batch 1540 on Training is 77.34222907203115\n",
            "Accuracy on batch 1541 on Training is 77.34476329442283\n",
            "Accuracy on batch 1542 on Training is 77.33919313026571\n",
            "Accuracy on batch 1543 on Training is 77.34577396373057\n",
            "Accuracy on batch 1544 on Training is 77.33414239482201\n",
            "Accuracy on batch 1545 on Training is 77.32656856403622\n",
            "Accuracy on batch 1546 on Training is 77.3331447963801\n",
            "Accuracy on batch 1547 on Training is 77.33163759689923\n",
            "Accuracy on batch 1548 on Training is 77.328114912847\n",
            "Accuracy on batch 1549 on Training is 77.32459677419355\n",
            "Accuracy on batch 1550 on Training is 77.31906834300452\n",
            "Accuracy on batch 1551 on Training is 77.31958762886597\n",
            "Accuracy on batch 1552 on Training is 77.31608177720541\n",
            "Accuracy on batch 1553 on Training is 77.31258043758044\n",
            "Accuracy on batch 1554 on Training is 77.31712218649518\n",
            "Accuracy on batch 1555 on Training is 77.31362467866325\n",
            "Accuracy on batch 1556 on Training is 77.31815992292871\n",
            "Accuracy on batch 1557 on Training is 77.31667201540436\n",
            "Accuracy on batch 1558 on Training is 77.3111770365619\n",
            "Accuracy on batch 1559 on Training is 77.31370192307692\n",
            "Batch Id 1560 is having training loss of 0.8218051791191101\n",
            "1.1188340187072754\n",
            "Accuracy on batch 1560 on Training is 77.31021780909673\n",
            "Accuracy on batch 1561 on Training is 77.30273687580025\n",
            "Accuracy on batch 1562 on Training is 77.29726487523992\n",
            "Accuracy on batch 1563 on Training is 77.2997921994885\n",
            "Accuracy on batch 1564 on Training is 77.29632587859425\n",
            "Accuracy on batch 1565 on Training is 77.29086845466156\n",
            "Accuracy on batch 1566 on Training is 77.28940650925335\n",
            "Accuracy on batch 1567 on Training is 77.28595344387755\n",
            "Accuracy on batch 1568 on Training is 77.2944550669216\n",
            "Accuracy on batch 1569 on Training is 77.29299363057325\n",
            "Accuracy on batch 1570 on Training is 77.29352323360916\n",
            "Accuracy on batch 1571 on Training is 77.29007633587786\n",
            "Accuracy on batch 1572 on Training is 77.28663382072473\n",
            "Accuracy on batch 1573 on Training is 77.28518106734434\n",
            "Accuracy on batch 1574 on Training is 77.2936507936508\n",
            "Accuracy on batch 1575 on Training is 77.3001269035533\n",
            "Accuracy on batch 1576 on Training is 77.29668674698796\n",
            "Accuracy on batch 1577 on Training is 77.30117237008872\n",
            "Accuracy on batch 1578 on Training is 77.30367321089297\n",
            "Accuracy on batch 1579 on Training is 77.30814873417721\n",
            "Batch Id 1580 is having training loss of 0.8219871520996094\n",
            "0.8910064101219177\n",
            "Accuracy on batch 1580 on Training is 77.31064199873498\n",
            "Accuracy on batch 1581 on Training is 77.3091814159292\n",
            "Accuracy on batch 1582 on Training is 77.30772267845862\n",
            "Accuracy on batch 1583 on Training is 77.31021148989899\n",
            "Accuracy on batch 1584 on Training is 77.3166403785489\n",
            "Accuracy on batch 1585 on Training is 77.31517969735182\n",
            "Accuracy on batch 1586 on Training is 77.31765910522999\n",
            "Accuracy on batch 1587 on Training is 77.31226385390428\n",
            "Accuracy on batch 1588 on Training is 77.3167086217747\n",
            "Accuracy on batch 1589 on Training is 77.31328616352201\n",
            "Accuracy on batch 1590 on Training is 77.31379635449403\n",
            "Accuracy on batch 1591 on Training is 77.32019472361809\n",
            "Accuracy on batch 1592 on Training is 77.31481481481481\n",
            "Accuracy on batch 1593 on Training is 77.31728356336261\n",
            "Accuracy on batch 1594 on Training is 77.31778996865204\n",
            "Accuracy on batch 1595 on Training is 77.3202537593985\n",
            "Accuracy on batch 1596 on Training is 77.31684408265498\n",
            "Accuracy on batch 1597 on Training is 77.3153942428035\n",
            "Accuracy on batch 1598 on Training is 77.30808317698562\n",
            "Accuracy on batch 1599 on Training is 77.3125\n",
            "Batch Id 1600 is having training loss of 0.821515679359436\n",
            "0.7696985602378845\n",
            "Accuracy on batch 1600 on Training is 77.31105559025609\n",
            "Accuracy on batch 1601 on Training is 77.32131710362047\n",
            "Accuracy on batch 1602 on Training is 77.3140205864005\n",
            "Accuracy on batch 1603 on Training is 77.32231920199501\n",
            "Accuracy on batch 1604 on Training is 77.32281931464175\n",
            "Accuracy on batch 1605 on Training is 77.32526463262765\n",
            "Accuracy on batch 1606 on Training is 77.33159614187927\n",
            "Accuracy on batch 1607 on Training is 77.3301461442786\n",
            "Accuracy on batch 1608 on Training is 77.31704474829087\n",
            "Accuracy on batch 1609 on Training is 77.31948757763975\n",
            "Accuracy on batch 1610 on Training is 77.32968653010552\n",
            "Accuracy on batch 1611 on Training is 77.32630272952854\n",
            "Accuracy on batch 1612 on Training is 77.32292312461253\n",
            "Accuracy on batch 1613 on Training is 77.32922862453532\n",
            "Accuracy on batch 1614 on Training is 77.33359133126935\n",
            "Accuracy on batch 1615 on Training is 77.32441212871286\n",
            "Accuracy on batch 1616 on Training is 77.32877241805814\n",
            "Accuracy on batch 1617 on Training is 77.33119592088998\n",
            "Accuracy on batch 1618 on Training is 77.33747683755405\n",
            "Accuracy on batch 1619 on Training is 77.33603395061728\n",
            "Batch Id 1620 is having training loss of 0.8200550079345703\n",
            "0.2957417368888855\n",
            "Accuracy on batch 1620 on Training is 77.34230413325108\n",
            "Accuracy on batch 1621 on Training is 77.34278668310728\n",
            "Accuracy on batch 1622 on Training is 77.33556685150955\n",
            "Accuracy on batch 1623 on Training is 77.33990147783251\n",
            "Accuracy on batch 1624 on Training is 77.33076923076923\n",
            "Accuracy on batch 1625 on Training is 77.33894526445265\n",
            "Accuracy on batch 1626 on Training is 77.34326982175784\n",
            "Accuracy on batch 1627 on Training is 77.34566953316953\n",
            "Accuracy on batch 1628 on Training is 77.34614794352363\n",
            "Accuracy on batch 1629 on Training is 77.34279141104294\n",
            "Accuracy on batch 1630 on Training is 77.3413549969344\n",
            "Accuracy on batch 1631 on Training is 77.34566482843137\n",
            "Accuracy on batch 1632 on Training is 77.35188303735457\n",
            "Accuracy on batch 1633 on Training is 77.34661872705018\n",
            "Accuracy on batch 1634 on Training is 77.35091743119266\n",
            "Accuracy on batch 1635 on Training is 77.34948044009779\n",
            "Accuracy on batch 1636 on Training is 77.34804520464264\n",
            "Accuracy on batch 1637 on Training is 77.34851953601954\n",
            "Accuracy on batch 1638 on Training is 77.3489932885906\n",
            "Accuracy on batch 1639 on Training is 77.34565548780488\n",
            "Batch Id 1640 is having training loss of 0.8199369311332703\n",
            "0.6977010369300842\n",
            "Accuracy on batch 1640 on Training is 77.35184338817794\n",
            "Accuracy on batch 1641 on Training is 77.35231425091352\n",
            "Accuracy on batch 1642 on Training is 77.34517650639074\n",
            "Accuracy on batch 1643 on Training is 77.34375\n",
            "Accuracy on batch 1644 on Training is 77.34042553191489\n",
            "Accuracy on batch 1645 on Training is 77.33900364520049\n",
            "Accuracy on batch 1646 on Training is 77.33378870673953\n",
            "Accuracy on batch 1647 on Training is 77.33047633495146\n",
            "Accuracy on batch 1648 on Training is 77.33285324439053\n",
            "Accuracy on batch 1649 on Training is 77.33522727272727\n",
            "Accuracy on batch 1650 on Training is 77.33759842519684\n",
            "Accuracy on batch 1651 on Training is 77.33996670702179\n",
            "Accuracy on batch 1652 on Training is 77.3461131276467\n",
            "Accuracy on batch 1653 on Training is 77.3428053204353\n",
            "Accuracy on batch 1654 on Training is 77.3489425981873\n",
            "Accuracy on batch 1655 on Training is 77.34752415458937\n",
            "Accuracy on batch 1656 on Training is 77.34799336149668\n",
            "Accuracy on batch 1657 on Training is 77.3541164053076\n",
            "Accuracy on batch 1658 on Training is 77.35646473779386\n",
            "Accuracy on batch 1659 on Training is 77.34939759036145\n",
            "Batch Id 1660 is having training loss of 0.8201661705970764\n",
            "0.7057459950447083\n",
            "Accuracy on batch 1660 on Training is 77.34986453943408\n",
            "Accuracy on batch 1661 on Training is 77.34845066185319\n",
            "Accuracy on batch 1662 on Training is 77.34891761876128\n",
            "Accuracy on batch 1663 on Training is 77.3456280048077\n",
            "Accuracy on batch 1664 on Training is 77.34797297297297\n",
            "Accuracy on batch 1665 on Training is 77.3390606242497\n",
            "Accuracy on batch 1666 on Training is 77.34328134373125\n",
            "Accuracy on batch 1667 on Training is 77.34000299760191\n",
            "Accuracy on batch 1668 on Training is 77.34047333732774\n",
            "Accuracy on batch 1669 on Training is 77.33907185628742\n",
            "Accuracy on batch 1670 on Training is 77.34702274087373\n",
            "Accuracy on batch 1671 on Training is 77.35496411483254\n",
            "Accuracy on batch 1672 on Training is 77.36102809324566\n",
            "Accuracy on batch 1673 on Training is 77.35961768219833\n",
            "Accuracy on batch 1674 on Training is 77.36380597014926\n",
            "Accuracy on batch 1675 on Training is 77.35866646778042\n",
            "Accuracy on batch 1676 on Training is 77.35539654144306\n",
            "Accuracy on batch 1677 on Training is 77.35585518474375\n",
            "Accuracy on batch 1678 on Training is 77.34886837403216\n",
            "Accuracy on batch 1679 on Training is 77.34747023809524\n",
            "Batch Id 1680 is having training loss of 0.8199501037597656\n",
            "1.2188445329666138\n",
            "Accuracy on batch 1680 on Training is 77.33677870315289\n",
            "Accuracy on batch 1681 on Training is 77.33538941736029\n",
            "Accuracy on batch 1682 on Training is 77.32657456922163\n",
            "Accuracy on batch 1683 on Training is 77.32890439429929\n",
            "Accuracy on batch 1684 on Training is 77.3293768545994\n",
            "Accuracy on batch 1685 on Training is 77.33355575326216\n",
            "Accuracy on batch 1686 on Training is 77.33402489626556\n",
            "Accuracy on batch 1687 on Training is 77.33449348341232\n",
            "Accuracy on batch 1688 on Training is 77.33681172291297\n",
            "Accuracy on batch 1689 on Training is 77.3280325443787\n",
            "Accuracy on batch 1690 on Training is 77.32850384387936\n",
            "Accuracy on batch 1691 on Training is 77.33082151300236\n",
            "Accuracy on batch 1692 on Training is 77.34051978735971\n",
            "Accuracy on batch 1693 on Training is 77.3409828807556\n",
            "Accuracy on batch 1694 on Training is 77.34328908554572\n",
            "Accuracy on batch 1695 on Training is 77.34006485849056\n",
            "Accuracy on batch 1696 on Training is 77.3405274012964\n",
            "Accuracy on batch 1697 on Training is 77.34282979976443\n",
            "Accuracy on batch 1698 on Training is 77.34512948793407\n",
            "Accuracy on batch 1699 on Training is 77.35110294117646\n",
            "Batch Id 1700 is having training loss of 0.8185849189758301\n",
            "0.4810214340686798\n",
            "Accuracy on batch 1700 on Training is 77.35706937095826\n",
            "Accuracy on batch 1701 on Training is 77.3575205640423\n",
            "Accuracy on batch 1702 on Training is 77.35246623605403\n",
            "Accuracy on batch 1703 on Training is 77.34741784037558\n",
            "Accuracy on batch 1704 on Training is 77.35153958944281\n",
            "Accuracy on batch 1705 on Training is 77.3501611957796\n",
            "Accuracy on batch 1706 on Training is 77.35244581136497\n",
            "Accuracy on batch 1707 on Training is 77.3528981264637\n",
            "Accuracy on batch 1708 on Training is 77.35334991222938\n",
            "Accuracy on batch 1709 on Training is 77.35197368421052\n",
            "Accuracy on batch 1710 on Training is 77.35242548217417\n",
            "Accuracy on batch 1711 on Training is 77.35470210280374\n",
            "Accuracy on batch 1712 on Training is 77.36062463514303\n",
            "Accuracy on batch 1713 on Training is 77.37018669778297\n",
            "Accuracy on batch 1714 on Training is 77.37427113702624\n",
            "Accuracy on batch 1715 on Training is 77.37470862470863\n",
            "Accuracy on batch 1716 on Training is 77.37514560279557\n",
            "Accuracy on batch 1717 on Training is 77.37922002328288\n",
            "Accuracy on batch 1718 on Training is 77.38147178592205\n",
            "Accuracy on batch 1719 on Training is 77.38008720930233\n",
            "Batch Id 1720 is having training loss of 0.8176360726356506\n",
            "0.7142990827560425\n",
            "Accuracy on batch 1720 on Training is 77.37688843695526\n",
            "Accuracy on batch 1721 on Training is 77.3845818815331\n",
            "Accuracy on batch 1722 on Training is 77.38863900174115\n",
            "Accuracy on batch 1723 on Training is 77.38725348027842\n",
            "Accuracy on batch 1724 on Training is 77.3876811594203\n",
            "Accuracy on batch 1725 on Training is 77.3899188876014\n",
            "Accuracy on batch 1726 on Training is 77.39215402431962\n",
            "Accuracy on batch 1727 on Training is 77.39076967592592\n",
            "Accuracy on batch 1728 on Training is 77.39119433198381\n",
            "Accuracy on batch 1729 on Training is 77.38619942196532\n",
            "Accuracy on batch 1730 on Training is 77.388431542461\n",
            "Accuracy on batch 1731 on Training is 77.39787817551964\n",
            "Accuracy on batch 1732 on Training is 77.40551067512983\n",
            "Accuracy on batch 1733 on Training is 77.4041234140715\n",
            "Accuracy on batch 1734 on Training is 77.40453890489914\n",
            "Accuracy on batch 1735 on Training is 77.40495391705069\n",
            "Accuracy on batch 1736 on Training is 77.40896660909614\n",
            "Accuracy on batch 1737 on Training is 77.4129746835443\n",
            "Accuracy on batch 1738 on Training is 77.42236917768832\n",
            "Accuracy on batch 1739 on Training is 77.42456896551724\n",
            "Batch Id 1740 is having training loss of 0.8160829544067383\n",
            "0.6502856612205505\n",
            "Accuracy on batch 1740 on Training is 77.42317633543941\n",
            "Accuracy on batch 1741 on Training is 77.41999138920781\n",
            "Accuracy on batch 1742 on Training is 77.4239816408491\n",
            "Accuracy on batch 1743 on Training is 77.41721616972477\n",
            "Accuracy on batch 1744 on Training is 77.41583094555874\n",
            "Accuracy on batch 1745 on Training is 77.42518613974799\n",
            "Accuracy on batch 1746 on Training is 77.42916428162565\n",
            "Accuracy on batch 1747 on Training is 77.4295623569794\n",
            "Accuracy on batch 1748 on Training is 77.42817324185249\n",
            "Accuracy on batch 1749 on Training is 77.43214285714286\n",
            "Accuracy on batch 1750 on Training is 77.43789263278127\n",
            "Accuracy on batch 1751 on Training is 77.43471746575342\n",
            "Accuracy on batch 1752 on Training is 77.43689389617798\n",
            "Accuracy on batch 1753 on Training is 77.43906784492589\n",
            "Accuracy on batch 1754 on Training is 77.43589743589743\n",
            "Accuracy on batch 1755 on Training is 77.438069476082\n",
            "Accuracy on batch 1756 on Training is 77.43846044393854\n",
            "Accuracy on batch 1757 on Training is 77.43529579067122\n",
            "Accuracy on batch 1758 on Training is 77.43746446844798\n",
            "Accuracy on batch 1759 on Training is 77.44140625\n",
            "Batch Id 1760 is having training loss of 0.8148284554481506\n",
            "0.37892526388168335\n",
            "Accuracy on batch 1760 on Training is 77.4488926746167\n",
            "Accuracy on batch 1761 on Training is 77.45459704880817\n",
            "Accuracy on batch 1762 on Training is 77.4585224049915\n",
            "Accuracy on batch 1763 on Training is 77.46244331065759\n",
            "Accuracy on batch 1764 on Training is 77.45750708215297\n",
            "Accuracy on batch 1765 on Training is 77.45788505096263\n",
            "Accuracy on batch 1766 on Training is 77.45295698924731\n",
            "Accuracy on batch 1767 on Training is 77.4568721719457\n",
            "Accuracy on batch 1768 on Training is 77.44488411531938\n",
            "Accuracy on batch 1769 on Training is 77.4417372881356\n",
            "Accuracy on batch 1770 on Training is 77.44388763410502\n",
            "Accuracy on batch 1771 on Training is 77.44956264108352\n",
            "Accuracy on batch 1772 on Training is 77.44994359842076\n",
            "Accuracy on batch 1773 on Training is 77.44680101465615\n",
            "Accuracy on batch 1774 on Training is 77.4524647887324\n",
            "Accuracy on batch 1775 on Training is 77.45460304054055\n",
            "Accuracy on batch 1776 on Training is 77.45849746764209\n",
            "Accuracy on batch 1777 on Training is 77.46062992125984\n",
            "Accuracy on batch 1778 on Training is 77.45397695334458\n",
            "Accuracy on batch 1779 on Training is 77.45259831460675\n",
            "Batch Id 1780 is having training loss of 0.8146136999130249\n",
            "0.44171637296676636\n",
            "Accuracy on batch 1780 on Training is 77.4564851207187\n",
            "Accuracy on batch 1781 on Training is 77.45861391694724\n",
            "Accuracy on batch 1782 on Training is 77.46599831744251\n",
            "Accuracy on batch 1783 on Training is 77.46111266816143\n",
            "Accuracy on batch 1784 on Training is 77.45798319327731\n",
            "Accuracy on batch 1785 on Training is 77.4653555431131\n",
            "Accuracy on batch 1786 on Training is 77.46397593732513\n",
            "Accuracy on batch 1787 on Training is 77.47133668903803\n",
            "Accuracy on batch 1788 on Training is 77.46471492453885\n",
            "Accuracy on batch 1789 on Training is 77.46682960893855\n",
            "Accuracy on batch 1790 on Training is 77.46894193188163\n",
            "Accuracy on batch 1791 on Training is 77.4658203125\n",
            "Accuracy on batch 1792 on Training is 77.46793084216397\n",
            "Accuracy on batch 1793 on Training is 77.47352285395763\n",
            "Accuracy on batch 1794 on Training is 77.47388579387187\n",
            "Accuracy on batch 1795 on Training is 77.47598830734967\n",
            "Accuracy on batch 1796 on Training is 77.47982749026154\n",
            "Accuracy on batch 1797 on Training is 77.46975806451613\n",
            "Accuracy on batch 1798 on Training is 77.47533351862145\n",
            "Accuracy on batch 1799 on Training is 77.47222222222223\n",
            "Batch Id 1800 is having training loss of 0.8134084343910217\n",
            "0.7581018805503845\n",
            "Accuracy on batch 1800 on Training is 77.47258467518046\n",
            "Accuracy on batch 1801 on Training is 77.47814927857935\n",
            "Accuracy on batch 1802 on Training is 77.48024126455907\n",
            "Accuracy on batch 1803 on Training is 77.47886640798227\n",
            "Accuracy on batch 1804 on Training is 77.47576177285319\n",
            "Accuracy on batch 1805 on Training is 77.48131229235881\n",
            "Accuracy on batch 1806 on Training is 77.48166851134476\n",
            "Accuracy on batch 1807 on Training is 77.48202433628319\n",
            "Accuracy on batch 1808 on Training is 77.48583471531232\n",
            "Accuracy on batch 1809 on Training is 77.48446132596685\n",
            "Accuracy on batch 1810 on Training is 77.47791275538377\n",
            "Accuracy on batch 1811 on Training is 77.48344370860927\n",
            "Accuracy on batch 1812 on Training is 77.48207391064534\n",
            "Accuracy on batch 1813 on Training is 77.48070562293275\n",
            "Accuracy on batch 1814 on Training is 77.48106060606061\n",
            "Accuracy on batch 1815 on Training is 77.48313601321586\n",
            "Accuracy on batch 1816 on Training is 77.48176940011007\n",
            "Accuracy on batch 1817 on Training is 77.48040429042905\n",
            "Accuracy on batch 1818 on Training is 77.48247663551402\n",
            "Accuracy on batch 1819 on Training is 77.48282967032966\n",
            "Batch Id 1820 is having training loss of 0.8129201531410217\n",
            "0.5016915798187256\n",
            "Accuracy on batch 1820 on Training is 77.48833058758923\n",
            "Accuracy on batch 1821 on Training is 77.4938254665203\n",
            "Accuracy on batch 1822 on Training is 77.49417169500823\n",
            "Accuracy on batch 1823 on Training is 77.48937774122807\n",
            "Accuracy on batch 1824 on Training is 77.48458904109589\n",
            "Accuracy on batch 1825 on Training is 77.48836254107339\n",
            "Accuracy on batch 1826 on Training is 77.48186918445539\n",
            "Accuracy on batch 1827 on Training is 77.48051148796499\n",
            "Accuracy on batch 1828 on Training is 77.47915527610716\n",
            "Accuracy on batch 1829 on Training is 77.47950819672131\n",
            "Accuracy on batch 1830 on Training is 77.47815401419989\n",
            "Accuracy on batch 1831 on Training is 77.48362445414847\n",
            "Accuracy on batch 1832 on Training is 77.48226950354609\n",
            "Accuracy on batch 1833 on Training is 77.4826199563795\n",
            "Accuracy on batch 1834 on Training is 77.48637602179836\n",
            "Accuracy on batch 1835 on Training is 77.49523420479302\n",
            "Accuracy on batch 1836 on Training is 77.49557702776265\n",
            "Accuracy on batch 1837 on Training is 77.49591947769315\n",
            "Accuracy on batch 1838 on Training is 77.49626155519304\n",
            "Accuracy on batch 1839 on Training is 77.49830163043478\n",
            "Batch Id 1840 is having training loss of 0.8123283386230469\n",
            "0.5522529482841492\n",
            "Accuracy on batch 1840 on Training is 77.50373438348724\n",
            "Accuracy on batch 1841 on Training is 77.5006786102063\n",
            "Accuracy on batch 1842 on Training is 77.49932175800326\n",
            "Accuracy on batch 1843 on Training is 77.49288232104121\n",
            "Accuracy on batch 1844 on Training is 77.49322493224932\n",
            "Accuracy on batch 1845 on Training is 77.48510292524377\n",
            "Accuracy on batch 1846 on Training is 77.47868164591229\n",
            "Accuracy on batch 1847 on Training is 77.47734036796537\n",
            "Accuracy on batch 1848 on Training is 77.47431043807464\n",
            "Accuracy on batch 1849 on Training is 77.47466216216216\n",
            "Accuracy on batch 1850 on Training is 77.47332522960562\n",
            "Accuracy on batch 1851 on Training is 77.47198974082073\n",
            "Accuracy on batch 1852 on Training is 77.47571505666487\n",
            "Accuracy on batch 1853 on Training is 77.48112189859762\n",
            "Accuracy on batch 1854 on Training is 77.47809973045823\n",
            "Accuracy on batch 1855 on Training is 77.4767645474138\n",
            "Accuracy on batch 1856 on Training is 77.4653338718363\n",
            "Accuracy on batch 1857 on Training is 77.46737082884822\n",
            "Accuracy on batch 1858 on Training is 77.4660435718128\n",
            "Accuracy on batch 1859 on Training is 77.46471774193549\n",
            "Batch Id 1860 is having training loss of 0.8137478828430176\n",
            "0.7714660167694092\n",
            "Accuracy on batch 1860 on Training is 77.46843095110155\n",
            "Accuracy on batch 1861 on Training is 77.46878356605801\n",
            "Accuracy on batch 1862 on Training is 77.47752281266774\n",
            "Accuracy on batch 1863 on Training is 77.47954667381974\n",
            "Accuracy on batch 1864 on Training is 77.48324396782841\n",
            "Accuracy on batch 1865 on Training is 77.48358788853162\n",
            "Accuracy on batch 1866 on Training is 77.48225763256562\n",
            "Accuracy on batch 1867 on Training is 77.4876204496788\n",
            "Accuracy on batch 1868 on Training is 77.48628945960407\n",
            "Accuracy on batch 1869 on Training is 77.49498663101605\n",
            "Accuracy on batch 1870 on Training is 77.4986638161411\n",
            "Accuracy on batch 1871 on Training is 77.50066773504274\n",
            "Accuracy on batch 1872 on Training is 77.50100106780566\n",
            "Accuracy on batch 1873 on Training is 77.50800426894344\n",
            "Accuracy on batch 1874 on Training is 77.51166666666667\n",
            "Accuracy on batch 1875 on Training is 77.51699093816632\n",
            "Accuracy on batch 1876 on Training is 77.51232019179542\n",
            "Accuracy on batch 1877 on Training is 77.51597444089457\n",
            "Accuracy on batch 1878 on Training is 77.51297232570516\n",
            "Accuracy on batch 1879 on Training is 77.51828457446808\n",
            "Batch Id 1880 is having training loss of 0.8126384615898132\n",
            "1.3449769020080566\n",
            "Accuracy on batch 1880 on Training is 77.50697767145135\n",
            "Accuracy on batch 1881 on Training is 77.50564558979809\n",
            "Accuracy on batch 1882 on Training is 77.51095326606479\n",
            "Accuracy on batch 1883 on Training is 77.51127919320595\n",
            "Accuracy on batch 1884 on Training is 77.51160477453581\n",
            "Accuracy on batch 1885 on Training is 77.51027306468717\n",
            "Accuracy on batch 1886 on Training is 77.50563063063063\n",
            "Accuracy on batch 1887 on Training is 77.50595868644068\n",
            "Accuracy on batch 1888 on Training is 77.50794070937003\n",
            "Accuracy on batch 1889 on Training is 77.50165343915344\n",
            "Accuracy on batch 1890 on Training is 77.50033051295611\n",
            "Accuracy on batch 1891 on Training is 77.495705602537\n",
            "Accuracy on batch 1892 on Training is 77.4960380348653\n",
            "Accuracy on batch 1893 on Training is 77.50626979936642\n",
            "Accuracy on batch 1894 on Training is 77.5065963060686\n",
            "Accuracy on batch 1895 on Training is 77.51351529535864\n",
            "Accuracy on batch 1896 on Training is 77.51548497627833\n",
            "Accuracy on batch 1897 on Training is 77.5190990516333\n",
            "Accuracy on batch 1898 on Training is 77.52106371774619\n",
            "Accuracy on batch 1899 on Training is 77.51809210526316\n",
            "Batch Id 1900 is having training loss of 0.81227707862854\n",
            "0.7943943738937378\n",
            "Accuracy on batch 1900 on Training is 77.52005523408732\n",
            "Accuracy on batch 1901 on Training is 77.52201629863302\n",
            "Accuracy on batch 1902 on Training is 77.5223331581713\n",
            "Accuracy on batch 1903 on Training is 77.52100840336135\n",
            "Accuracy on batch 1904 on Training is 77.51640419947506\n",
            "Accuracy on batch 1905 on Training is 77.51016526757607\n",
            "Accuracy on batch 1906 on Training is 77.50721027792343\n",
            "Accuracy on batch 1907 on Training is 77.50917190775681\n",
            "Accuracy on batch 1908 on Training is 77.50622053431115\n",
            "Accuracy on batch 1909 on Training is 77.50490837696336\n",
            "Accuracy on batch 1910 on Training is 77.50523286237572\n",
            "Accuracy on batch 1911 on Training is 77.5055570083682\n",
            "Accuracy on batch 1912 on Training is 77.50261369576582\n",
            "Accuracy on batch 1913 on Training is 77.49967345872518\n",
            "Accuracy on batch 1914 on Training is 77.5016318537859\n",
            "Accuracy on batch 1915 on Training is 77.5035882045929\n",
            "Accuracy on batch 1916 on Training is 77.50554251434534\n",
            "Accuracy on batch 1917 on Training is 77.50912408759125\n",
            "Accuracy on batch 1918 on Training is 77.50944502344971\n",
            "Accuracy on batch 1919 on Training is 77.5146484375\n",
            "Batch Id 1920 is having training loss of 0.812865674495697\n",
            "0.20267130434513092\n",
            "Accuracy on batch 1920 on Training is 77.52635346173868\n",
            "Accuracy on batch 1921 on Training is 77.52178720083246\n",
            "Accuracy on batch 1922 on Training is 77.51397555902236\n",
            "Accuracy on batch 1923 on Training is 77.51429313929314\n",
            "Accuracy on batch 1924 on Training is 77.51623376623377\n",
            "Accuracy on batch 1925 on Training is 77.51492731048806\n",
            "Accuracy on batch 1926 on Training is 77.51686559418786\n",
            "Accuracy on batch 1927 on Training is 77.50745591286307\n",
            "Accuracy on batch 1928 on Training is 77.50939606013479\n",
            "Accuracy on batch 1929 on Training is 77.50971502590673\n",
            "Accuracy on batch 1930 on Training is 77.51003366131538\n",
            "Accuracy on batch 1931 on Training is 77.50711697722568\n",
            "Accuracy on batch 1932 on Training is 77.51390325918261\n",
            "Accuracy on batch 1933 on Training is 77.51583505687694\n",
            "Accuracy on batch 1934 on Training is 77.52099483204134\n",
            "Accuracy on batch 1935 on Training is 77.5261492768595\n",
            "Accuracy on batch 1936 on Training is 77.52484512132163\n",
            "Accuracy on batch 1937 on Training is 77.52676728586171\n",
            "Accuracy on batch 1938 on Training is 77.5222408457968\n",
            "Accuracy on batch 1939 on Training is 77.51932989690722\n",
            "Batch Id 1940 is having training loss of 0.8126654028892517\n",
            "0.5186752676963806\n",
            "Accuracy on batch 1940 on Training is 77.52125193199382\n",
            "Accuracy on batch 1941 on Training is 77.5263903192585\n",
            "Accuracy on batch 1942 on Training is 77.52830674215132\n",
            "Accuracy on batch 1943 on Training is 77.52379115226337\n",
            "Accuracy on batch 1944 on Training is 77.5241002570694\n",
            "Accuracy on batch 1945 on Training is 77.52922661870504\n",
            "Accuracy on batch 1946 on Training is 77.53274268104776\n",
            "Accuracy on batch 1947 on Training is 77.52983829568788\n",
            "Accuracy on batch 1948 on Training is 77.52693689071319\n",
            "Accuracy on batch 1949 on Training is 77.52724358974359\n",
            "Accuracy on batch 1950 on Training is 77.52754997437212\n",
            "Accuracy on batch 1951 on Training is 77.52465420081967\n",
            "Accuracy on batch 1952 on Training is 77.5297619047619\n",
            "Accuracy on batch 1953 on Training is 77.52846724667349\n",
            "Accuracy on batch 1954 on Training is 77.53356777493606\n",
            "Accuracy on batch 1955 on Training is 77.52588190184049\n",
            "Accuracy on batch 1956 on Training is 77.52139754726622\n",
            "Accuracy on batch 1957 on Training is 77.52330183861083\n",
            "Accuracy on batch 1958 on Training is 77.52520418580909\n",
            "Accuracy on batch 1959 on Training is 77.52551020408163\n",
            "Batch Id 1960 is having training loss of 0.8120948076248169\n",
            "1.2326385974884033\n",
            "Accuracy on batch 1960 on Training is 77.51944161142275\n",
            "Accuracy on batch 1961 on Training is 77.51815749235475\n",
            "Accuracy on batch 1962 on Training is 77.512098828324\n",
            "Accuracy on batch 1963 on Training is 77.51718431771894\n",
            "Accuracy on batch 1964 on Training is 77.51590330788804\n",
            "Accuracy on batch 1965 on Training is 77.51303407934893\n",
            "Accuracy on batch 1966 on Training is 77.51334519572954\n",
            "Accuracy on batch 1967 on Training is 77.5120680894309\n",
            "Accuracy on batch 1968 on Training is 77.51237938039614\n",
            "Accuracy on batch 1969 on Training is 77.51110406091371\n",
            "Accuracy on batch 1970 on Training is 77.50983003551497\n",
            "Accuracy on batch 1971 on Training is 77.50538793103448\n",
            "Accuracy on batch 1972 on Training is 77.50411809427268\n",
            "Accuracy on batch 1973 on Training is 77.50126646403243\n",
            "Accuracy on batch 1974 on Training is 77.50158227848101\n",
            "Accuracy on batch 1975 on Training is 77.50189777327935\n",
            "Accuracy on batch 1976 on Training is 77.5022129489125\n",
            "Accuracy on batch 1977 on Training is 77.5025278058645\n",
            "Accuracy on batch 1978 on Training is 77.5028423446185\n",
            "Accuracy on batch 1979 on Training is 77.50315656565657\n",
            "Batch Id 1980 is having training loss of 0.8122014403343201\n",
            "0.4689849019050598\n",
            "Accuracy on batch 1980 on Training is 77.50820292781424\n",
            "Accuracy on batch 1981 on Training is 77.51324419778003\n",
            "Accuracy on batch 1982 on Training is 77.51512859304084\n",
            "Accuracy on batch 1983 on Training is 77.51386088709677\n",
            "Accuracy on batch 1984 on Training is 77.51889168765743\n",
            "Accuracy on batch 1985 on Training is 77.52077039274924\n",
            "Accuracy on batch 1986 on Training is 77.52579265223956\n",
            "Accuracy on batch 1987 on Training is 77.52766599597585\n",
            "Accuracy on batch 1988 on Training is 77.52639517345399\n",
            "Accuracy on batch 1989 on Training is 77.52826633165829\n",
            "Accuracy on batch 1990 on Training is 77.52542692114515\n",
            "Accuracy on batch 1991 on Training is 77.52415913654619\n",
            "Accuracy on batch 1992 on Training is 77.5213246362268\n",
            "Accuracy on batch 1993 on Training is 77.52006018054162\n",
            "Accuracy on batch 1994 on Training is 77.5203634085213\n",
            "Accuracy on batch 1995 on Training is 77.5191007014028\n",
            "Accuracy on batch 1996 on Training is 77.52253380070105\n",
            "Accuracy on batch 1997 on Training is 77.52283533533533\n",
            "Accuracy on batch 1998 on Training is 77.52626313156578\n",
            "Accuracy on batch 1999 on Training is 77.528125\n",
            "Batch Id 2000 is having training loss of 0.8110607862472534\n",
            "0.7105603218078613\n",
            "Accuracy on batch 2000 on Training is 77.5268615692154\n",
            "Accuracy on batch 2001 on Training is 77.52247752247752\n",
            "Accuracy on batch 2002 on Training is 77.52121817274089\n",
            "Accuracy on batch 2003 on Training is 77.51372255489022\n",
            "Accuracy on batch 2004 on Training is 77.51402743142144\n",
            "Accuracy on batch 2005 on Training is 77.51744765702891\n",
            "Accuracy on batch 2006 on Training is 77.51930742401595\n",
            "Accuracy on batch 2007 on Training is 77.52272161354581\n",
            "Accuracy on batch 2008 on Training is 77.51991040318566\n",
            "Accuracy on batch 2009 on Training is 77.52021144278606\n",
            "Accuracy on batch 2010 on Training is 77.51740427647937\n",
            "Accuracy on batch 2011 on Training is 77.5239189860835\n",
            "Accuracy on batch 2012 on Training is 77.52576999503229\n",
            "Accuracy on batch 2013 on Training is 77.52451588877855\n",
            "Accuracy on batch 2014 on Training is 77.51861042183623\n",
            "Accuracy on batch 2015 on Training is 77.51736111111111\n",
            "Accuracy on batch 2016 on Training is 77.51611303916708\n",
            "Accuracy on batch 2017 on Training is 77.51331764122894\n",
            "Accuracy on batch 2018 on Training is 77.51052501238237\n",
            "Accuracy on batch 2019 on Training is 77.51237623762377\n",
            "Batch Id 2020 is having training loss of 0.8113500475883484\n",
            "0.8727027773857117\n",
            "Accuracy on batch 2020 on Training is 77.50804057397328\n",
            "Accuracy on batch 2021 on Training is 77.50370919881306\n",
            "Accuracy on batch 2022 on Training is 77.50556104794859\n",
            "Accuracy on batch 2023 on Training is 77.50895503952569\n",
            "Accuracy on batch 2024 on Training is 77.50771604938272\n",
            "Accuracy on batch 2025 on Training is 77.50030848963475\n",
            "Accuracy on batch 2026 on Training is 77.50370004933399\n",
            "Accuracy on batch 2027 on Training is 77.50246548323472\n",
            "Accuracy on batch 2028 on Training is 77.50123213405618\n",
            "Accuracy on batch 2029 on Training is 77.50153940886699\n",
            "Accuracy on batch 2030 on Training is 77.49723042836041\n",
            "Accuracy on batch 2031 on Training is 77.49753937007874\n",
            "Accuracy on batch 2032 on Training is 77.49784800787015\n",
            "Accuracy on batch 2033 on Training is 77.50122910521141\n",
            "Accuracy on batch 2034 on Training is 77.50153562653563\n",
            "Accuracy on batch 2035 on Training is 77.49723722986248\n",
            "Accuracy on batch 2036 on Training is 77.49907952871871\n",
            "Accuracy on batch 2037 on Training is 77.4978532875368\n",
            "Accuracy on batch 2038 on Training is 77.50275870524767\n",
            "Accuracy on batch 2039 on Training is 77.5015318627451\n",
            "Batch Id 2040 is having training loss of 0.8115438222885132\n",
            "0.6989312767982483\n",
            "Accuracy on batch 2040 on Training is 77.50489955903969\n",
            "Accuracy on batch 2041 on Training is 77.508263956905\n",
            "Accuracy on batch 2042 on Training is 77.51315467449828\n",
            "Accuracy on batch 2043 on Training is 77.51192514677103\n",
            "Accuracy on batch 2044 on Training is 77.51375305623472\n",
            "Accuracy on batch 2045 on Training is 77.51710654936461\n",
            "Accuracy on batch 2046 on Training is 77.51893014167074\n",
            "Accuracy on batch 2047 on Training is 77.5177001953125\n",
            "Accuracy on batch 2048 on Training is 77.51647144948755\n",
            "Accuracy on batch 2049 on Training is 77.51371951219512\n",
            "Accuracy on batch 2050 on Training is 77.50792296440761\n",
            "Accuracy on batch 2051 on Training is 77.50517787524366\n",
            "Accuracy on batch 2052 on Training is 77.50852411105699\n",
            "Accuracy on batch 2053 on Training is 77.51338851022395\n",
            "Accuracy on batch 2054 on Training is 77.52128953771289\n",
            "Accuracy on batch 2055 on Training is 77.52766293774319\n",
            "Accuracy on batch 2056 on Training is 77.51731891103549\n",
            "Accuracy on batch 2057 on Training is 77.52065111758989\n",
            "Accuracy on batch 2058 on Training is 77.51942690626518\n",
            "Accuracy on batch 2059 on Training is 77.52123786407768\n",
            "Batch Id 2060 is having training loss of 0.8111456036567688\n",
            "0.4464770555496216\n",
            "Accuracy on batch 2060 on Training is 77.52759582726831\n",
            "Accuracy on batch 2061 on Training is 77.530916585839\n",
            "Accuracy on batch 2062 on Training is 77.53271934076588\n",
            "Accuracy on batch 2063 on Training is 77.53149224806202\n",
            "Accuracy on batch 2064 on Training is 77.5272397094431\n",
            "Accuracy on batch 2065 on Training is 77.52450387221684\n",
            "Accuracy on batch 2066 on Training is 77.52781809385583\n",
            "Accuracy on batch 2067 on Training is 77.52508462282398\n",
            "Accuracy on batch 2068 on Training is 77.52537457709037\n",
            "Accuracy on batch 2069 on Training is 77.52415458937197\n",
            "Accuracy on batch 2070 on Training is 77.52444471269918\n",
            "Accuracy on batch 2071 on Training is 77.52473455598455\n",
            "Accuracy on batch 2072 on Training is 77.52502411963339\n",
            "Accuracy on batch 2073 on Training is 77.5283269045323\n",
            "Accuracy on batch 2074 on Training is 77.53313253012048\n",
            "Accuracy on batch 2075 on Training is 77.5364282273603\n",
            "Accuracy on batch 2076 on Training is 77.5397207510833\n",
            "Accuracy on batch 2077 on Training is 77.53248315688161\n",
            "Accuracy on batch 2078 on Training is 77.53878066378067\n",
            "Accuracy on batch 2079 on Training is 77.54056490384616\n",
            "Batch Id 2080 is having training loss of 0.8104090690612793\n",
            "0.28943902254104614\n",
            "Accuracy on batch 2080 on Training is 77.54835415665545\n",
            "Accuracy on batch 2081 on Training is 77.55613592699328\n",
            "Accuracy on batch 2082 on Training is 77.55640902544407\n",
            "Accuracy on batch 2083 on Training is 77.55818138195778\n",
            "Accuracy on batch 2084 on Training is 77.56145083932854\n",
            "Accuracy on batch 2085 on Training is 77.5647171620326\n",
            "Accuracy on batch 2086 on Training is 77.56798035457595\n",
            "Accuracy on batch 2087 on Training is 77.57273706896552\n",
            "Accuracy on batch 2088 on Training is 77.57599329822882\n",
            "Accuracy on batch 2089 on Training is 77.58223684210526\n",
            "Accuracy on batch 2090 on Training is 77.58249641319942\n",
            "Accuracy on batch 2091 on Training is 77.58424952198853\n",
            "Accuracy on batch 2092 on Training is 77.57853559483995\n",
            "Accuracy on batch 2093 on Training is 77.58327363896848\n",
            "Accuracy on batch 2094 on Training is 77.58204057279237\n",
            "Accuracy on batch 2095 on Training is 77.58379055343511\n",
            "Accuracy on batch 2096 on Training is 77.58106819265618\n",
            "Accuracy on batch 2097 on Training is 77.58132745471877\n",
            "Accuracy on batch 2098 on Training is 77.58307527393997\n",
            "Accuracy on batch 2099 on Training is 77.58184523809524\n",
            "Batch Id 2100 is having training loss of 0.81027752161026\n",
            "1.091732144355774\n",
            "Accuracy on batch 2100 on Training is 77.58210376011424\n",
            "Accuracy on batch 2101 on Training is 77.57938867745004\n",
            "Accuracy on batch 2102 on Training is 77.5751902044698\n",
            "Accuracy on batch 2103 on Training is 77.56951045627376\n",
            "Accuracy on batch 2104 on Training is 77.57125890736341\n",
            "Accuracy on batch 2105 on Training is 77.56855413105413\n",
            "Accuracy on batch 2106 on Training is 77.57326767916469\n",
            "Accuracy on batch 2107 on Training is 77.57945920303605\n",
            "Accuracy on batch 2108 on Training is 77.5856448553817\n",
            "Accuracy on batch 2109 on Training is 77.58738151658768\n",
            "Accuracy on batch 2110 on Training is 77.59207721459025\n",
            "Accuracy on batch 2111 on Training is 77.59824810606061\n",
            "Accuracy on batch 2112 on Training is 77.60293421675343\n",
            "Accuracy on batch 2113 on Training is 77.60022469252601\n",
            "Accuracy on batch 2114 on Training is 77.59751773049645\n",
            "Accuracy on batch 2115 on Training is 77.60515122873346\n",
            "Accuracy on batch 2116 on Training is 77.60687293339632\n",
            "Accuracy on batch 2117 on Training is 77.60859301227573\n",
            "Accuracy on batch 2118 on Training is 77.60736196319019\n",
            "Accuracy on batch 2119 on Training is 77.60908018867924\n",
            "Batch Id 2120 is having training loss of 0.8093591332435608\n",
            "1.006432294845581\n",
            "Accuracy on batch 2120 on Training is 77.60637670909948\n",
            "Accuracy on batch 2121 on Training is 77.60809377945334\n",
            "Accuracy on batch 2122 on Training is 77.60539331135186\n",
            "Accuracy on batch 2123 on Training is 77.60416666666667\n",
            "Accuracy on batch 2124 on Training is 77.60441176470589\n",
            "Accuracy on batch 2125 on Training is 77.60759642521167\n",
            "Accuracy on batch 2126 on Training is 77.60490126939351\n",
            "Accuracy on batch 2127 on Training is 77.60514567669173\n",
            "Accuracy on batch 2128 on Training is 77.60392202912165\n",
            "Accuracy on batch 2129 on Training is 77.60416666666667\n",
            "Accuracy on batch 2130 on Training is 77.60147817925856\n",
            "Accuracy on batch 2131 on Training is 77.60318949343339\n",
            "Accuracy on batch 2132 on Training is 77.60636427566807\n",
            "Accuracy on batch 2133 on Training is 77.60074976569823\n",
            "Accuracy on batch 2134 on Training is 77.60245901639344\n",
            "Accuracy on batch 2135 on Training is 77.60709269662921\n",
            "Accuracy on batch 2136 on Training is 77.61172204024334\n",
            "Accuracy on batch 2137 on Training is 77.60903882132834\n",
            "Accuracy on batch 2138 on Training is 77.61512388966807\n",
            "Accuracy on batch 2139 on Training is 77.61244158878505\n",
            "Batch Id 2140 is having training loss of 0.8086275458335876\n",
            "0.4944697618484497\n",
            "Accuracy on batch 2140 on Training is 77.61705978514713\n",
            "Accuracy on batch 2141 on Training is 77.62167366946778\n",
            "Accuracy on batch 2142 on Training is 77.6219085394307\n",
            "Accuracy on batch 2143 on Training is 77.62360074626865\n",
            "Accuracy on batch 2144 on Training is 77.62674825174825\n",
            "Accuracy on batch 2145 on Training is 77.62552423112768\n",
            "Accuracy on batch 2146 on Training is 77.62721238938053\n",
            "Accuracy on batch 2147 on Training is 77.63180865921788\n",
            "Accuracy on batch 2148 on Training is 77.63785481619358\n",
            "Accuracy on batch 2149 on Training is 77.63953488372093\n",
            "Accuracy on batch 2150 on Training is 77.63830776383078\n",
            "Accuracy on batch 2151 on Training is 77.63853392193309\n",
            "Accuracy on batch 2152 on Training is 77.64311425917325\n",
            "Accuracy on batch 2153 on Training is 77.64333797585887\n",
            "Accuracy on batch 2154 on Training is 77.64356148491879\n",
            "Accuracy on batch 2155 on Training is 77.64378478664193\n",
            "Accuracy on batch 2156 on Training is 77.64400788131664\n",
            "Accuracy on batch 2157 on Training is 77.63843836886005\n",
            "Accuracy on batch 2158 on Training is 77.63866373320982\n",
            "Accuracy on batch 2159 on Training is 77.64033564814815\n",
            "Batch Id 2160 is having training loss of 0.808060884475708\n",
            "0.8116350769996643\n",
            "Accuracy on batch 2160 on Training is 77.6405599259602\n",
            "Accuracy on batch 2161 on Training is 77.64801110083256\n",
            "Accuracy on batch 2162 on Training is 77.65112112806287\n",
            "Accuracy on batch 2163 on Training is 77.6556723659889\n",
            "Accuracy on batch 2164 on Training is 77.65588914549653\n",
            "Accuracy on batch 2165 on Training is 77.65466297322253\n",
            "Accuracy on batch 2166 on Training is 77.6577641901246\n",
            "Accuracy on batch 2167 on Training is 77.66086254612546\n",
            "Accuracy on batch 2168 on Training is 77.6610765329645\n",
            "Accuracy on batch 2169 on Training is 77.66417050691244\n",
            "Accuracy on batch 2170 on Training is 77.6643827729157\n",
            "Accuracy on batch 2171 on Training is 77.66747237569061\n",
            "Accuracy on batch 2172 on Training is 77.66624482282559\n",
            "Accuracy on batch 2173 on Training is 77.66358095676173\n",
            "Accuracy on batch 2174 on Training is 77.66235632183908\n",
            "Accuracy on batch 2175 on Training is 77.65826056985294\n",
            "Accuracy on batch 2176 on Training is 77.66278135048232\n",
            "Accuracy on batch 2177 on Training is 77.66442837465564\n",
            "Accuracy on batch 2178 on Training is 77.66750803120698\n",
            "Accuracy on batch 2179 on Training is 77.67058486238533\n",
            "Batch Id 2180 is having training loss of 0.8073394894599915\n",
            "0.6759811043739319\n",
            "Accuracy on batch 2180 on Training is 77.67079321412196\n",
            "Accuracy on batch 2181 on Training is 77.67100137488542\n",
            "Accuracy on batch 2182 on Training is 77.67120934493816\n",
            "Accuracy on batch 2183 on Training is 77.66569368131869\n",
            "Accuracy on batch 2184 on Training is 77.66447368421052\n",
            "Accuracy on batch 2185 on Training is 77.66611390667886\n",
            "Accuracy on batch 2186 on Training is 77.67346822130773\n",
            "Accuracy on batch 2187 on Training is 77.66938985374772\n",
            "Accuracy on batch 2188 on Training is 77.66959798994975\n",
            "Accuracy on batch 2189 on Training is 77.66552511415524\n",
            "Accuracy on batch 2190 on Training is 77.66716111364674\n",
            "Accuracy on batch 2191 on Training is 77.6616674270073\n",
            "Accuracy on batch 2192 on Training is 77.66187870497036\n",
            "Accuracy on batch 2193 on Training is 77.66493846855059\n",
            "Accuracy on batch 2194 on Training is 77.66372437357631\n",
            "Accuracy on batch 2195 on Training is 77.66678051001821\n",
            "Accuracy on batch 2196 on Training is 77.65845471096951\n",
            "Accuracy on batch 2197 on Training is 77.66151046405824\n",
            "Accuracy on batch 2198 on Training is 77.66030013642565\n",
            "Accuracy on batch 2199 on Training is 77.66051136363636\n",
            "Batch Id 2200 is having training loss of 0.8072264790534973\n",
            "0.9248273968696594\n",
            "Accuracy on batch 2200 on Training is 77.66072239890958\n",
            "Accuracy on batch 2201 on Training is 77.6595140781108\n",
            "Accuracy on batch 2202 on Training is 77.6583068542896\n",
            "Accuracy on batch 2203 on Training is 77.66135435571688\n",
            "Accuracy on batch 2204 on Training is 77.66298185941044\n",
            "Accuracy on batch 2205 on Training is 77.66177470534905\n",
            "Accuracy on batch 2206 on Training is 77.657736746715\n",
            "Accuracy on batch 2207 on Training is 77.65936367753623\n",
            "Accuracy on batch 2208 on Training is 77.66240380262562\n",
            "Accuracy on batch 2209 on Training is 77.66119909502262\n",
            "Accuracy on batch 2210 on Training is 77.66140886476707\n",
            "Accuracy on batch 2211 on Training is 77.6686821880651\n",
            "Accuracy on batch 2212 on Training is 77.67312471757795\n",
            "Accuracy on batch 2213 on Training is 77.66909439927733\n",
            "Accuracy on batch 2214 on Training is 77.66647855530474\n",
            "Accuracy on batch 2215 on Training is 77.66527527075812\n",
            "Accuracy on batch 2216 on Training is 77.66125394677492\n",
            "Accuracy on batch 2217 on Training is 77.65864517583408\n",
            "Accuracy on batch 2218 on Training is 77.66026363226679\n",
            "Accuracy on batch 2219 on Training is 77.65625\n",
            "Batch Id 2220 is having training loss of 0.8073060512542725\n",
            "0.5232909917831421\n",
            "Accuracy on batch 2220 on Training is 77.6578680774426\n",
            "Accuracy on batch 2221 on Training is 77.65385913591359\n",
            "Accuracy on batch 2222 on Training is 77.65547683310841\n",
            "Accuracy on batch 2223 on Training is 77.65709307553956\n",
            "Accuracy on batch 2224 on Training is 77.65730337078652\n",
            "Accuracy on batch 2225 on Training is 77.65330188679245\n",
            "Accuracy on batch 2226 on Training is 77.66052986079929\n",
            "Accuracy on batch 2227 on Training is 77.65793312387791\n",
            "Accuracy on batch 2228 on Training is 77.66094661283087\n",
            "Accuracy on batch 2229 on Training is 77.65695067264573\n",
            "Accuracy on batch 2230 on Training is 77.66136261766025\n",
            "Accuracy on batch 2231 on Training is 77.66297043010752\n",
            "Accuracy on batch 2232 on Training is 77.66317733990148\n",
            "Accuracy on batch 2233 on Training is 77.66758057296329\n",
            "Accuracy on batch 2234 on Training is 77.66918344519016\n",
            "Accuracy on batch 2235 on Training is 77.67078488372093\n",
            "Accuracy on batch 2236 on Training is 77.66679704962003\n",
            "Accuracy on batch 2237 on Training is 77.6656054512958\n",
            "Accuracy on batch 2238 on Training is 77.66162349263064\n",
            "Accuracy on batch 2239 on Training is 77.65764508928571\n",
            "Batch Id 2240 is having training loss of 0.8077002167701721\n",
            "0.7223884463310242\n",
            "Accuracy on batch 2240 on Training is 77.66064257028113\n",
            "Accuracy on batch 2241 on Training is 77.66363737734166\n",
            "Accuracy on batch 2242 on Training is 77.66941596076683\n",
            "Accuracy on batch 2243 on Training is 77.66822638146168\n",
            "Accuracy on batch 2244 on Training is 77.67260579064587\n",
            "Accuracy on batch 2245 on Training is 77.67837266251114\n",
            "Accuracy on batch 2246 on Training is 77.67857142857143\n",
            "Accuracy on batch 2247 on Training is 77.67598976868328\n",
            "Accuracy on batch 2248 on Training is 77.6761894175189\n",
            "Accuracy on batch 2249 on Training is 77.675\n",
            "Accuracy on batch 2250 on Training is 77.6710350955131\n",
            "Accuracy on batch 2251 on Training is 77.671236678508\n",
            "Accuracy on batch 2252 on Training is 77.67282512205948\n",
            "Accuracy on batch 2253 on Training is 77.67163930789707\n",
            "Accuracy on batch 2254 on Training is 77.67322616407982\n",
            "Accuracy on batch 2255 on Training is 77.67619680851064\n",
            "Accuracy on batch 2256 on Training is 77.67639565795304\n",
            "Accuracy on batch 2257 on Training is 77.67936226749336\n",
            "Accuracy on batch 2258 on Training is 77.68232625055334\n",
            "Accuracy on batch 2259 on Training is 77.68113938053098\n",
            "Batch Id 2260 is having training loss of 0.8068095445632935\n",
            "0.9601221680641174\n",
            "Accuracy on batch 2260 on Training is 77.68409995577179\n",
            "Accuracy on batch 2261 on Training is 77.68705791335101\n",
            "Accuracy on batch 2262 on Training is 77.68863234644277\n",
            "Accuracy on batch 2263 on Training is 77.69434628975264\n",
            "Accuracy on batch 2264 on Training is 77.69453642384106\n",
            "Accuracy on batch 2265 on Training is 77.69472639011474\n",
            "Accuracy on batch 2266 on Training is 77.69215924128805\n",
            "Accuracy on batch 2267 on Training is 77.69235008818342\n",
            "Accuracy on batch 2268 on Training is 77.68565447333627\n",
            "Accuracy on batch 2269 on Training is 77.68584801762114\n",
            "Accuracy on batch 2270 on Training is 77.68604139145751\n",
            "Accuracy on batch 2271 on Training is 77.68623459507042\n",
            "Accuracy on batch 2272 on Training is 77.68505279366477\n",
            "Accuracy on batch 2273 on Training is 77.69074318381706\n",
            "Accuracy on batch 2274 on Training is 77.69368131868131\n",
            "Accuracy on batch 2275 on Training is 77.69524384885764\n",
            "Accuracy on batch 2276 on Training is 77.69543258673693\n",
            "Accuracy on batch 2277 on Training is 77.69562115891132\n",
            "Accuracy on batch 2278 on Training is 77.69032470381747\n",
            "Accuracy on batch 2279 on Training is 77.69188596491227\n",
            "Batch Id 2280 is having training loss of 0.8066441416740417\n",
            "0.278338760137558\n",
            "Accuracy on batch 2280 on Training is 77.69892590968874\n",
            "Accuracy on batch 2281 on Training is 77.69911262050833\n",
            "Accuracy on batch 2282 on Training is 77.69929916776172\n",
            "Accuracy on batch 2283 on Training is 77.69948555166374\n",
            "Accuracy on batch 2284 on Training is 77.69830415754923\n",
            "Accuracy on batch 2285 on Training is 77.69438976377953\n",
            "Accuracy on batch 2286 on Training is 77.69457804984697\n",
            "Accuracy on batch 2287 on Training is 77.69476617132867\n",
            "Accuracy on batch 2288 on Training is 77.69495412844037\n",
            "Accuracy on batch 2289 on Training is 77.69923580786026\n",
            "Accuracy on batch 2290 on Training is 77.70487778262768\n",
            "Accuracy on batch 2291 on Training is 77.70369764397905\n",
            "Accuracy on batch 2292 on Training is 77.70388137810728\n",
            "Accuracy on batch 2293 on Training is 77.70406495204882\n",
            "Accuracy on batch 2294 on Training is 77.70424836601308\n",
            "Accuracy on batch 2295 on Training is 77.70170949477352\n",
            "Accuracy on batch 2296 on Training is 77.69917283413147\n",
            "Accuracy on batch 2297 on Training is 77.70615752828546\n",
            "Accuracy on batch 2298 on Training is 77.70498042627229\n",
            "Accuracy on batch 2299 on Training is 77.7038043478261\n",
            "Batch Id 2300 is having training loss of 0.806263267993927\n",
            "0.5495059490203857\n",
            "Accuracy on batch 2300 on Training is 77.708061712299\n",
            "Accuracy on batch 2301 on Training is 77.70417028670721\n",
            "Accuracy on batch 2302 on Training is 77.7057099435519\n",
            "Accuracy on batch 2303 on Training is 77.70724826388889\n",
            "Accuracy on batch 2304 on Training is 77.70471800433839\n",
            "Accuracy on batch 2305 on Training is 77.70490026019081\n",
            "Accuracy on batch 2306 on Training is 77.70237321196359\n",
            "Accuracy on batch 2307 on Training is 77.70255632582322\n",
            "Accuracy on batch 2308 on Training is 77.70544608055435\n",
            "Accuracy on batch 2309 on Training is 77.70292207792208\n",
            "Accuracy on batch 2310 on Training is 77.7017524881004\n",
            "Accuracy on batch 2311 on Training is 77.70463884083046\n",
            "Accuracy on batch 2312 on Training is 77.70617163856464\n",
            "Accuracy on batch 2313 on Training is 77.70500216076059\n",
            "Accuracy on batch 2314 on Training is 77.70383369330453\n",
            "Accuracy on batch 2315 on Training is 77.70671416234887\n",
            "Accuracy on batch 2316 on Training is 77.7082434182132\n",
            "Accuracy on batch 2317 on Training is 77.70707506471096\n",
            "Accuracy on batch 2318 on Training is 77.71399310047434\n",
            "Accuracy on batch 2319 on Training is 77.71417025862068\n",
            "Batch Id 2320 is having training loss of 0.806267499923706\n",
            "1.3335827589035034\n",
            "Accuracy on batch 2320 on Training is 77.70896165445929\n",
            "Accuracy on batch 2321 on Training is 77.71048664944014\n",
            "Accuracy on batch 2322 on Training is 77.70528411536806\n",
            "Accuracy on batch 2323 on Training is 77.70546471600689\n",
            "Accuracy on batch 2324 on Training is 77.70295698924731\n",
            "Accuracy on batch 2325 on Training is 77.70313843508168\n",
            "Accuracy on batch 2326 on Training is 77.69794800171896\n",
            "Accuracy on batch 2327 on Training is 77.69141967353951\n",
            "Accuracy on batch 2328 on Training is 77.69160583941606\n",
            "Accuracy on batch 2329 on Training is 77.6931330472103\n",
            "Accuracy on batch 2330 on Training is 77.69063706563706\n",
            "Accuracy on batch 2331 on Training is 77.69216337907376\n",
            "Accuracy on batch 2332 on Training is 77.69100942991857\n",
            "Accuracy on batch 2333 on Training is 77.69253427592116\n",
            "Accuracy on batch 2334 on Training is 77.688704496788\n",
            "Accuracy on batch 2335 on Training is 77.68755351027397\n",
            "Accuracy on batch 2336 on Training is 77.68640350877193\n",
            "Accuracy on batch 2337 on Training is 77.69060094097519\n",
            "Accuracy on batch 2338 on Training is 77.6921227020094\n",
            "Accuracy on batch 2339 on Training is 77.69497863247864\n",
            "Batch Id 2340 is having training loss of 0.8065465688705444\n",
            "0.9763514995574951\n",
            "Accuracy on batch 2340 on Training is 77.69249252456216\n",
            "Accuracy on batch 2341 on Training is 77.69134286934245\n",
            "Accuracy on batch 2342 on Training is 77.69152795561246\n",
            "Accuracy on batch 2343 on Training is 77.69704564846417\n",
            "Accuracy on batch 2344 on Training is 77.69856076759062\n",
            "Accuracy on batch 2345 on Training is 77.70140664961637\n",
            "Accuracy on batch 2346 on Training is 77.70425010651896\n",
            "Accuracy on batch 2347 on Training is 77.70576022146507\n",
            "Accuracy on batch 2348 on Training is 77.70992975734355\n",
            "Accuracy on batch 2349 on Training is 77.70744680851064\n",
            "Accuracy on batch 2350 on Training is 77.70895363675032\n",
            "Accuracy on batch 2351 on Training is 77.70913052721089\n",
            "Accuracy on batch 2352 on Training is 77.70930726731832\n",
            "Accuracy on batch 2353 on Training is 77.70682880203908\n",
            "Accuracy on batch 2354 on Training is 77.70700636942675\n",
            "Accuracy on batch 2355 on Training is 77.71248938879457\n",
            "Accuracy on batch 2356 on Training is 77.71796775562156\n",
            "Accuracy on batch 2357 on Training is 77.72476675148431\n",
            "Accuracy on batch 2358 on Training is 77.72493641373464\n",
            "Accuracy on batch 2359 on Training is 77.72113347457628\n",
            "Batch Id 2360 is having training loss of 0.8056701421737671\n",
            "0.48510977625846863\n",
            "Accuracy on batch 2360 on Training is 77.72527530707328\n",
            "Accuracy on batch 2361 on Training is 77.72809060118544\n",
            "Accuracy on batch 2362 on Training is 77.72958104104951\n",
            "Accuracy on batch 2363 on Training is 77.73107021996616\n",
            "Accuracy on batch 2364 on Training is 77.72727272727273\n",
            "Accuracy on batch 2365 on Training is 77.73140321217244\n",
            "Accuracy on batch 2366 on Training is 77.72760878749472\n",
            "Accuracy on batch 2367 on Training is 77.72909628378379\n",
            "Accuracy on batch 2368 on Training is 77.72794428028705\n",
            "Accuracy on batch 2369 on Training is 77.728111814346\n",
            "Accuracy on batch 2370 on Training is 77.73223323492198\n",
            "Accuracy on batch 2371 on Training is 77.73371627318718\n",
            "Accuracy on batch 2372 on Training is 77.73651495996629\n",
            "Accuracy on batch 2373 on Training is 77.74062763268745\n",
            "Accuracy on batch 2374 on Training is 77.74342105263158\n",
            "Accuracy on batch 2375 on Training is 77.74095117845118\n",
            "Accuracy on batch 2376 on Training is 77.7450567942785\n",
            "Accuracy on batch 2377 on Training is 77.74258830950379\n",
            "Accuracy on batch 2378 on Training is 77.7440626313577\n",
            "Accuracy on batch 2379 on Training is 77.74816176470588\n",
            "Batch Id 2380 is having training loss of 0.8043191432952881\n",
            "0.4790218770503998\n",
            "Accuracy on batch 2380 on Training is 77.75094498110037\n",
            "Accuracy on batch 2381 on Training is 77.75110201511335\n",
            "Accuracy on batch 2382 on Training is 77.7499475451112\n",
            "Accuracy on batch 2383 on Training is 77.75010486577182\n",
            "Accuracy on batch 2384 on Training is 77.74633123689728\n",
            "Accuracy on batch 2385 on Training is 77.75303855825649\n",
            "Accuracy on batch 2386 on Training is 77.7531943862589\n",
            "Accuracy on batch 2387 on Training is 77.75596733668341\n",
            "Accuracy on batch 2388 on Training is 77.75088949351193\n",
            "Accuracy on batch 2389 on Training is 77.7510460251046\n",
            "Accuracy on batch 2390 on Training is 77.75120242576328\n",
            "Accuracy on batch 2391 on Training is 77.75397157190635\n",
            "Accuracy on batch 2392 on Training is 77.7541266193063\n",
            "Accuracy on batch 2393 on Training is 77.75950292397661\n",
            "Accuracy on batch 2394 on Training is 77.76617954070981\n",
            "Accuracy on batch 2395 on Training is 77.76502504173622\n",
            "Accuracy on batch 2396 on Training is 77.75344180225282\n",
            "Accuracy on batch 2397 on Training is 77.75880942452044\n",
            "Accuracy on batch 2398 on Training is 77.76156731971655\n",
            "Accuracy on batch 2399 on Training is 77.76302083333333\n",
            "Batch Id 2400 is having training loss of 0.8035979270935059\n",
            "0.6603348255157471\n",
            "Accuracy on batch 2400 on Training is 77.76447313619325\n",
            "Accuracy on batch 2401 on Training is 77.76592422980849\n",
            "Accuracy on batch 2402 on Training is 77.76347274240533\n",
            "Accuracy on batch 2403 on Training is 77.76232321131448\n",
            "Accuracy on batch 2404 on Training is 77.76247401247402\n",
            "Accuracy on batch 2405 on Training is 77.76522236076475\n",
            "Accuracy on batch 2406 on Training is 77.76407353552139\n",
            "Accuracy on batch 2407 on Training is 77.77071220930233\n",
            "Accuracy on batch 2408 on Training is 77.76696762141968\n",
            "Accuracy on batch 2409 on Training is 77.76452282157676\n",
            "Accuracy on batch 2410 on Training is 77.76078390709249\n",
            "Accuracy on batch 2411 on Training is 77.76482172470979\n",
            "Accuracy on batch 2412 on Training is 77.76238085370908\n",
            "Accuracy on batch 2413 on Training is 77.76512013256007\n",
            "Accuracy on batch 2414 on Training is 77.76656314699792\n",
            "Accuracy on batch 2415 on Training is 77.76800496688742\n",
            "Accuracy on batch 2416 on Training is 77.77073851882498\n",
            "Accuracy on batch 2417 on Training is 77.77346980976013\n",
            "Accuracy on batch 2418 on Training is 77.7736151302191\n",
            "Accuracy on batch 2419 on Training is 77.77117768595042\n",
            "Batch Id 2420 is having training loss of 0.8031465411186218\n",
            "1.0078184604644775\n",
            "Accuracy on batch 2420 on Training is 77.76486988847584\n",
            "Accuracy on batch 2421 on Training is 77.76372832369943\n",
            "Accuracy on batch 2422 on Training is 77.77032604209657\n",
            "Accuracy on batch 2423 on Training is 77.7678939768977\n",
            "Accuracy on batch 2424 on Training is 77.76932989690722\n",
            "Accuracy on batch 2425 on Training is 77.76947650453421\n",
            "Accuracy on batch 2426 on Training is 77.77091058920477\n",
            "Accuracy on batch 2427 on Training is 77.76848228995058\n",
            "Accuracy on batch 2428 on Training is 77.76991560312887\n",
            "Accuracy on batch 2429 on Training is 77.76877572016461\n",
            "Accuracy on batch 2430 on Training is 77.76378033730975\n",
            "Accuracy on batch 2431 on Training is 77.76007401315789\n",
            "Accuracy on batch 2432 on Training is 77.76150842581175\n",
            "Accuracy on batch 2433 on Training is 77.76037387017256\n",
            "Accuracy on batch 2434 on Training is 77.75410677618069\n",
            "Accuracy on batch 2435 on Training is 77.75425903119869\n",
            "Accuracy on batch 2436 on Training is 77.75697578990562\n",
            "Accuracy on batch 2437 on Training is 77.75712674323216\n",
            "Accuracy on batch 2438 on Training is 77.75984009840099\n",
            "Accuracy on batch 2439 on Training is 77.75998975409836\n",
            "Batch Id 2440 is having training loss of 0.8028701543807983\n",
            "0.6923690438270569\n",
            "Accuracy on batch 2440 on Training is 77.76141950020484\n",
            "Accuracy on batch 2441 on Training is 77.76284807534807\n",
            "Accuracy on batch 2442 on Training is 77.76555464592714\n",
            "Accuracy on batch 2443 on Training is 77.76698036006546\n",
            "Accuracy on batch 2444 on Training is 77.76840490797547\n",
            "Accuracy on batch 2445 on Training is 77.76599550286181\n",
            "Accuracy on batch 2446 on Training is 77.76741928892521\n",
            "Accuracy on batch 2447 on Training is 77.76373570261438\n",
            "Accuracy on batch 2448 on Training is 77.76388321763986\n",
            "Accuracy on batch 2449 on Training is 77.7640306122449\n",
            "Accuracy on batch 2450 on Training is 77.76290289677682\n",
            "Accuracy on batch 2451 on Training is 77.76050163132138\n",
            "Accuracy on batch 2452 on Training is 77.76192417448023\n",
            "Accuracy on batch 2453 on Training is 77.76461898940505\n",
            "Accuracy on batch 2454 on Training is 77.76476578411405\n",
            "Accuracy on batch 2455 on Training is 77.767457247557\n",
            "Accuracy on batch 2456 on Training is 77.76505901505901\n",
            "Accuracy on batch 2457 on Training is 77.76139137510171\n",
            "Accuracy on batch 2458 on Training is 77.75899755998374\n",
            "Accuracy on batch 2459 on Training is 77.76295731707317\n",
            "Batch Id 2460 is having training loss of 0.8023952841758728\n",
            "0.6299138069152832\n",
            "Accuracy on batch 2460 on Training is 77.76183462007315\n",
            "Accuracy on batch 2461 on Training is 77.76452071486597\n",
            "Accuracy on batch 2462 on Training is 77.76339829476248\n",
            "Accuracy on batch 2463 on Training is 77.76608157467533\n",
            "Accuracy on batch 2464 on Training is 77.76876267748479\n",
            "Accuracy on batch 2465 on Training is 77.7676399026764\n",
            "Accuracy on batch 2466 on Training is 77.77285164167004\n",
            "Accuracy on batch 2467 on Training is 77.77426053484604\n",
            "Accuracy on batch 2468 on Training is 77.77819967598218\n",
            "Accuracy on batch 2469 on Training is 77.77834008097166\n",
            "Accuracy on batch 2470 on Training is 77.77468636179684\n",
            "Accuracy on batch 2471 on Training is 77.7760922330097\n",
            "Accuracy on batch 2472 on Training is 77.77749696724626\n",
            "Accuracy on batch 2473 on Training is 77.78016370250606\n",
            "Accuracy on batch 2474 on Training is 77.78156565656566\n",
            "Accuracy on batch 2475 on Training is 77.78296647819063\n",
            "Accuracy on batch 2476 on Training is 77.78310456197012\n",
            "Accuracy on batch 2477 on Training is 77.78450363196126\n",
            "Accuracy on batch 2478 on Training is 77.78464098426785\n",
            "Accuracy on batch 2479 on Training is 77.78603830645162\n",
            "Batch Id 2480 is having training loss of 0.8015700578689575\n",
            "0.9699665307998657\n",
            "Accuracy on batch 2480 on Training is 77.78365578395808\n",
            "Accuracy on batch 2481 on Training is 77.78757050765512\n",
            "Accuracy on batch 2482 on Training is 77.79274063632703\n",
            "Accuracy on batch 2483 on Training is 77.79287439613526\n",
            "Accuracy on batch 2484 on Training is 77.79426559356136\n",
            "Accuracy on batch 2485 on Training is 77.79314159292035\n",
            "Accuracy on batch 2486 on Training is 77.7945315641335\n",
            "Accuracy on batch 2487 on Training is 77.79592041800643\n",
            "Accuracy on batch 2488 on Training is 77.7973081558859\n",
            "Accuracy on batch 2489 on Training is 77.79492971887551\n",
            "Accuracy on batch 2490 on Training is 77.79255319148936\n",
            "Accuracy on batch 2491 on Training is 77.79268659711076\n",
            "Accuracy on batch 2492 on Training is 77.79783393501805\n",
            "Accuracy on batch 2493 on Training is 77.79921812349639\n",
            "Accuracy on batch 2494 on Training is 77.79684368737475\n",
            "Accuracy on batch 2495 on Training is 77.80198317307692\n",
            "Accuracy on batch 2496 on Training is 77.79960953143772\n",
            "Accuracy on batch 2497 on Training is 77.80099079263411\n",
            "Accuracy on batch 2498 on Training is 77.8048719487795\n",
            "Accuracy on batch 2499 on Training is 77.8075\n",
            "Batch Id 2500 is having training loss of 0.8012388348579407\n",
            "0.6905118823051453\n",
            "Accuracy on batch 2500 on Training is 77.8076269492203\n",
            "Accuracy on batch 2501 on Training is 77.80650479616307\n",
            "Accuracy on batch 2502 on Training is 77.80788054334798\n",
            "Accuracy on batch 2503 on Training is 77.80176717252397\n",
            "Accuracy on batch 2504 on Training is 77.79815369261478\n",
            "Accuracy on batch 2505 on Training is 77.79953112529928\n",
            "Accuracy on batch 2506 on Training is 77.80589349820502\n",
            "Accuracy on batch 2507 on Training is 77.80726674641149\n",
            "Accuracy on batch 2508 on Training is 77.80614786767636\n",
            "Accuracy on batch 2509 on Training is 77.81125498007968\n",
            "Accuracy on batch 2510 on Training is 77.8126244524094\n",
            "Accuracy on batch 2511 on Training is 77.81150477707007\n",
            "Accuracy on batch 2512 on Training is 77.81038599283724\n",
            "Accuracy on batch 2513 on Training is 77.80926809864758\n",
            "Accuracy on batch 2514 on Training is 77.80939363817097\n",
            "Accuracy on batch 2515 on Training is 77.80703497615262\n",
            "Accuracy on batch 2516 on Training is 77.80591974572904\n",
            "Accuracy on batch 2517 on Training is 77.80232327243844\n",
            "Accuracy on batch 2518 on Training is 77.79872965462485\n",
            "Accuracy on batch 2519 on Training is 77.80133928571429\n",
            "Batch Id 2520 is having training loss of 0.8010767102241516\n",
            "0.8966072797775269\n",
            "Accuracy on batch 2520 on Training is 77.80022808409362\n",
            "Accuracy on batch 2521 on Training is 77.79911776367962\n",
            "Accuracy on batch 2522 on Training is 77.80296274276655\n",
            "Accuracy on batch 2523 on Training is 77.80309033280507\n",
            "Accuracy on batch 2524 on Training is 77.80074257425743\n",
            "Accuracy on batch 2525 on Training is 77.8045823436263\n",
            "Accuracy on batch 2526 on Training is 77.80470914127424\n",
            "Accuracy on batch 2527 on Training is 77.8035996835443\n",
            "Accuracy on batch 2528 on Training is 77.80619810201661\n",
            "Accuracy on batch 2529 on Training is 77.8026185770751\n",
            "Accuracy on batch 2530 on Training is 77.80027657052548\n",
            "Accuracy on batch 2531 on Training is 77.80040481832543\n",
            "Accuracy on batch 2532 on Training is 77.80053296486379\n",
            "Accuracy on batch 2533 on Training is 77.7994277821626\n",
            "Accuracy on batch 2534 on Training is 77.79955621301775\n",
            "Accuracy on batch 2535 on Training is 77.80338130914826\n",
            "Accuracy on batch 2536 on Training is 77.80227631060308\n",
            "Accuracy on batch 2537 on Training is 77.80363475177305\n",
            "Accuracy on batch 2538 on Training is 77.80499212288302\n",
            "Accuracy on batch 2539 on Training is 77.80388779527559\n",
            "Batch Id 2540 is having training loss of 0.8008500933647156\n",
            "0.7604365944862366\n",
            "Accuracy on batch 2540 on Training is 77.80524399842582\n",
            "Accuracy on batch 2541 on Training is 77.80168174665617\n",
            "Accuracy on batch 2542 on Training is 77.80426661423516\n",
            "Accuracy on batch 2543 on Training is 77.80807783018868\n",
            "Accuracy on batch 2544 on Training is 77.80943025540275\n",
            "Accuracy on batch 2545 on Training is 77.80955420267085\n",
            "Accuracy on batch 2546 on Training is 77.81458578720063\n",
            "Accuracy on batch 2547 on Training is 77.80857535321822\n",
            "Accuracy on batch 2548 on Training is 77.80502157708905\n",
            "Accuracy on batch 2549 on Training is 77.80637254901961\n",
            "Accuracy on batch 2550 on Training is 77.80649745197961\n",
            "Accuracy on batch 2551 on Training is 77.80417319749216\n",
            "Accuracy on batch 2552 on Training is 77.80552291421857\n",
            "Accuracy on batch 2553 on Training is 77.80687157400156\n",
            "Accuracy on batch 2554 on Training is 77.8021037181996\n",
            "Accuracy on batch 2555 on Training is 77.80100743348983\n",
            "Accuracy on batch 2556 on Training is 77.80113414157215\n",
            "Accuracy on batch 2557 on Training is 77.80248240813135\n",
            "Accuracy on batch 2558 on Training is 77.8013872606487\n",
            "Accuracy on batch 2559 on Training is 77.803955078125\n",
            "Batch Id 2560 is having training loss of 0.8006520867347717\n",
            "0.9233493208885193\n",
            "Accuracy on batch 2560 on Training is 77.80286021085513\n",
            "Accuracy on batch 2561 on Training is 77.80542544886808\n",
            "Accuracy on batch 2562 on Training is 77.80555013655872\n",
            "Accuracy on batch 2563 on Training is 77.80445592823713\n",
            "Accuracy on batch 2564 on Training is 77.80458089668616\n",
            "Accuracy on batch 2565 on Training is 77.80957716289946\n",
            "Accuracy on batch 2566 on Training is 77.80970003895598\n",
            "Accuracy on batch 2567 on Training is 77.80982281931465\n",
            "Accuracy on batch 2568 on Training is 77.81237835733748\n",
            "Accuracy on batch 2569 on Training is 77.81493190661479\n",
            "Accuracy on batch 2570 on Training is 77.81383702839362\n",
            "Accuracy on batch 2571 on Training is 77.81395800933126\n",
            "Accuracy on batch 2572 on Training is 77.81650796735329\n",
            "Accuracy on batch 2573 on Training is 77.82027000777\n",
            "Accuracy on batch 2574 on Training is 77.81553398058253\n",
            "Accuracy on batch 2575 on Training is 77.81565411490683\n",
            "Accuracy on batch 2576 on Training is 77.81577415599534\n",
            "Accuracy on batch 2577 on Training is 77.81953064391001\n",
            "Accuracy on batch 2578 on Training is 77.8172256688639\n",
            "Accuracy on batch 2579 on Training is 77.81734496124031\n",
            "Batch Id 2580 is having training loss of 0.8001261353492737\n",
            "0.8942223191261292\n",
            "Accuracy on batch 2580 on Training is 77.81867493219683\n",
            "Accuracy on batch 2581 on Training is 77.81758326878389\n",
            "Accuracy on batch 2582 on Training is 77.81649245063879\n",
            "Accuracy on batch 2583 on Training is 77.81782120743034\n",
            "Accuracy on batch 2584 on Training is 77.81914893617021\n",
            "Accuracy on batch 2585 on Training is 77.82289249806651\n",
            "Accuracy on batch 2586 on Training is 77.82180131426362\n",
            "Accuracy on batch 2587 on Training is 77.82191846986089\n",
            "Accuracy on batch 2588 on Training is 77.82203553495557\n",
            "Accuracy on batch 2589 on Training is 77.81853281853282\n",
            "Accuracy on batch 2590 on Training is 77.8186510999614\n",
            "Accuracy on batch 2591 on Training is 77.81876929012346\n",
            "Accuracy on batch 2592 on Training is 77.82491322792133\n",
            "Accuracy on batch 2593 on Training is 77.82984772552044\n",
            "Accuracy on batch 2594 on Training is 77.83236994219654\n",
            "Accuracy on batch 2595 on Training is 77.83368644067797\n",
            "Accuracy on batch 2596 on Training is 77.83379861378513\n",
            "Accuracy on batch 2597 on Training is 77.83751924557352\n",
            "Accuracy on batch 2598 on Training is 77.83762985763755\n",
            "Accuracy on batch 2599 on Training is 77.84134615384616\n",
            "Batch Id 2600 is having training loss of 0.7992694973945618\n",
            "0.7240082025527954\n",
            "Accuracy on batch 2600 on Training is 77.84265667051135\n",
            "Accuracy on batch 2601 on Training is 77.8391621829362\n",
            "Accuracy on batch 2602 on Training is 77.83446984248944\n",
            "Accuracy on batch 2603 on Training is 77.83698156682027\n",
            "Accuracy on batch 2604 on Training is 77.83469289827255\n",
            "Accuracy on batch 2605 on Training is 77.83120683039141\n",
            "Accuracy on batch 2606 on Training is 77.83251822017645\n",
            "Accuracy on batch 2607 on Training is 77.83622507668711\n",
            "Accuracy on batch 2608 on Training is 77.83873131467995\n",
            "Accuracy on batch 2609 on Training is 77.83884099616859\n",
            "Accuracy on batch 2610 on Training is 77.83895059364228\n",
            "Accuracy on batch 2611 on Training is 77.83906010719755\n",
            "Accuracy on batch 2612 on Training is 77.83797359357061\n",
            "Accuracy on batch 2613 on Training is 77.84047436878348\n",
            "Accuracy on batch 2614 on Training is 77.84297323135755\n",
            "Accuracy on batch 2615 on Training is 77.8418864678899\n",
            "Accuracy on batch 2616 on Training is 77.8408005349637\n",
            "Accuracy on batch 2617 on Training is 77.83613445378151\n",
            "Accuracy on batch 2618 on Training is 77.83743795341734\n",
            "Accuracy on batch 2619 on Training is 77.83635496183206\n",
            "Batch Id 2620 is having training loss of 0.7994059324264526\n",
            "0.523961067199707\n",
            "Accuracy on batch 2620 on Training is 77.8388496756963\n",
            "Accuracy on batch 2621 on Training is 77.84134248665141\n",
            "Accuracy on batch 2622 on Training is 77.84502478078537\n",
            "Accuracy on batch 2623 on Training is 77.84632240853658\n",
            "Accuracy on batch 2624 on Training is 77.84404761904761\n",
            "Accuracy on batch 2625 on Training is 77.84772467631379\n",
            "Accuracy on batch 2626 on Training is 77.85020936429387\n",
            "Accuracy on batch 2627 on Training is 77.84912480974126\n",
            "Accuracy on batch 2628 on Training is 77.85398440471663\n",
            "Accuracy on batch 2629 on Training is 77.85765209125475\n",
            "Accuracy on batch 2630 on Training is 77.86131698973774\n",
            "Accuracy on batch 2631 on Training is 77.86260448328268\n",
            "Accuracy on batch 2632 on Training is 77.86389099886061\n",
            "Accuracy on batch 2633 on Training is 77.86754935459378\n",
            "Accuracy on batch 2634 on Training is 77.86764705882354\n",
            "Accuracy on batch 2635 on Training is 77.86418816388468\n",
            "Accuracy on batch 2636 on Training is 77.86665718619643\n",
            "Accuracy on batch 2637 on Training is 77.86438589840789\n",
            "Accuracy on batch 2638 on Training is 77.86330049261083\n",
            "Accuracy on batch 2639 on Training is 77.85984848484848\n",
            "Batch Id 2640 is having training loss of 0.7984515428543091\n",
            "0.7808975577354431\n",
            "Accuracy on batch 2640 on Training is 77.86113214691404\n",
            "Accuracy on batch 2641 on Training is 77.86123202119606\n",
            "Accuracy on batch 2642 on Training is 77.86251418842225\n",
            "Accuracy on batch 2643 on Training is 77.86024962178517\n",
            "Accuracy on batch 2644 on Training is 77.86153119092627\n",
            "Accuracy on batch 2645 on Training is 77.86399281934996\n",
            "Accuracy on batch 2646 on Training is 77.86291084246317\n",
            "Accuracy on batch 2647 on Training is 77.86891049848943\n",
            "Accuracy on batch 2648 on Training is 77.86546810117025\n",
            "Accuracy on batch 2649 on Training is 77.86320754716981\n",
            "Accuracy on batch 2650 on Training is 77.85976989815164\n",
            "Accuracy on batch 2651 on Training is 77.85751319758673\n",
            "Accuracy on batch 2652 on Training is 77.85643611006408\n",
            "Accuracy on batch 2653 on Training is 77.85418236623964\n",
            "Accuracy on batch 2654 on Training is 77.85428436911488\n",
            "Accuracy on batch 2655 on Training is 77.85909262048193\n",
            "Accuracy on batch 2656 on Training is 77.86389725254045\n",
            "Accuracy on batch 2657 on Training is 77.86399548532731\n",
            "Accuracy on batch 2658 on Training is 77.86291839037231\n",
            "Accuracy on batch 2659 on Training is 77.86536654135338\n",
            "Batch Id 2660 is having training loss of 0.7983255386352539\n",
            "1.167181372642517\n",
            "Accuracy on batch 2660 on Training is 77.8619409996242\n",
            "Accuracy on batch 2661 on Training is 77.86438767843727\n",
            "Accuracy on batch 2662 on Training is 77.8668325197146\n",
            "Accuracy on batch 2663 on Training is 77.86458333333333\n",
            "Accuracy on batch 2664 on Training is 77.86819887429644\n",
            "Accuracy on batch 2665 on Training is 77.86243435858965\n",
            "Accuracy on batch 2666 on Training is 77.86370453693289\n",
            "Accuracy on batch 2667 on Training is 77.86614505247377\n",
            "Accuracy on batch 2668 on Training is 77.86975458973399\n",
            "Accuracy on batch 2669 on Training is 77.8686797752809\n",
            "Accuracy on batch 2670 on Training is 77.87696555597155\n",
            "Accuracy on batch 2671 on Training is 77.87939745508982\n",
            "Accuracy on batch 2672 on Training is 77.8818275346053\n",
            "Accuracy on batch 2673 on Training is 77.88425579655946\n",
            "Accuracy on batch 2674 on Training is 77.8855140186916\n",
            "Accuracy on batch 2675 on Training is 77.88793908819133\n",
            "Accuracy on batch 2676 on Training is 77.8903623459096\n",
            "Accuracy on batch 2677 on Training is 77.88811613144138\n",
            "Accuracy on batch 2678 on Training is 77.89053751399776\n",
            "Accuracy on batch 2679 on Training is 77.89179104477611\n",
            "Batch Id 2680 is having training loss of 0.7975049018859863\n",
            "1.1067042350769043\n",
            "Accuracy on batch 2680 on Training is 77.89071242073852\n",
            "Accuracy on batch 2681 on Training is 77.88730425055928\n",
            "Accuracy on batch 2682 on Training is 77.89088706671636\n",
            "Accuracy on batch 2683 on Training is 77.88398845007451\n",
            "Accuracy on batch 2684 on Training is 77.8840782122905\n",
            "Accuracy on batch 2685 on Training is 77.88300446760982\n",
            "Accuracy on batch 2686 on Training is 77.88658355042799\n",
            "Accuracy on batch 2687 on Training is 77.88899739583333\n",
            "Accuracy on batch 2688 on Training is 77.88792301970993\n",
            "Accuracy on batch 2689 on Training is 77.88917286245353\n",
            "Accuracy on batch 2690 on Training is 77.88577666295058\n",
            "Accuracy on batch 2691 on Training is 77.88006129271916\n",
            "Accuracy on batch 2692 on Training is 77.88015224656517\n",
            "Accuracy on batch 2693 on Training is 77.87560319227914\n",
            "Accuracy on batch 2694 on Training is 77.8756957328386\n",
            "Accuracy on batch 2695 on Training is 77.8746290801187\n",
            "Accuracy on batch 2696 on Training is 77.8781979977753\n",
            "Accuracy on batch 2697 on Training is 77.87944773906598\n",
            "Accuracy on batch 2698 on Training is 77.88069655427937\n",
            "Accuracy on batch 2699 on Training is 77.87731481481481\n",
            "Batch Id 2700 is having training loss of 0.7975628972053528\n",
            "0.671148419380188\n",
            "Accuracy on batch 2700 on Training is 77.87740651610514\n",
            "Accuracy on batch 2701 on Training is 77.88096780162843\n",
            "Accuracy on batch 2702 on Training is 77.88337032926378\n",
            "Accuracy on batch 2703 on Training is 77.88345968934911\n",
            "Accuracy on batch 2704 on Training is 77.88470425138632\n",
            "Accuracy on batch 2705 on Training is 77.88710273466371\n",
            "Accuracy on batch 2706 on Training is 77.88603620243812\n",
            "Accuracy on batch 2707 on Training is 77.88266248153619\n",
            "Accuracy on batch 2708 on Training is 77.8827519379845\n",
            "Accuracy on batch 2709 on Training is 77.88399446494465\n",
            "Accuracy on batch 2710 on Training is 77.88638878642567\n",
            "Accuracy on batch 2711 on Training is 77.88993362831859\n",
            "Accuracy on batch 2712 on Training is 77.89232399557685\n",
            "Accuracy on batch 2713 on Training is 77.8970154753132\n",
            "Accuracy on batch 2714 on Training is 77.89249539594843\n",
            "Accuracy on batch 2715 on Training is 77.89488217967599\n",
            "Accuracy on batch 2716 on Training is 77.89611704085388\n",
            "Accuracy on batch 2717 on Training is 77.8950515084621\n",
            "Accuracy on batch 2718 on Training is 77.89743471864656\n",
            "Accuracy on batch 2719 on Training is 77.9009650735294\n",
            "Batch Id 2720 is having training loss of 0.7966071367263794\n",
            "0.47246018052101135\n",
            "Accuracy on batch 2720 on Training is 77.90449283351708\n",
            "Accuracy on batch 2721 on Training is 77.90686994856723\n",
            "Accuracy on batch 2722 on Training is 77.90350716121924\n",
            "Accuracy on batch 2723 on Training is 77.90817731277534\n",
            "Accuracy on batch 2724 on Training is 77.90825688073394\n",
            "Accuracy on batch 2725 on Training is 77.91292186353631\n",
            "Accuracy on batch 2726 on Training is 77.91414558122479\n",
            "Accuracy on batch 2727 on Training is 77.91193181818181\n",
            "Accuracy on batch 2728 on Training is 77.91430010993038\n",
            "Accuracy on batch 2729 on Training is 77.91781135531136\n",
            "Accuracy on batch 2730 on Training is 77.91903149029659\n",
            "Accuracy on batch 2731 on Training is 77.9179630307467\n",
            "Accuracy on batch 2732 on Training is 77.91918221734358\n",
            "Accuracy on batch 2733 on Training is 77.91811448427212\n",
            "Accuracy on batch 2734 on Training is 77.922760511883\n",
            "Accuracy on batch 2735 on Training is 77.92397660818713\n",
            "Accuracy on batch 2736 on Training is 77.92405005480452\n",
            "Accuracy on batch 2737 on Training is 77.92298210372535\n",
            "Accuracy on batch 2738 on Training is 77.9219149324571\n",
            "Accuracy on batch 2739 on Training is 77.9242700729927\n",
            "Batch Id 2740 is having training loss of 0.7962228655815125\n",
            "0.561400830745697\n",
            "Accuracy on batch 2740 on Training is 77.92434330536301\n",
            "Accuracy on batch 2741 on Training is 77.92555616338439\n",
            "Accuracy on batch 2742 on Training is 77.92904666423624\n",
            "Accuracy on batch 2743 on Training is 77.92570153061224\n",
            "Accuracy on batch 2744 on Training is 77.92577413479053\n",
            "Accuracy on batch 2745 on Training is 77.92698470502549\n",
            "Accuracy on batch 2746 on Training is 77.92705678922461\n",
            "Accuracy on batch 2747 on Training is 77.9271288209607\n",
            "Accuracy on batch 2748 on Training is 77.92947435431066\n",
            "Accuracy on batch 2749 on Training is 77.92727272727272\n",
            "Accuracy on batch 2750 on Training is 77.9228007997092\n",
            "Accuracy on batch 2751 on Training is 77.92400981104652\n",
            "Accuracy on batch 2752 on Training is 77.92635306937886\n",
            "Accuracy on batch 2753 on Training is 77.92529048656499\n",
            "Accuracy on batch 2754 on Training is 77.92536297640653\n",
            "Accuracy on batch 2755 on Training is 77.92770319303338\n",
            "Accuracy on batch 2756 on Training is 77.92777475516866\n",
            "Accuracy on batch 2757 on Training is 77.93237853517041\n",
            "Accuracy on batch 2758 on Training is 77.93131569409206\n",
            "Accuracy on batch 2759 on Training is 77.93365036231884\n",
            "Batch Id 2760 is having training loss of 0.7957872152328491\n",
            "0.43287739157676697\n",
            "Accuracy on batch 2760 on Training is 77.9359833393698\n",
            "Accuracy on batch 2761 on Training is 77.93605177407676\n",
            "Accuracy on batch 2762 on Training is 77.93951321027868\n",
            "Accuracy on batch 2763 on Training is 77.93958031837916\n",
            "Accuracy on batch 2764 on Training is 77.94190777576854\n",
            "Accuracy on batch 2765 on Training is 77.93858459869848\n",
            "Accuracy on batch 2766 on Training is 77.93639320563787\n",
            "Accuracy on batch 2767 on Training is 77.93871929190752\n",
            "Accuracy on batch 2768 on Training is 77.93878656554713\n",
            "Accuracy on batch 2769 on Training is 77.93772563176896\n",
            "Accuracy on batch 2770 on Training is 77.93892096715987\n",
            "Accuracy on batch 2771 on Training is 77.93673340548341\n",
            "Accuracy on batch 2772 on Training is 77.93792823656689\n",
            "Accuracy on batch 2773 on Training is 77.9379956741168\n",
            "Accuracy on batch 2774 on Training is 77.93806306306307\n",
            "Accuracy on batch 2775 on Training is 77.9403818443804\n",
            "Accuracy on batch 2776 on Training is 77.94157364061937\n",
            "Accuracy on batch 2777 on Training is 77.9438894888409\n",
            "Accuracy on batch 2778 on Training is 77.94283015473192\n",
            "Accuracy on batch 2779 on Training is 77.94514388489209\n",
            "Batch Id 2780 is having training loss of 0.79526686668396\n",
            "0.6941332817077637\n",
            "Accuracy on batch 2780 on Training is 77.94633225458468\n",
            "Accuracy on batch 2781 on Training is 77.9441498921639\n",
            "Accuracy on batch 2782 on Training is 77.9453377650018\n",
            "Accuracy on batch 2783 on Training is 77.93866738505747\n",
            "Accuracy on batch 2784 on Training is 77.93649012567325\n",
            "Accuracy on batch 2785 on Training is 77.93767946877243\n",
            "Accuracy on batch 2786 on Training is 77.93998923573736\n",
            "Accuracy on batch 2787 on Training is 77.94117647058823\n",
            "Accuracy on batch 2788 on Training is 77.94012190749372\n",
            "Accuracy on batch 2789 on Training is 77.94018817204301\n",
            "Accuracy on batch 2790 on Training is 77.94473307058402\n",
            "Accuracy on batch 2791 on Training is 77.94479763610315\n",
            "Accuracy on batch 2792 on Training is 77.94598102398854\n",
            "Accuracy on batch 2793 on Training is 77.94604509663564\n",
            "Accuracy on batch 2794 on Training is 77.94610912343471\n",
            "Accuracy on batch 2795 on Training is 77.94393776824035\n",
            "Accuracy on batch 2796 on Training is 77.94400250268144\n",
            "Accuracy on batch 2797 on Training is 77.9440671908506\n",
            "Accuracy on batch 2798 on Training is 77.94413183279742\n",
            "Accuracy on batch 2799 on Training is 77.94642857142857\n",
            "Batch Id 2800 is having training loss of 0.7953430414199829\n",
            "0.8576977849006653\n",
            "Accuracy on batch 2800 on Training is 77.945376651196\n",
            "Accuracy on batch 2801 on Training is 77.93763383297645\n",
            "Accuracy on batch 2802 on Training is 77.93770067784517\n",
            "Accuracy on batch 2803 on Training is 77.93888195435093\n",
            "Accuracy on batch 2804 on Training is 77.94229055258467\n",
            "Accuracy on batch 2805 on Training is 77.94346935138988\n",
            "Accuracy on batch 2806 on Training is 77.94353402208763\n",
            "Accuracy on batch 2807 on Training is 77.94137286324786\n",
            "Accuracy on batch 2808 on Training is 77.94811320754717\n",
            "Accuracy on batch 2809 on Training is 77.94928825622776\n",
            "Accuracy on batch 2810 on Training is 77.94490394877268\n",
            "Accuracy on batch 2811 on Training is 77.9449679943101\n",
            "Accuracy on batch 2812 on Training is 77.94614290792748\n",
            "Accuracy on batch 2813 on Training is 77.94509594882729\n",
            "Accuracy on batch 2814 on Training is 77.94849023090586\n",
            "Accuracy on batch 2815 on Training is 77.95299183238636\n",
            "Accuracy on batch 2816 on Training is 77.94750621228258\n",
            "Accuracy on batch 2817 on Training is 77.94978708303762\n",
            "Accuracy on batch 2818 on Training is 77.95317488471089\n",
            "Accuracy on batch 2819 on Training is 77.95101950354609\n",
            "Batch Id 2820 is having training loss of 0.7948619723320007\n",
            "0.6754376292228699\n",
            "Accuracy on batch 2820 on Training is 77.95218894009217\n",
            "Accuracy on batch 2821 on Training is 77.9500354358611\n",
            "Accuracy on batch 2822 on Training is 77.94899043570669\n",
            "Accuracy on batch 2823 on Training is 77.95237252124646\n",
            "Accuracy on batch 2824 on Training is 77.95022123893806\n",
            "Accuracy on batch 2825 on Training is 77.94917728237792\n",
            "Accuracy on batch 2826 on Training is 77.94923947647683\n",
            "Accuracy on batch 2827 on Training is 77.95372171145686\n",
            "Accuracy on batch 2828 on Training is 77.95267762460233\n",
            "Accuracy on batch 2829 on Training is 77.95384275618375\n",
            "Accuracy on batch 2830 on Training is 77.95500706464146\n",
            "Accuracy on batch 2831 on Training is 77.95506709039547\n",
            "Accuracy on batch 2832 on Training is 77.95733321567243\n",
            "Accuracy on batch 2833 on Training is 77.95298165137615\n",
            "Accuracy on batch 2834 on Training is 77.9563492063492\n",
            "Accuracy on batch 2835 on Training is 77.95640867418899\n",
            "Accuracy on batch 2836 on Training is 77.953163553049\n",
            "Accuracy on batch 2837 on Training is 77.95542635658914\n",
            "Accuracy on batch 2838 on Training is 77.95438534695315\n",
            "Accuracy on batch 2839 on Training is 77.95334507042253\n",
            "Batch Id 2840 is having training loss of 0.7951110005378723\n",
            "0.9208840131759644\n",
            "Accuracy on batch 2840 on Training is 77.95230552622316\n",
            "Accuracy on batch 2841 on Training is 77.9490675580577\n",
            "Accuracy on batch 2842 on Training is 77.94912944073162\n",
            "Accuracy on batch 2843 on Training is 77.94809247538677\n",
            "Accuracy on batch 2844 on Training is 77.94705623901582\n",
            "Accuracy on batch 2845 on Training is 77.94492269852424\n",
            "Accuracy on batch 2846 on Training is 77.94827889005971\n",
            "Accuracy on batch 2847 on Training is 77.94724367977528\n",
            "Accuracy on batch 2848 on Training is 77.94840294840294\n",
            "Accuracy on batch 2849 on Training is 77.94956140350877\n",
            "Accuracy on batch 2850 on Training is 77.94852683269029\n",
            "Accuracy on batch 2851 on Training is 77.94749298737727\n",
            "Accuracy on batch 2852 on Training is 77.94755520504732\n",
            "Accuracy on batch 2853 on Training is 77.94980728801681\n",
            "Accuracy on batch 2854 on Training is 77.94658493870404\n",
            "Accuracy on batch 2855 on Training is 77.94883578431373\n",
            "Accuracy on batch 2856 on Training is 77.94889744487224\n",
            "Accuracy on batch 2857 on Training is 77.95114590622813\n",
            "Accuracy on batch 2858 on Training is 77.95011367611053\n",
            "Accuracy on batch 2859 on Training is 77.95126748251748\n",
            "Batch Id 2860 is having training loss of 0.7952063083648682\n",
            "0.7567950487136841\n",
            "Accuracy on batch 2860 on Training is 77.95023593149249\n",
            "Accuracy on batch 2861 on Training is 77.95138888888889\n",
            "Accuracy on batch 2862 on Training is 77.95035801606706\n",
            "Accuracy on batch 2863 on Training is 77.95260125698324\n",
            "Accuracy on batch 2864 on Training is 77.95157068062828\n",
            "Accuracy on batch 2865 on Training is 77.95490230286113\n",
            "Accuracy on batch 2866 on Training is 77.95714161144053\n",
            "Accuracy on batch 2867 on Training is 77.96155857740585\n",
            "Accuracy on batch 2868 on Training is 77.96161554548623\n",
            "Accuracy on batch 2869 on Training is 77.9616724738676\n",
            "Accuracy on batch 2870 on Training is 77.96281783350749\n",
            "Accuracy on batch 2871 on Training is 77.96287430362116\n",
            "Accuracy on batch 2872 on Training is 77.96293073442395\n",
            "Accuracy on batch 2873 on Training is 77.96189979123173\n",
            "Accuracy on batch 2874 on Training is 77.96304347826087\n",
            "Accuracy on batch 2875 on Training is 77.96853268428373\n",
            "Accuracy on batch 2876 on Training is 77.96858706986444\n",
            "Accuracy on batch 2877 on Training is 77.97189888811675\n",
            "Accuracy on batch 2878 on Training is 77.97303751302536\n",
            "Accuracy on batch 2879 on Training is 77.96983506944444\n",
            "Batch Id 2880 is having training loss of 0.7944316864013672\n",
            "0.7374212741851807\n",
            "Accuracy on batch 2880 on Training is 77.96771954182576\n",
            "Accuracy on batch 2881 on Training is 77.96885843164469\n",
            "Accuracy on batch 2882 on Training is 77.96674471037115\n",
            "Accuracy on batch 2883 on Training is 77.967883148405\n",
            "Accuracy on batch 2884 on Training is 77.96902079722703\n",
            "Accuracy on batch 2885 on Training is 77.96690921690922\n",
            "Accuracy on batch 2886 on Training is 77.96263422237617\n",
            "Accuracy on batch 2887 on Training is 77.96701869806094\n",
            "Accuracy on batch 2888 on Training is 77.96707338179301\n",
            "Accuracy on batch 2889 on Training is 77.96820934256056\n",
            "Accuracy on batch 2890 on Training is 77.9758301625735\n",
            "Accuracy on batch 2891 on Training is 77.9758817427386\n",
            "Accuracy on batch 2892 on Training is 77.97593328724507\n",
            "Accuracy on batch 2893 on Training is 77.97166551485833\n",
            "Accuracy on batch 2894 on Training is 77.9706390328152\n",
            "Accuracy on batch 2895 on Training is 77.96853418508287\n",
            "Accuracy on batch 2896 on Training is 77.97290300310667\n",
            "Accuracy on batch 2897 on Training is 77.97403381642512\n",
            "Accuracy on batch 2898 on Training is 77.97300793377026\n",
            "Accuracy on batch 2899 on Training is 77.97198275862068\n",
            "Batch Id 2900 is having training loss of 0.7939609289169312\n",
            "0.7072898745536804\n",
            "Accuracy on batch 2900 on Training is 77.97095829024474\n",
            "Accuracy on batch 2901 on Training is 77.97316505858029\n",
            "Accuracy on batch 2902 on Training is 77.9753703065794\n",
            "Accuracy on batch 2903 on Training is 77.97542183195593\n",
            "Accuracy on batch 2904 on Training is 77.97439759036145\n",
            "Accuracy on batch 2905 on Training is 77.97444941500343\n",
            "Accuracy on batch 2906 on Training is 77.96805125558996\n",
            "Accuracy on batch 2907 on Training is 77.9681052269601\n",
            "Accuracy on batch 2908 on Training is 77.97030766586455\n",
            "Accuracy on batch 2909 on Training is 77.96713917525773\n",
            "Accuracy on batch 2910 on Training is 77.97041394709721\n",
            "Accuracy on batch 2911 on Training is 77.97154017857143\n",
            "Accuracy on batch 2912 on Training is 77.97695674562307\n",
            "Accuracy on batch 2913 on Training is 77.97486273164036\n",
            "Accuracy on batch 2914 on Training is 77.97813036020584\n",
            "Accuracy on batch 2915 on Training is 77.98032407407408\n",
            "Accuracy on batch 2916 on Training is 77.98144497771683\n",
            "Accuracy on batch 2917 on Training is 77.98042323509253\n",
            "Accuracy on batch 2918 on Training is 77.98368448098664\n",
            "Accuracy on batch 2919 on Training is 77.98052226027397\n",
            "Batch Id 2920 is having training loss of 0.7937606573104858\n",
            "0.6443530321121216\n",
            "Accuracy on batch 2920 on Training is 77.98057172201301\n",
            "Accuracy on batch 2921 on Training is 77.97741273100615\n",
            "Accuracy on batch 2922 on Training is 77.97639411563462\n",
            "Accuracy on batch 2923 on Training is 77.9796511627907\n",
            "Accuracy on batch 2924 on Training is 77.97863247863248\n",
            "Accuracy on batch 2925 on Training is 77.97868250170882\n",
            "Accuracy on batch 2926 on Training is 77.97766484455073\n",
            "Accuracy on batch 2927 on Training is 77.97451331967213\n",
            "Accuracy on batch 2928 on Training is 77.97563161488563\n",
            "Accuracy on batch 2929 on Training is 77.97674914675768\n",
            "Accuracy on batch 2930 on Training is 77.9767997270556\n",
            "Accuracy on batch 2931 on Training is 77.9800477489768\n",
            "Accuracy on batch 2932 on Training is 77.98116263211729\n",
            "Accuracy on batch 2933 on Training is 77.98121165644172\n",
            "Accuracy on batch 2934 on Training is 77.97700170357751\n",
            "Accuracy on batch 2935 on Training is 77.97705211171662\n",
            "Accuracy on batch 2936 on Training is 77.97816649642492\n",
            "Accuracy on batch 2937 on Training is 77.98247106875425\n",
            "Accuracy on batch 2938 on Training is 77.98358285130998\n",
            "Accuracy on batch 2939 on Training is 77.98469387755102\n",
            "Batch Id 2940 is having training loss of 0.7935103178024292\n",
            "0.7928638458251953\n",
            "Accuracy on batch 2940 on Training is 77.98367902074125\n",
            "Accuracy on batch 2941 on Training is 77.98054044867438\n",
            "Accuracy on batch 2942 on Training is 77.97846585117227\n",
            "Accuracy on batch 2943 on Training is 77.98063858695652\n",
            "Accuracy on batch 2944 on Training is 77.97962648556876\n",
            "Accuracy on batch 2945 on Training is 77.97967583163611\n",
            "Accuracy on batch 2946 on Training is 77.97866474380726\n",
            "Accuracy on batch 2947 on Training is 77.97659430122117\n",
            "Accuracy on batch 2948 on Training is 77.97876398779248\n",
            "Accuracy on batch 2949 on Training is 77.97669491525424\n",
            "Accuracy on batch 2950 on Training is 77.97356828193833\n",
            "Accuracy on batch 2951 on Training is 77.97679539295393\n",
            "Accuracy on batch 2952 on Training is 77.977903826617\n",
            "Accuracy on batch 2953 on Training is 77.97583784698713\n",
            "Accuracy on batch 2954 on Training is 77.98011844331641\n",
            "Accuracy on batch 2955 on Training is 77.97911028416779\n",
            "Accuracy on batch 2956 on Training is 77.97704599256002\n",
            "Accuracy on batch 2957 on Training is 77.97920892494929\n",
            "Accuracy on batch 2958 on Training is 77.97714599526867\n",
            "Accuracy on batch 2959 on Training is 77.9782516891892\n",
            "Batch Id 2960 is having training loss of 0.7930992245674133\n",
            "0.795414924621582\n",
            "Accuracy on batch 2960 on Training is 77.97830124957784\n",
            "Accuracy on batch 2961 on Training is 77.981515867657\n",
            "Accuracy on batch 2962 on Training is 77.98472831589605\n",
            "Accuracy on batch 2963 on Training is 77.979504048583\n",
            "Accuracy on batch 2964 on Training is 77.9816610455312\n",
            "Accuracy on batch 2965 on Training is 77.98170937289278\n",
            "Accuracy on batch 2966 on Training is 77.98175766767778\n",
            "Accuracy on batch 2967 on Training is 77.98496462264151\n",
            "Accuracy on batch 2968 on Training is 77.98816941731222\n",
            "Accuracy on batch 2969 on Training is 77.99242424242425\n",
            "Accuracy on batch 2970 on Training is 77.99562436889936\n",
            "Accuracy on batch 2971 on Training is 77.9967193808883\n",
            "Accuracy on batch 2972 on Training is 77.99781365623949\n",
            "Accuracy on batch 2973 on Training is 77.99575487558843\n",
            "Accuracy on batch 2974 on Training is 77.99789915966386\n",
            "Accuracy on batch 2975 on Training is 78.00214213709677\n",
            "Accuracy on batch 2976 on Training is 78.00533254954652\n",
            "Accuracy on batch 2977 on Training is 78.00852081934184\n",
            "Accuracy on batch 2978 on Training is 78.00960892917087\n",
            "Accuracy on batch 2979 on Training is 78.01069630872483\n",
            "Batch Id 2980 is having training loss of 0.7920628190040588\n",
            "0.3379562199115753\n",
            "Accuracy on batch 2980 on Training is 78.0149278765515\n",
            "Accuracy on batch 2981 on Training is 78.01286887994634\n",
            "Accuracy on batch 2982 on Training is 78.0118588669125\n",
            "Accuracy on batch 2983 on Training is 78.01399128686327\n",
            "Accuracy on batch 2984 on Training is 78.01281782767143\n",
            "Batch Id 0 is having validation loss of 0.7704421281814575\n",
            "0.7704421281814575\n",
            "Batch Id 0 is having validation accuracy of 87.5\n",
            "Batch Id 1 is having validation loss of 0.7704697251319885\n",
            "0.7704973220825195\n",
            "Batch Id 1 is having validation accuracy of 82.8125\n",
            "Batch Id 2 is having validation loss of 0.7465439438819885\n",
            "0.6986923217773438\n",
            "Batch Id 2 is having validation accuracy of 81.25\n",
            "Batch Id 3 is having validation loss of 0.6302053928375244\n",
            "0.28118976950645447\n",
            "Batch Id 3 is having validation accuracy of 84.375\n",
            "Batch Id 4 is having validation loss of 0.7341843843460083\n",
            "1.1501002311706543\n",
            "Batch Id 4 is having validation accuracy of 80.625\n",
            "Batch Id 5 is having validation loss of 0.7508213520050049\n",
            "0.8340061902999878\n",
            "Batch Id 5 is having validation accuracy of 81.25\n",
            "Batch Id 6 is having validation loss of 0.8147289752960205\n",
            "1.1981745958328247\n",
            "Batch Id 6 is having validation accuracy of 79.91071428571429\n",
            "Batch Id 7 is having validation loss of 0.7425478100776672\n",
            "0.23727980256080627\n",
            "Batch Id 7 is having validation accuracy of 80.859375\n",
            "Batch Id 8 is having validation loss of 0.7019915580749512\n",
            "0.37754160165786743\n",
            "Batch Id 8 is having validation accuracy of 81.59722222222223\n",
            "Batch Id 9 is having validation loss of 0.667799711227417\n",
            "0.36007317900657654\n",
            "Batch Id 9 is having validation accuracy of 82.8125\n",
            "Batch Id 10 is having validation loss of 0.6675101518630981\n",
            "0.6646145582199097\n",
            "Batch Id 10 is having validation accuracy of 82.38636363636364\n",
            "Batch Id 11 is having validation loss of 0.6894431114196777\n",
            "0.9307054281234741\n",
            "Batch Id 11 is having validation accuracy of 81.51041666666667\n",
            "Batch Id 12 is having validation loss of 0.7365222573280334\n",
            "1.3014717102050781\n",
            "Batch Id 12 is having validation accuracy of 80.28846153846153\n",
            "Batch Id 13 is having validation loss of 0.7279009819030762\n",
            "0.6158243417739868\n",
            "Batch Id 13 is having validation accuracy of 79.46428571428571\n",
            "Batch Id 14 is having validation loss of 0.7253831624984741\n",
            "0.6901338696479797\n",
            "Batch Id 14 is having validation accuracy of 79.79166666666667\n",
            "Batch Id 15 is having validation loss of 0.7010498642921448\n",
            "0.3360503613948822\n",
            "Batch Id 15 is having validation accuracy of 80.46875\n",
            "Batch Id 16 is having validation loss of 0.7356104254722595\n",
            "1.2885797023773193\n",
            "Batch Id 16 is having validation accuracy of 80.1470588235294\n",
            "Batch Id 17 is having validation loss of 0.7453077435493469\n",
            "0.9101617932319641\n",
            "Batch Id 17 is having validation accuracy of 80.38194444444444\n",
            "Batch Id 18 is having validation loss of 0.7303836345672607\n",
            "0.461749792098999\n",
            "Batch Id 18 is having validation accuracy of 81.08552631578948\n",
            "Batch Id 19 is having validation loss of 0.726738452911377\n",
            "0.6574795246124268\n",
            "Batch Id 19 is having validation accuracy of 81.09375\n",
            "Batch Id 20 is having validation loss of 0.7230770587921143\n",
            "0.6498494148254395\n",
            "Batch Id 20 is having validation accuracy of 81.10119047619048\n",
            "Batch Id 21 is having validation loss of 0.7072881460189819\n",
            "0.3757212162017822\n",
            "Batch Id 21 is having validation accuracy of 81.39204545454545\n",
            "Batch Id 22 is having validation loss of 0.7017590999603271\n",
            "0.5801200270652771\n",
            "Batch Id 22 is having validation accuracy of 81.38586956521739\n",
            "Batch Id 23 is having validation loss of 0.7006679177284241\n",
            "0.6755711436271667\n",
            "Batch Id 23 is having validation accuracy of 81.640625\n",
            "Batch Id 24 is having validation loss of 0.692741334438324\n",
            "0.5025031566619873\n",
            "Batch Id 24 is having validation accuracy of 81.625\n",
            "Batch Id 25 is having validation loss of 0.7095707058906555\n",
            "1.1303046941757202\n",
            "Batch Id 25 is having validation accuracy of 81.3701923076923\n",
            "Batch Id 26 is having validation loss of 0.7108625769615173\n",
            "0.7444506287574768\n",
            "Batch Id 26 is having validation accuracy of 81.48148148148148\n",
            "Batch Id 27 is having validation loss of 0.718502938747406\n",
            "0.9247924089431763\n",
            "Batch Id 27 is having validation accuracy of 81.02678571428571\n",
            "Batch Id 28 is having validation loss of 0.7249734997749329\n",
            "0.9061490893363953\n",
            "Batch Id 28 is having validation accuracy of 80.81896551724138\n",
            "Batch Id 29 is having validation loss of 0.7149285078048706\n",
            "0.4236232340335846\n",
            "Batch Id 29 is having validation accuracy of 81.04166666666667\n",
            "Batch Id 30 is having validation loss of 0.7103528380393982\n",
            "0.5730820894241333\n",
            "Batch Id 30 is having validation accuracy of 81.1491935483871\n",
            "Batch Id 31 is having validation loss of 0.7129663825035095\n",
            "0.7939857840538025\n",
            "Batch Id 31 is having validation accuracy of 81.0546875\n",
            "Batch Id 32 is having validation loss of 0.7099841237068176\n",
            "0.6145511269569397\n",
            "Batch Id 32 is having validation accuracy of 81.06060606060606\n",
            "Batch Id 33 is having validation loss of 0.7039839029312134\n",
            "0.5059758424758911\n",
            "Batch Id 33 is having validation accuracy of 80.88235294117646\n",
            "Batch Id 34 is having validation loss of 0.7123313546180725\n",
            "0.9961451888084412\n",
            "Batch Id 34 is having validation accuracy of 80.71428571428571\n",
            "Batch Id 35 is having validation loss of 0.7283362746238708\n",
            "1.288508415222168\n",
            "Batch Id 35 is having validation accuracy of 80.55555555555556\n",
            "Batch Id 36 is having validation loss of 0.7282211780548096\n",
            "0.724077045917511\n",
            "Batch Id 36 is having validation accuracy of 80.82770270270271\n",
            "Batch Id 37 is having validation loss of 0.7215405106544495\n",
            "0.4743557274341583\n",
            "Batch Id 37 is having validation accuracy of 81.0032894736842\n",
            "Batch Id 38 is having validation loss of 0.7284231781959534\n",
            "0.9899646639823914\n",
            "Batch Id 38 is having validation accuracy of 80.92948717948718\n",
            "Batch Id 39 is having validation loss of 0.723795473575592\n",
            "0.543316125869751\n",
            "Batch Id 39 is having validation accuracy of 81.09375\n",
            "Batch Id 40 is having validation loss of 0.7191532254219055\n",
            "0.5334644913673401\n",
            "Batch Id 40 is having validation accuracy of 81.17378048780488\n",
            "Batch Id 41 is having validation loss of 0.7206990122795105\n",
            "0.7840771079063416\n",
            "Batch Id 41 is having validation accuracy of 81.17559523809524\n",
            "Batch Id 42 is having validation loss of 0.7190085053443909\n",
            "0.6480063796043396\n",
            "Batch Id 42 is having validation accuracy of 81.17732558139535\n",
            "Batch Id 43 is having validation loss of 0.7195385694503784\n",
            "0.7423309087753296\n",
            "Batch Id 43 is having validation accuracy of 81.10795454545455\n",
            "Batch Id 44 is having validation loss of 0.7289403676986694\n",
            "1.142620325088501\n",
            "Batch Id 44 is having validation accuracy of 80.625\n",
            "Batch Id 45 is having validation loss of 0.7269734740257263\n",
            "0.6384621858596802\n",
            "Batch Id 45 is having validation accuracy of 80.50271739130434\n",
            "Batch Id 46 is having validation loss of 0.7200778722763062\n",
            "0.4028794467449188\n",
            "Batch Id 46 is having validation accuracy of 80.71808510638297\n",
            "Batch Id 47 is having validation loss of 0.7137526273727417\n",
            "0.4164655804634094\n",
            "Batch Id 47 is having validation accuracy of 80.92447916666667\n",
            "Batch Id 48 is having validation loss of 0.7116292715072632\n",
            "0.6097095608711243\n",
            "Batch Id 48 is having validation accuracy of 80.9311224489796\n",
            "Batch Id 49 is having validation loss of 0.7117091417312622\n",
            "0.7156237959861755\n",
            "Batch Id 49 is having validation accuracy of 80.875\n",
            "Batch Id 50 is having validation loss of 0.7107546925544739\n",
            "0.6630322933197021\n",
            "Batch Id 50 is having validation accuracy of 80.82107843137256\n",
            "Batch Id 51 is having validation loss of 0.7045329809188843\n",
            "0.3872252106666565\n",
            "Batch Id 51 is having validation accuracy of 81.06971153846153\n",
            "Batch Id 52 is having validation loss of 0.7124411463737488\n",
            "1.1236658096313477\n",
            "Batch Id 52 is having validation accuracy of 81.13207547169812\n",
            "Batch Id 53 is having validation loss of 0.7153052687644958\n",
            "0.867102861404419\n",
            "Batch Id 53 is having validation accuracy of 81.01851851851852\n",
            "Batch Id 54 is having validation loss of 0.7121118903160095\n",
            "0.5396702289581299\n",
            "Batch Id 54 is having validation accuracy of 81.02272727272727\n",
            "Batch Id 55 is having validation loss of 0.7065271735191345\n",
            "0.39936742186546326\n",
            "Batch Id 55 is having validation accuracy of 81.25\n",
            "Batch Id 56 is having validation loss of 0.7159335613250732\n",
            "1.2426918745040894\n",
            "Batch Id 56 is having validation accuracy of 81.08552631578948\n",
            "Batch Id 57 is having validation loss of 0.7123046517372131\n",
            "0.5054556131362915\n",
            "Batch Id 57 is having validation accuracy of 81.14224137931035\n",
            "Batch Id 58 is having validation loss of 0.7143939733505249\n",
            "0.8355738520622253\n",
            "Batch Id 58 is having validation accuracy of 81.19703389830508\n",
            "Batch Id 59 is having validation loss of 0.7101058959960938\n",
            "0.4571080207824707\n",
            "Batch Id 59 is having validation accuracy of 81.30208333333333\n",
            "Batch Id 60 is having validation loss of 0.7047020196914673\n",
            "0.3804680109024048\n",
            "Batch Id 60 is having validation accuracy of 81.45491803278688\n",
            "Batch Id 61 is having validation loss of 0.7051414847373962\n",
            "0.7319498658180237\n",
            "Batch Id 61 is having validation accuracy of 81.3508064516129\n",
            "Batch Id 62 is having validation loss of 0.7009212374687195\n",
            "0.4392662048339844\n",
            "Batch Id 62 is having validation accuracy of 81.39880952380952\n",
            "Batch Id 63 is having validation loss of 0.7055280804634094\n",
            "0.9957583546638489\n",
            "Batch Id 63 is having validation accuracy of 81.103515625\n",
            "Batch Id 64 is having validation loss of 0.7087811827659607\n",
            "0.91698157787323\n",
            "Batch Id 64 is having validation accuracy of 81.0576923076923\n",
            "Batch Id 65 is having validation loss of 0.7040918469429016\n",
            "0.3992849588394165\n",
            "Batch Id 65 is having validation accuracy of 81.10795454545455\n",
            "Batch Id 66 is having validation loss of 0.7001488208770752\n",
            "0.4399089217185974\n",
            "Batch Id 66 is having validation accuracy of 81.15671641791045\n",
            "Batch Id 67 is having validation loss of 0.6995670199394226\n",
            "0.6605863571166992\n",
            "Batch Id 67 is having validation accuracy of 81.11213235294117\n",
            "Batch Id 68 is having validation loss of 0.6954373717308044\n",
            "0.414620578289032\n",
            "Batch Id 68 is having validation accuracy of 81.20471014492753\n",
            "Batch Id 69 is having validation loss of 0.6958171725273132\n",
            "0.7220229506492615\n",
            "Batch Id 69 is having validation accuracy of 81.16071428571429\n",
            "Batch Id 70 is having validation loss of 0.6998216509819031\n",
            "0.9801362156867981\n",
            "Batch Id 70 is having validation accuracy of 81.11795774647888\n",
            "Batch Id 71 is having validation loss of 0.6926063299179077\n",
            "0.1803174763917923\n",
            "Batch Id 71 is having validation accuracy of 81.33680555555556\n",
            "Batch Id 72 is having validation loss of 0.6881192326545715\n",
            "0.36504799127578735\n",
            "Batch Id 72 is having validation accuracy of 81.42123287671232\n",
            "Batch Id 73 is having validation loss of 0.6871106028556824\n",
            "0.6134823560714722\n",
            "Batch Id 73 is having validation accuracy of 81.41891891891892\n",
            "Batch Id 74 is having validation loss of 0.6912155151367188\n",
            "0.9949795603752136\n",
            "Batch Id 74 is having validation accuracy of 81.25\n",
            "Batch Id 75 is having validation loss of 0.6921245455741882\n",
            "0.7603037357330322\n",
            "Batch Id 75 is having validation accuracy of 81.12664473684211\n",
            "Batch Id 76 is having validation loss of 0.6923790574073792\n",
            "0.711721658706665\n",
            "Batch Id 76 is having validation accuracy of 81.12824675324676\n",
            "Batch Id 77 is having validation loss of 0.6880571842193604\n",
            "0.3552709221839905\n",
            "Batch Id 77 is having validation accuracy of 81.1698717948718\n",
            "Batch Id 78 is having validation loss of 0.6847085356712341\n",
            "0.4235122799873352\n",
            "Batch Id 78 is having validation accuracy of 81.32911392405063\n",
            "Batch Id 79 is having validation loss of 0.6798492670059204\n",
            "0.2959667444229126\n",
            "Batch Id 79 is having validation accuracy of 81.4453125\n",
            "Batch Id 80 is having validation loss of 0.6854909062385559\n",
            "1.1368207931518555\n",
            "Batch Id 80 is having validation accuracy of 81.21141975308642\n",
            "Batch Id 81 is having validation loss of 0.6882784366607666\n",
            "0.9140688180923462\n",
            "Batch Id 81 is having validation accuracy of 81.09756097560975\n",
            "Batch Id 82 is having validation loss of 0.6920837759971619\n",
            "1.0041232109069824\n",
            "Batch Id 82 is having validation accuracy of 81.02409638554217\n",
            "Batch Id 83 is having validation loss of 0.6914997100830078\n",
            "0.6430214047431946\n",
            "Batch Id 83 is having validation accuracy of 80.98958333333333\n",
            "Batch Id 84 is having validation loss of 0.6905243396759033\n",
            "0.608592689037323\n",
            "Batch Id 84 is having validation accuracy of 81.06617647058823\n",
            "Batch Id 85 is having validation loss of 0.6919860243797302\n",
            "0.816227912902832\n",
            "Batch Id 85 is having validation accuracy of 81.03197674418605\n",
            "Batch Id 86 is having validation loss of 0.6933116912841797\n",
            "0.8073197603225708\n",
            "Batch Id 86 is having validation accuracy of 81.03448275862068\n",
            "Batch Id 87 is having validation loss of 0.6896135210990906\n",
            "0.3678714632987976\n",
            "Batch Id 87 is having validation accuracy of 81.10795454545455\n",
            "Batch Id 88 is having validation loss of 0.6893451809883118\n",
            "0.6657307147979736\n",
            "Batch Id 88 is having validation accuracy of 81.14466292134831\n",
            "Batch Id 89 is having validation loss of 0.6873930096626282\n",
            "0.5136486887931824\n",
            "Batch Id 89 is having validation accuracy of 81.14583333333333\n",
            "Batch Id 90 is having validation loss of 0.6852254867553711\n",
            "0.49014943838119507\n",
            "Batch Id 90 is having validation accuracy of 81.18131868131869\n",
            "Batch Id 91 is having validation loss of 0.6820387840270996\n",
            "0.3920508027076721\n",
            "Batch Id 91 is having validation accuracy of 81.3179347826087\n",
            "Batch Id 92 is having validation loss of 0.6800241470336914\n",
            "0.4946771264076233\n",
            "Batch Id 92 is having validation accuracy of 81.31720430107526\n",
            "Batch Id 93 is having validation loss of 0.6814692616462708\n",
            "0.8158636093139648\n",
            "Batch Id 93 is having validation accuracy of 81.18351063829788\n",
            "Batch Id 94 is having validation loss of 0.6792658567428589\n",
            "0.4721446633338928\n",
            "Batch Id 94 is having validation accuracy of 81.21710526315789\n",
            "Batch Id 95 is having validation loss of 0.6846292614936829\n",
            "1.1941543817520142\n",
            "Batch Id 95 is having validation accuracy of 81.11979166666667\n",
            "Batch Id 96 is having validation loss of 0.6854443550109863\n",
            "0.7636960744857788\n",
            "Batch Id 96 is having validation accuracy of 81.02448453608247\n",
            "Batch Id 97 is having validation loss of 0.685558557510376\n",
            "0.69663405418396\n",
            "Batch Id 97 is having validation accuracy of 81.02678571428571\n",
            "Batch Id 98 is having validation loss of 0.6858873963356018\n",
            "0.7181165218353271\n",
            "Batch Id 98 is having validation accuracy of 80.93434343434343\n",
            "Batch Id 99 is having validation loss of 0.686059296131134\n",
            "0.7030773758888245\n",
            "Batch Id 99 is having validation accuracy of 80.90625\n",
            "Batch Id 100 is having validation loss of 0.6837771534919739\n",
            "0.4555649161338806\n",
            "Batch Id 100 is having validation accuracy of 81.03341584158416\n",
            "Batch Id 101 is having validation loss of 0.6825314164161682\n",
            "0.5567130446434021\n",
            "Batch Id 101 is having validation accuracy of 81.06617647058823\n",
            "Batch Id 102 is having validation loss of 0.6860275268554688\n",
            "1.042628526687622\n",
            "Batch Id 102 is having validation accuracy of 80.97694174757281\n",
            "Batch Id 103 is having validation loss of 0.6842493414878845\n",
            "0.5010975003242493\n",
            "Batch Id 103 is having validation accuracy of 81.00961538461539\n",
            "Batch Id 104 is having validation loss of 0.684233546257019\n",
            "0.682592511177063\n",
            "Batch Id 104 is having validation accuracy of 81.04166666666667\n",
            "Batch Id 105 is having validation loss of 0.6827325224876404\n",
            "0.5251268148422241\n",
            "Batch Id 105 is having validation accuracy of 81.13207547169812\n",
            "Batch Id 106 is having validation loss of 0.6816642880439758\n",
            "0.5684289932250977\n",
            "Batch Id 106 is having validation accuracy of 81.10397196261682\n",
            "Batch Id 107 is having validation loss of 0.6798192858695984\n",
            "0.48240548372268677\n",
            "Batch Id 107 is having validation accuracy of 81.13425925925925\n",
            "Batch Id 108 is having validation loss of 0.6790119409561157\n",
            "0.591818630695343\n",
            "Batch Id 108 is having validation accuracy of 81.19266055045871\n",
            "Batch Id 109 is having validation loss of 0.6795861124992371\n",
            "0.7421724200248718\n",
            "Batch Id 109 is having validation accuracy of 81.13636363636364\n",
            "Batch Id 110 is having validation loss of 0.6795021295547485\n",
            "0.6702664494514465\n",
            "Batch Id 110 is having validation accuracy of 81.02477477477477\n",
            "Batch Id 111 is having validation loss of 0.6812652945518494\n",
            "0.8769793510437012\n",
            "Batch Id 111 is having validation accuracy of 80.99888392857143\n",
            "Batch Id 112 is having validation loss of 0.6838688850402832\n",
            "0.9754679799079895\n",
            "Batch Id 112 is having validation accuracy of 80.89048672566372\n",
            "Batch Id 113 is having validation loss of 0.6826015114784241\n",
            "0.5393912196159363\n",
            "Batch Id 113 is having validation accuracy of 80.92105263157895\n",
            "Batch Id 114 is having validation loss of 0.6829186677932739\n",
            "0.7190777063369751\n",
            "Batch Id 114 is having validation accuracy of 80.89673913043478\n",
            "Batch Id 115 is having validation loss of 0.6824616193771362\n",
            "0.6299017071723938\n",
            "Batch Id 115 is having validation accuracy of 80.95366379310344\n",
            "Batch Id 116 is having validation loss of 0.6850138902664185\n",
            "0.9810802936553955\n",
            "Batch Id 116 is having validation accuracy of 80.84935897435898\n",
            "Batch Id 117 is having validation loss of 0.6882224082946777\n",
            "1.0636208057403564\n",
            "Batch Id 117 is having validation accuracy of 80.8792372881356\n",
            "Batch Id 118 is having validation loss of 0.6876122951507568\n",
            "0.6156171560287476\n",
            "Batch Id 118 is having validation accuracy of 80.88235294117646\n",
            "Batch Id 119 is having validation loss of 0.6860131621360779\n",
            "0.49571841955184937\n",
            "Batch Id 119 is having validation accuracy of 80.91145833333333\n",
            "Batch Id 120 is having validation loss of 0.6845456957817078\n",
            "0.5084512829780579\n",
            "Batch Id 120 is having validation accuracy of 80.9400826446281\n",
            "Batch Id 121 is having validation loss of 0.6849356889724731\n",
            "0.7321242690086365\n",
            "Batch Id 121 is having validation accuracy of 80.94262295081967\n",
            "Batch Id 122 is having validation loss of 0.6870590448379517\n",
            "0.9461102485656738\n",
            "Batch Id 122 is having validation accuracy of 80.84349593495935\n",
            "Batch Id 123 is having validation loss of 0.6852448582649231\n",
            "0.4620993137359619\n",
            "Batch Id 123 is having validation accuracy of 80.89717741935483\n",
            "Batch Id 124 is having validation loss of 0.6835094690322876\n",
            "0.46832075715065\n",
            "Batch Id 124 is having validation accuracy of 81.0\n",
            "Batch Id 125 is having validation loss of 0.6869201064109802\n",
            "1.1132513284683228\n",
            "Batch Id 125 is having validation accuracy of 80.92757936507937\n",
            "Batch Id 126 is having validation loss of 0.6860232949256897\n",
            "0.5730252861976624\n",
            "Batch Id 126 is having validation accuracy of 80.93011811023622\n",
            "Batch Id 127 is having validation loss of 0.6862010955810547\n",
            "0.7087841033935547\n",
            "Batch Id 127 is having validation accuracy of 80.908203125\n",
            "Batch Id 128 is having validation loss of 0.6885033845901489\n",
            "0.9831946492195129\n",
            "Batch Id 128 is having validation accuracy of 80.83817829457364\n",
            "Batch Id 129 is having validation loss of 0.6884665489196777\n",
            "0.6837117671966553\n",
            "Batch Id 129 is having validation accuracy of 80.79326923076923\n",
            "Batch Id 130 is having validation loss of 0.6883397102355957\n",
            "0.6718510389328003\n",
            "Batch Id 130 is having validation accuracy of 80.74904580152672\n",
            "Batch Id 131 is having validation loss of 0.6892892122268677\n",
            "0.8136732578277588\n",
            "Batch Id 131 is having validation accuracy of 80.72916666666667\n",
            "Batch Id 132 is having validation loss of 0.6904038786888123\n",
            "0.8375436663627625\n",
            "Batch Id 132 is having validation accuracy of 80.70958646616542\n",
            "Batch Id 133 is having validation loss of 0.6901234984397888\n",
            "0.652831494808197\n",
            "Batch Id 133 is having validation accuracy of 80.76026119402985\n",
            "Batch Id 134 is having validation loss of 0.6932170987129211\n",
            "1.1077613830566406\n",
            "Batch Id 134 is having validation accuracy of 80.67129629629629\n",
            "Batch Id 135 is having validation loss of 0.6945268511772156\n",
            "0.8713425993919373\n",
            "Batch Id 135 is having validation accuracy of 80.62959558823529\n",
            "Batch Id 136 is having validation loss of 0.698540985584259\n",
            "1.2444627285003662\n",
            "Batch Id 136 is having validation accuracy of 80.54288321167883\n",
            "Batch Id 137 is having validation loss of 0.6980572938919067\n",
            "0.6317936182022095\n",
            "Batch Id 137 is having validation accuracy of 80.52536231884058\n",
            "Batch Id 138 is having validation loss of 0.6987127661705017\n",
            "0.7891697287559509\n",
            "Batch Id 138 is having validation accuracy of 80.46312949640287\n",
            "Batch Id 139 is having validation loss of 0.6970767378807068\n",
            "0.46966683864593506\n",
            "Batch Id 139 is having validation accuracy of 80.53571428571429\n",
            "Batch Id 140 is having validation loss of 0.6982376575469971\n",
            "0.8607643246650696\n",
            "Batch Id 140 is having validation accuracy of 80.42996453900709\n",
            "Batch Id 141 is having validation loss of 0.6986310482025146\n",
            "0.7541018724441528\n",
            "Batch Id 141 is having validation accuracy of 80.45774647887323\n",
            "Batch Id 142 is having validation loss of 0.7026804089546204\n",
            "1.2776881456375122\n",
            "Batch Id 142 is having validation accuracy of 80.37587412587412\n",
            "Batch Id 143 is having validation loss of 0.7022135853767395\n",
            "0.6354584693908691\n",
            "Batch Id 143 is having validation accuracy of 80.40364583333333\n",
            "Batch Id 144 is having validation loss of 0.7006438374519348\n",
            "0.47459977865219116\n",
            "Batch Id 144 is having validation accuracy of 80.43103448275862\n",
            "Batch Id 145 is having validation loss of 0.699249804019928\n",
            "0.4971136748790741\n",
            "Batch Id 145 is having validation accuracy of 80.45804794520548\n",
            "Batch Id 146 is having validation loss of 0.6991779804229736\n",
            "0.6886879205703735\n",
            "Batch Id 146 is having validation accuracy of 80.46343537414965\n",
            "Batch Id 147 is having validation loss of 0.701413631439209\n",
            "1.0300514698028564\n",
            "Batch Id 147 is having validation accuracy of 80.4054054054054\n",
            "Batch Id 148 is having validation loss of 0.7011281251907349\n",
            "0.6588712930679321\n",
            "Batch Id 148 is having validation accuracy of 80.39010067114094\n",
            "Batch Id 149 is having validation loss of 0.6999439001083374\n",
            "0.5234968662261963\n",
            "Batch Id 149 is having validation accuracy of 80.45833333333333\n",
            "Batch Id 150 is having validation loss of 0.7011932134628296\n",
            "0.888592541217804\n",
            "Batch Id 150 is having validation accuracy of 80.38079470198676\n",
            "Batch Id 151 is having validation loss of 0.7020878195762634\n",
            "0.8371711373329163\n",
            "Batch Id 151 is having validation accuracy of 80.34539473684211\n",
            "Batch Id 152 is having validation loss of 0.7012028694152832\n",
            "0.5666944980621338\n",
            "Batch Id 152 is having validation accuracy of 80.3921568627451\n",
            "Batch Id 153 is having validation loss of 0.7003193497657776\n",
            "0.5651416778564453\n",
            "Batch Id 153 is having validation accuracy of 80.35714285714286\n",
            "Batch Id 154 is having validation loss of 0.702177882194519\n",
            "0.988387405872345\n",
            "Batch Id 154 is having validation accuracy of 80.3225806451613\n",
            "Batch Id 155 is having validation loss of 0.6997167468070984\n",
            "0.31824249029159546\n",
            "Batch Id 155 is having validation accuracy of 80.3886217948718\n",
            "Batch Id 156 is having validation loss of 0.7028579711914062\n",
            "1.1928869485855103\n",
            "Batch Id 156 is having validation accuracy of 80.31449044585987\n",
            "Batch Id 157 is having validation loss of 0.7024389505386353\n",
            "0.6366500854492188\n",
            "Batch Id 157 is having validation accuracy of 80.34018987341773\n",
            "Batch Id 158 is having validation loss of 0.7025346159934998\n",
            "0.7176513075828552\n",
            "Batch Id 158 is having validation accuracy of 80.34591194968553\n",
            "Batch Id 159 is having validation loss of 0.7024544477462769\n",
            "0.6897037625312805\n",
            "Batch Id 159 is having validation accuracy of 80.33203125\n",
            "Batch Id 160 is having validation loss of 0.706143856048584\n",
            "1.296452283859253\n",
            "Batch Id 160 is having validation accuracy of 80.22127329192547\n",
            "Batch Id 161 is having validation loss of 0.7045870423316956\n",
            "0.4539433419704437\n",
            "Batch Id 161 is having validation accuracy of 80.24691358024691\n",
            "Batch Id 162 is having validation loss of 0.7052550315856934\n",
            "0.8134680986404419\n",
            "Batch Id 162 is having validation accuracy of 80.23389570552148\n",
            "Batch Id 163 is having validation loss of 0.7057928442955017\n",
            "0.7934528589248657\n",
            "Batch Id 163 is having validation accuracy of 80.24009146341463\n",
            "Batch Id 164 is having validation loss of 0.7069835662841797\n",
            "0.9022586941719055\n",
            "Batch Id 164 is having validation accuracy of 80.22727272727273\n",
            "Batch Id 165 is having validation loss of 0.7095351815223694\n",
            "1.130553126335144\n",
            "Batch Id 165 is having validation accuracy of 80.19578313253012\n",
            "Batch Id 166 is having validation loss of 0.7059276103973389\n",
            "0.10706660151481628\n",
            "Batch Id 166 is having validation accuracy of 80.29565868263474\n",
            "Batch Id 167 is having validation loss of 0.7052721977233887\n",
            "0.5958136916160583\n",
            "Batch Id 167 is having validation accuracy of 80.31994047619048\n",
            "Batch Id 168 is having validation loss of 0.7019879221916199\n",
            "0.15023161470890045\n",
            "Batch Id 168 is having validation accuracy of 80.43639053254438\n",
            "Batch Id 169 is having validation loss of 0.6999850869178772\n",
            "0.3615092635154724\n",
            "Batch Id 169 is having validation accuracy of 80.4779411764706\n",
            "Batch Id 170 is having validation loss of 0.6993388533592224\n",
            "0.5894819498062134\n",
            "Batch Id 170 is having validation accuracy of 80.48245614035088\n",
            "Batch Id 171 is having validation loss of 0.7003616094589233\n",
            "0.8752539157867432\n",
            "Batch Id 171 is having validation accuracy of 80.41424418604652\n",
            "Batch Id 172 is having validation loss of 0.6975368857383728\n",
            "0.21168753504753113\n",
            "Batch Id 172 is having validation accuracy of 80.49132947976878\n",
            "Batch Id 173 is having validation loss of 0.6978636980056763\n",
            "0.754406213760376\n",
            "Batch Id 173 is having validation accuracy of 80.47772988505747\n",
            "Batch Id 174 is having validation loss of 0.699135959148407\n",
            "0.920508086681366\n",
            "Batch Id 174 is having validation accuracy of 80.41071428571429\n",
            "Batch Id 175 is having validation loss of 0.7003026604652405\n",
            "0.9044754505157471\n",
            "Batch Id 175 is having validation accuracy of 80.39772727272727\n",
            "Batch Id 176 is having validation loss of 0.6999255418777466\n",
            "0.6335578560829163\n",
            "Batch Id 176 is having validation accuracy of 80.38488700564972\n",
            "Batch Id 177 is having validation loss of 0.69914710521698\n",
            "0.5613669753074646\n",
            "Batch Id 177 is having validation accuracy of 80.38974719101124\n",
            "Batch Id 178 is having validation loss of 0.6992830038070679\n",
            "0.7234749794006348\n",
            "Batch Id 178 is having validation accuracy of 80.3945530726257\n",
            "Batch Id 179 is having validation loss of 0.7007583379745483\n",
            "0.9648423194885254\n",
            "Batch Id 179 is having validation accuracy of 80.34722222222223\n",
            "Batch Id 180 is having validation loss of 0.7013591527938843\n",
            "0.8095027804374695\n",
            "Batch Id 180 is having validation accuracy of 80.33494475138122\n",
            "Batch Id 181 is having validation loss of 0.7013756632804871\n",
            "0.7043631076812744\n",
            "Batch Id 181 is having validation accuracy of 80.3228021978022\n",
            "Batch Id 182 is having validation loss of 0.7025522589683533\n",
            "0.9166881442070007\n",
            "Batch Id 182 is having validation accuracy of 80.29371584699453\n",
            "Batch Id 183 is having validation loss of 0.7014209628105164\n",
            "0.49439504742622375\n",
            "Batch Id 183 is having validation accuracy of 80.31589673913044\n",
            "Batch Id 184 is having validation loss of 0.6998045444488525\n",
            "0.40238314867019653\n",
            "Batch Id 184 is having validation accuracy of 80.37162162162163\n",
            "Batch Id 185 is having validation loss of 0.6983250379562378\n",
            "0.42462173104286194\n",
            "Batch Id 185 is having validation accuracy of 80.40994623655914\n",
            "Batch Id 186 is having validation loss of 0.6983290910720825\n",
            "0.699083149433136\n",
            "Batch Id 186 is having validation accuracy of 80.43114973262033\n",
            "Batch Id 187 is having validation loss of 0.7002618312835693\n",
            "1.0616793632507324\n",
            "Batch Id 187 is having validation accuracy of 80.3690159574468\n",
            "Batch Id 188 is having validation loss of 0.7020936608314514\n",
            "1.0464774370193481\n",
            "Batch Id 188 is having validation accuracy of 80.30753968253968\n",
            "Batch Id 189 is having validation loss of 0.7009116411209106\n",
            "0.4775109589099884\n",
            "Batch Id 189 is having validation accuracy of 80.32894736842105\n",
            "Batch Id 190 is having validation loss of 0.701016366481781\n",
            "0.7209132313728333\n",
            "Batch Id 190 is having validation accuracy of 80.30104712041884\n",
            "Batch Id 191 is having validation loss of 0.7015724182128906\n",
            "0.807779848575592\n",
            "Batch Id 191 is having validation accuracy of 80.322265625\n",
            "Batch Id 192 is having validation loss of 0.701305627822876\n",
            "0.6500808596611023\n",
            "Batch Id 192 is having validation accuracy of 80.34326424870466\n",
            "Batch Id 193 is having validation loss of 0.7014550566673279\n",
            "0.7302945256233215\n",
            "Batch Id 193 is having validation accuracy of 80.33182989690722\n",
            "Batch Id 194 is having validation loss of 0.7010475993156433\n",
            "0.6219986081123352\n",
            "Batch Id 194 is having validation accuracy of 80.32051282051282\n",
            "Batch Id 195 is having validation loss of 0.7015826106071472\n",
            "0.8059155344963074\n",
            "Batch Id 195 is having validation accuracy of 80.29336734693878\n",
            "Batch Id 196 is having validation loss of 0.7022244930267334\n",
            "0.8280338644981384\n",
            "Batch Id 196 is having validation accuracy of 80.28236040609137\n",
            "Batch Id 197 is having validation loss of 0.7042750120162964\n",
            "1.10822594165802\n",
            "Batch Id 197 is having validation accuracy of 80.28724747474747\n",
            "Batch Id 198 is having validation loss of 0.7061107158660889\n",
            "1.0695815086364746\n",
            "Batch Id 198 is having validation accuracy of 80.19786432160804\n",
            "Batch Id 199 is having validation loss of 0.7054082751274109\n",
            "0.5656256079673767\n",
            "Batch Id 199 is having validation accuracy of 80.203125\n",
            "Batch Id 200 is having validation loss of 0.7072298526763916\n",
            "1.071549892425537\n",
            "Batch Id 200 is having validation accuracy of 80.17723880597015\n",
            "Batch Id 201 is having validation loss of 0.7089418768882751\n",
            "1.0530613660812378\n",
            "Batch Id 201 is having validation accuracy of 80.13613861386139\n",
            "Batch Id 202 is having validation loss of 0.712373673915863\n",
            "1.40559983253479\n",
            "Batch Id 202 is having validation accuracy of 80.08004926108374\n",
            "Batch Id 203 is having validation loss of 0.7121055722236633\n",
            "0.6576773524284363\n",
            "Batch Id 203 is having validation accuracy of 80.05514705882354\n",
            "Batch Id 204 is having validation loss of 0.7128956317901611\n",
            "0.8740673661231995\n",
            "Batch Id 204 is having validation accuracy of 80.03048780487805\n",
            "Batch Id 205 is having validation loss of 0.710858166217804\n",
            "0.2931772470474243\n",
            "Batch Id 205 is having validation accuracy of 80.1122572815534\n",
            "Batch Id 206 is having validation loss of 0.7093954086303711\n",
            "0.4080668091773987\n",
            "Batch Id 206 is having validation accuracy of 80.1328502415459\n",
            "Batch Id 207 is having validation loss of 0.7082180380821228\n",
            "0.46449920535087585\n",
            "Batch Id 207 is having validation accuracy of 80.18329326923077\n",
            "Batch Id 208 is having validation loss of 0.7084471583366394\n",
            "0.7561062574386597\n",
            "Batch Id 208 is having validation accuracy of 80.14354066985646\n",
            "Batch Id 209 is having validation loss of 0.7071199417114258\n",
            "0.4297265410423279\n",
            "Batch Id 209 is having validation accuracy of 80.16369047619048\n",
            "Batch Id 210 is having validation loss of 0.7121100425720215\n",
            "1.7600252628326416\n",
            "Batch Id 210 is having validation accuracy of 80.06516587677726\n",
            "Batch Id 211 is having validation loss of 0.710157036781311\n",
            "0.2980746328830719\n",
            "Batch Id 211 is having validation accuracy of 80.1002358490566\n",
            "Batch Id 212 is having validation loss of 0.7124742269515991\n",
            "1.2037179470062256\n",
            "Batch Id 212 is having validation accuracy of 80.0469483568075\n",
            "Batch Id 213 is having validation loss of 0.7151180505752563\n",
            "1.2782498598098755\n",
            "Batch Id 213 is having validation accuracy of 79.9357476635514\n",
            "Batch Id 214 is having validation loss of 0.7140953540802002\n",
            "0.49524199962615967\n",
            "Batch Id 214 is having validation accuracy of 79.98546511627907\n",
            "Batch Id 215 is having validation loss of 0.7143475413322449\n",
            "0.7685660123825073\n",
            "Batch Id 215 is having validation accuracy of 79.99131944444444\n",
            "Batch Id 216 is having validation loss of 0.7150765061378479\n",
            "0.8725324869155884\n",
            "Batch Id 216 is having validation accuracy of 79.95391705069125\n",
            "Batch Id 217 is having validation loss of 0.71420818567276\n",
            "0.5257777571678162\n",
            "Batch Id 217 is having validation accuracy of 79.97419724770643\n",
            "Batch Id 218 is having validation loss of 0.7143517136573792\n",
            "0.7456440329551697\n",
            "Batch Id 218 is having validation accuracy of 79.95148401826484\n",
            "Batch Id 219 is having validation loss of 0.7148754000663757\n",
            "0.8295678496360779\n",
            "Batch Id 219 is having validation accuracy of 79.91477272727273\n",
            "Batch Id 220 is having validation loss of 0.7132404446601868\n",
            "0.35355567932128906\n",
            "Batch Id 220 is having validation accuracy of 79.94909502262443\n",
            "Batch Id 221 is having validation loss of 0.7140428423881531\n",
            "0.8913705945014954\n",
            "Batch Id 221 is having validation accuracy of 79.9268018018018\n",
            "Batch Id 222 is having validation loss of 0.7143983840942383\n",
            "0.7933334708213806\n",
            "Batch Id 222 is having validation accuracy of 79.93273542600897\n",
            "Batch Id 223 is having validation loss of 0.7145194411277771\n",
            "0.7415115833282471\n",
            "Batch Id 223 is having validation accuracy of 79.92466517857143\n",
            "Batch Id 224 is having validation loss of 0.7144098281860352\n",
            "0.6898583173751831\n",
            "Batch Id 224 is having validation accuracy of 79.94444444444444\n",
            "Batch Id 225 is having validation loss of 0.7143734693527222\n",
            "0.7061892747879028\n",
            "Batch Id 225 is having validation accuracy of 79.95022123893806\n",
            "Batch Id 226 is having validation loss of 0.7129766345024109\n",
            "0.39729824662208557\n",
            "Batch Id 226 is having validation accuracy of 79.96971365638767\n",
            "Batch Id 227 is having validation loss of 0.712526798248291\n",
            "0.610415518283844\n",
            "Batch Id 227 is having validation accuracy of 79.9890350877193\n",
            "Batch Id 228 is having validation loss of 0.7129681706428528\n",
            "0.8136007189750671\n",
            "Batch Id 228 is having validation accuracy of 80.00818777292577\n",
            "Batch Id 229 is having validation loss of 0.7132423520088196\n",
            "0.7760269045829773\n",
            "Batch Id 229 is having validation accuracy of 80.0\n",
            "Batch Id 230 is having validation loss of 0.7125685214996338\n",
            "0.5575876832008362\n",
            "Batch Id 230 is having validation accuracy of 80.00541125541126\n",
            "Batch Id 231 is having validation loss of 0.7121530771255493\n",
            "0.6161901354789734\n",
            "Batch Id 231 is having validation accuracy of 80.02424568965517\n",
            "Batch Id 232 is having validation loss of 0.7118292450904846\n",
            "0.6366986632347107\n",
            "Batch Id 232 is having validation accuracy of 80.01609442060087\n",
            "Batch Id 233 is having validation loss of 0.7102632522583008\n",
            "0.34538477659225464\n",
            "Batch Id 233 is having validation accuracy of 80.07478632478633\n",
            "Batch Id 234 is having validation loss of 0.7094681859016418\n",
            "0.5234237313270569\n",
            "Batch Id 234 is having validation accuracy of 80.13297872340425\n",
            "Batch Id 235 is having validation loss of 0.7105620503425598\n",
            "0.9676238298416138\n",
            "Batch Id 235 is having validation accuracy of 80.11122881355932\n",
            "Batch Id 236 is having validation loss of 0.710037112236023\n",
            "0.5861514806747437\n",
            "Batch Id 236 is having validation accuracy of 80.08966244725738\n",
            "Batch Id 237 is having validation loss of 0.7102222442626953\n",
            "0.7540991306304932\n",
            "Batch Id 237 is having validation accuracy of 80.09453781512605\n",
            "Batch Id 238 is having validation loss of 0.7134708166122437\n",
            "1.4866286516189575\n",
            "Batch Id 238 is having validation accuracy of 79.99476987447699\n",
            "Batch Id 239 is having validation loss of 0.7114554643630981\n",
            "0.22978484630584717\n",
            "Batch Id 239 is having validation accuracy of 80.05208333333333\n",
            "Batch Id 240 is having validation loss of 0.7120546698570251\n",
            "0.8558644652366638\n",
            "Batch Id 240 is having validation accuracy of 80.04408713692946\n",
            "Batch Id 241 is having validation loss of 0.7121948599815369\n",
            "0.7459784150123596\n",
            "Batch Id 241 is having validation accuracy of 80.03615702479338\n",
            "Batch Id 242 is having validation loss of 0.7107882499694824\n",
            "0.37039071321487427\n",
            "Batch Id 242 is having validation accuracy of 80.0925925925926\n",
            "Batch Id 243 is having validation loss of 0.7092199921607971\n",
            "0.3281363844871521\n",
            "Batch Id 243 is having validation accuracy of 80.1485655737705\n",
            "Batch Id 244 is having validation loss of 0.7086488604545593\n",
            "0.5692876577377319\n",
            "Batch Id 244 is having validation accuracy of 80.15306122448979\n",
            "Batch Id 245 is having validation loss of 0.7101311683654785\n",
            "1.0732980966567993\n",
            "Batch Id 245 is having validation accuracy of 80.0685975609756\n",
            "Batch Id 246 is having validation loss of 0.711725652217865\n",
            "1.1039642095565796\n",
            "Batch Id 246 is having validation accuracy of 80.06072874493927\n",
            "Batch Id 247 is having validation loss of 0.7112129926681519\n",
            "0.5845810770988464\n",
            "Batch Id 247 is having validation accuracy of 80.05292338709677\n",
            "Batch Id 248 is having validation loss of 0.7116112112998962\n",
            "0.810371458530426\n",
            "Batch Id 248 is having validation accuracy of 80.05773092369478\n",
            "Batch Id 249 is having validation loss of 0.7109212875366211\n",
            "0.5391272306442261\n",
            "Batch Id 249 is having validation accuracy of 80.05\n",
            "Batch Id 250 is having validation loss of 0.7101162672042847\n",
            "0.5088682770729065\n",
            "Batch Id 250 is having validation accuracy of 80.06723107569721\n",
            "Batch Id 251 is having validation loss of 0.7113686800003052\n",
            "1.025729775428772\n",
            "Batch Id 251 is having validation accuracy of 80.04712301587301\n",
            "Batch Id 252 is having validation loss of 0.7111984491348267\n",
            "0.6682990789413452\n",
            "Batch Id 252 is having validation accuracy of 80.03952569169961\n",
            "Batch Id 253 is having validation loss of 0.7145436406135559\n",
            "1.5608758926391602\n",
            "Batch Id 253 is having validation accuracy of 79.98277559055119\n",
            "Batch Id 254 is having validation loss of 0.7137235403060913\n",
            "0.5054214000701904\n",
            "Batch Id 254 is having validation accuracy of 79.97549019607843\n",
            "Batch Id 255 is having validation loss of 0.712522029876709\n",
            "0.4061388671398163\n",
            "Batch Id 255 is having validation accuracy of 80.0048828125\n",
            "Batch Id 256 is having validation loss of 0.7123554944992065\n",
            "0.6697245836257935\n",
            "Batch Id 256 is having validation accuracy of 79.99756809338521\n",
            "Batch Id 257 is having validation loss of 0.710405170917511\n",
            "0.20917722582817078\n",
            "Batch Id 257 is having validation accuracy of 80.05087209302326\n",
            "Batch Id 258 is having validation loss of 0.7101044654846191\n",
            "0.6325250267982483\n",
            "Batch Id 258 is having validation accuracy of 80.09169884169884\n",
            "Batch Id 259 is having validation loss of 0.7089774012565613\n",
            "0.41707006096839905\n",
            "Batch Id 259 is having validation accuracy of 80.13221153846153\n",
            "Batch Id 260 is having validation loss of 0.7077452540397644\n",
            "0.38738128542900085\n",
            "Batch Id 260 is having validation accuracy of 80.1484674329502\n",
            "Batch Id 261 is having validation loss of 0.7076086401939392\n",
            "0.6719546318054199\n",
            "Batch Id 261 is having validation accuracy of 80.1526717557252\n",
            "Batch Id 262 is having validation loss of 0.7072297930717468\n",
            "0.6079786419868469\n",
            "Batch Id 262 is having validation accuracy of 80.14496197718631\n",
            "Batch Id 263 is having validation loss of 0.7070228457450867\n",
            "0.652590274810791\n",
            "Batch Id 263 is having validation accuracy of 80.14914772727273\n",
            "Batch Id 264 is having validation loss of 0.7060896158218384\n",
            "0.4597153663635254\n",
            "Batch Id 264 is having validation accuracy of 80.15330188679245\n",
            "Batch Id 265 is having validation loss of 0.7076960802078247\n",
            "1.1334162950515747\n",
            "Batch Id 265 is having validation accuracy of 80.11043233082707\n",
            "Batch Id 266 is having validation loss of 0.7084494233131409\n",
            "0.9088343977928162\n",
            "Batch Id 266 is having validation accuracy of 80.09129213483146\n",
            "Batch Id 267 is having validation loss of 0.7079631686210632\n",
            "0.5781320929527283\n",
            "Batch Id 267 is having validation accuracy of 80.11893656716418\n",
            "Batch Id 268 is having validation loss of 0.7063226699829102\n",
            "0.2666706442832947\n",
            "Batch Id 268 is having validation accuracy of 80.15799256505576\n",
            "Batch Id 269 is having validation loss of 0.7058178782463074\n",
            "0.5700284242630005\n",
            "Batch Id 269 is having validation accuracy of 80.17361111111111\n",
            "Batch Id 270 is having validation loss of 0.7074170708656311\n",
            "1.139195203781128\n",
            "Batch Id 270 is having validation accuracy of 80.13145756457564\n",
            "Batch Id 271 is having validation loss of 0.7073472142219543\n",
            "0.6884217262268066\n",
            "Batch Id 271 is having validation accuracy of 80.12408088235294\n",
            "Batch Id 272 is having validation loss of 0.7074398994445801\n",
            "0.7326452136039734\n",
            "Batch Id 272 is having validation accuracy of 80.10531135531136\n",
            "Batch Id 273 is having validation loss of 0.7079554200172424\n",
            "0.8486947417259216\n",
            "Batch Id 273 is having validation accuracy of 80.09808394160584\n",
            "Batch Id 274 is having validation loss of 0.7081702351570129\n",
            "0.7670217752456665\n",
            "Batch Id 274 is having validation accuracy of 80.07954545454545\n",
            "Batch Id 275 is having validation loss of 0.7084282040596008\n",
            "0.7793775200843811\n",
            "Batch Id 275 is having validation accuracy of 80.09510869565217\n",
            "Batch Id 276 is having validation loss of 0.7074632048606873\n",
            "0.4411202073097229\n",
            "Batch Id 276 is having validation accuracy of 80.09927797833934\n",
            "Batch Id 277 is having validation loss of 0.704918384552002\n",
            "6.8544877649401315e-06\n",
            "Batch Id 277 is having validation accuracy of 80.1037672005414\n",
            "Эпоха #1 train_loss: 8.296920896100346e-06, val_loss: 7.950804865686223e-05\n",
            "Потрачено 23.0 минут на 1 эпоху\n"
          ]
        }
      ],
      "source": [
        "# Загрузка словарей с лоссами\n",
        "if last_epoch is not None:\n",
        "    train_losses = checkpoint['losses_train']\n",
        "    val_losses = checkpoint['losses_val']\n",
        "else:\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "if last_epoch is None:\n",
        "    start_epoch = 0\n",
        "else:\n",
        "    start_epoch = last_epoch +1\n",
        "\n",
        "try:\n",
        "    for epoch in range(start_epoch, n_epochs):\n",
        "        start = time.time()\n",
        "        train_loss = train(train_data_loader, epoch)\n",
        "        val_loss = val(val_data_loader, epoch)\n",
        "        #val_loss = 0 # удалить потом\n",
        "        #lr_scheduler.step()\n",
        "        print(f\"Эпоха #{epoch} train_loss: {train_loss}, val_loss: {val_loss}\")  \n",
        "        end = time.time()\n",
        "        print(f\"Потрачено {round((end - start) / 60, 1)} минут на {epoch} эпоху\")\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        torch.save({'epoch': epoch,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    #'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
        "                    'losses_train': train_losses,\n",
        "                    'losses_val': val_losses\n",
        "                    }, os.path.join(checkpoints_path, f'chkpt_classifier_{model_name}_{epoch}.pth'))\n",
        "    \n",
        "        torch.save(model, os.path.join(checkpoints_path, f'model_classifier_{model_name}_{epoch}.pth'))\n",
        "    \n",
        "except KeyboardInterrupt:\n",
        "    print('Прервано пользователем')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(71, 72)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "item = 13\n",
        "model.eval()\n",
        "pred = model(val_dataset.__getitem__(item)['images'].unsqueeze(0).to(device)).data.max(1,keepdim=True)[1], \n",
        "int(pred[0][0][0]), int(val_dataset.__getitem__(item)['labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "uEkHoBJQtHIe"
      },
      "outputs": [],
      "source": [
        "last_epoch = 2\n",
        "# Загрузка весов модели\n",
        "checkpoint = torch.load(os.path.join(checkpoints_path, f'model_detector_resnet50_augmented_{last_epoch}.pth'), map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "lr_scheduler.load_state_dict(checkpoint['lr_scheduler_state_dict'])\n",
        "#epoch = checkpoint['epoch']\n",
        "train_losses = checkpoint['losses_train']\n",
        "val_losses = checkpoint['losses_val']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KK7V4WjRtHIe"
      },
      "outputs": [],
      "source": [
        "if last_epoch == None:\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "else:\n",
        "    # Загрузка весов модели\n",
        "    checkpoint = torch.load(os.path.join(checkpoints_path, f'chkpt_{model_name}_{last_epoch}.pth'), map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    lr_scheduler.load_state_dict(checkpoint['lr_scheduler_state_dict'])\n",
        "    #epoch = checkpoint['epoch']\n",
        "    train_losses = checkpoint['losses_train']\n",
        "    val_losses = checkpoint['losses_val']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "r4ln2ar7t_dK"
      },
      "outputs": [],
      "source": [
        "last_epoch = 3\n",
        "model_name = 'detector_resnet50_augmented'\n",
        "\n",
        "\n",
        "if last_epoch == None:\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "else:\n",
        "    # Загрузка весов модели\n",
        "    checkpoint = torch.load(os.path.join(checkpoints_path, f'chkpt_{model_name}_{last_epoch}.pth'), map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    lr_scheduler.load_state_dict(checkpoint['lr_scheduler_state_dict'])\n",
        "    #epoch = checkpoint['epoch']\n",
        "    train_losses = checkpoint['losses_train']\n",
        "    val_losses = checkpoint['losses_val']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "KZUIkmvXGEGm"
      },
      "outputs": [],
      "source": [
        "model_name = 'detector_resnet50_augmented'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPWlmt4_J28a",
        "outputId": "cc17a6f9-1d38-4f4c-da5e-72d95b81de97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tЭпоха 4. Итерация 0/3387. Loss: 0.11603625863790512\n",
            "\tЭпоха 4. Итерация 50/3387. Loss: 0.09132763743400574\n",
            "\tЭпоха 4. Итерация 100/3387. Loss: 0.09525351226329803\n",
            "\tЭпоха 4. Итерация 150/3387. Loss: 0.08201088011264801\n",
            "\tЭпоха 4. Итерация 200/3387. Loss: 0.11788337677717209\n",
            "\tЭпоха 4. Итерация 250/3387. Loss: 0.09946923702955246\n",
            "\tЭпоха 4. Итерация 300/3387. Loss: 0.09350793063640594\n",
            "\tЭпоха 4. Итерация 350/3387. Loss: 0.09657343477010727\n",
            "\tЭпоха 4. Итерация 400/3387. Loss: 0.08998836576938629\n",
            "\tЭпоха 4. Итерация 450/3387. Loss: 0.08942896872758865\n",
            "\tЭпоха 4. Итерация 500/3387. Loss: 0.0664619505405426\n",
            "\tЭпоха 4. Итерация 550/3387. Loss: 0.08274924755096436\n",
            "\tЭпоха 4. Итерация 600/3387. Loss: 0.06323593109846115\n",
            "\tЭпоха 4. Итерация 650/3387. Loss: 0.07226131856441498\n",
            "\tЭпоха 4. Итерация 700/3387. Loss: 0.07295990735292435\n",
            "\tЭпоха 4. Итерация 750/3387. Loss: 0.06773833930492401\n",
            "\tЭпоха 4. Итерация 800/3387. Loss: 0.11777892708778381\n",
            "\tЭпоха 4. Итерация 850/3387. Loss: 0.09019037336111069\n",
            "\tЭпоха 4. Итерация 900/3387. Loss: 0.07822129130363464\n",
            "\tЭпоха 4. Итерация 950/3387. Loss: 0.06052594259381294\n",
            "\tЭпоха 4. Итерация 1000/3387. Loss: 0.07345942407846451\n",
            "\tЭпоха 4. Итерация 1050/3387. Loss: 0.07717742770910263\n",
            "\tЭпоха 4. Итерация 1100/3387. Loss: 0.06409034878015518\n",
            "\tЭпоха 4. Итерация 1150/3387. Loss: 0.09143809229135513\n",
            "\tЭпоха 4. Итерация 1200/3387. Loss: 0.07615028321743011\n",
            "\tЭпоха 4. Итерация 1250/3387. Loss: 0.09745550155639648\n",
            "\tЭпоха 4. Итерация 1300/3387. Loss: 0.09170036762952805\n",
            "\tЭпоха 4. Итерация 1350/3387. Loss: 0.06408890336751938\n",
            "\tЭпоха 4. Итерация 1400/3387. Loss: 0.10413724184036255\n",
            "\tЭпоха 4. Итерация 1450/3387. Loss: 0.08004828542470932\n",
            "\tЭпоха 4. Итерация 1500/3387. Loss: 0.08290593326091766\n",
            "\tЭпоха 4. Итерация 1550/3387. Loss: 0.07502719014883041\n",
            "\tЭпоха 4. Итерация 1600/3387. Loss: 0.08438242971897125\n",
            "\tЭпоха 4. Итерация 1650/3387. Loss: 0.05449351295828819\n",
            "\tЭпоха 4. Итерация 1700/3387. Loss: 0.14444123208522797\n",
            "\tЭпоха 4. Итерация 1750/3387. Loss: 0.08392274379730225\n",
            "\tЭпоха 4. Итерация 1800/3387. Loss: 0.11227336525917053\n",
            "\tЭпоха 4. Итерация 1850/3387. Loss: 0.06348342448472977\n",
            "\tЭпоха 4. Итерация 1900/3387. Loss: 0.09295957535505295\n",
            "\tЭпоха 4. Итерация 1950/3387. Loss: 0.07618264853954315\n",
            "\tЭпоха 4. Итерация 2000/3387. Loss: 0.06287781894207001\n",
            "\tЭпоха 4. Итерация 2050/3387. Loss: 0.10074707120656967\n",
            "\tЭпоха 4. Итерация 2100/3387. Loss: 0.09345182776451111\n",
            "\tЭпоха 4. Итерация 2150/3387. Loss: 0.09661507606506348\n",
            "\tЭпоха 4. Итерация 2200/3387. Loss: 0.07314032316207886\n",
            "\tЭпоха 4. Итерация 2250/3387. Loss: 0.07773245871067047\n",
            "\tЭпоха 4. Итерация 2300/3387. Loss: 0.07221110910177231\n",
            "\tЭпоха 4. Итерация 2350/3387. Loss: 0.09711426496505737\n",
            "\tЭпоха 4. Итерация 2400/3387. Loss: 0.07089707255363464\n",
            "\tЭпоха 4. Итерация 2450/3387. Loss: 0.10149577260017395\n",
            "\tЭпоха 4. Итерация 2500/3387. Loss: 0.09024722874164581\n",
            "\tЭпоха 4. Итерация 2550/3387. Loss: 0.08304310590028763\n",
            "\tЭпоха 4. Итерация 2600/3387. Loss: 0.13452155888080597\n",
            "\tЭпоха 4. Итерация 2650/3387. Loss: 0.06688462197780609\n",
            "\tЭпоха 4. Итерация 2700/3387. Loss: 0.0783919245004654\n",
            "\tЭпоха 4. Итерация 2750/3387. Loss: 0.09943044185638428\n",
            "\tЭпоха 4. Итерация 2800/3387. Loss: 0.09517356753349304\n",
            "\tЭпоха 4. Итерация 2850/3387. Loss: 0.08536633849143982\n",
            "\tЭпоха 4. Итерация 2900/3387. Loss: 0.056873586028814316\n",
            "\tЭпоха 4. Итерация 2950/3387. Loss: 0.07926899939775467\n",
            "\tЭпоха 4. Итерация 3000/3387. Loss: 0.10848496854305267\n",
            "\tЭпоха 4. Итерация 3050/3387. Loss: 0.0754411518573761\n",
            "\tЭпоха 4. Итерация 3100/3387. Loss: 0.08374585211277008\n",
            "\tЭпоха 4. Итерация 3150/3387. Loss: 0.09555276483297348\n",
            "\tЭпоха 4. Итерация 3200/3387. Loss: 0.07873371988534927\n",
            "\tЭпоха 4. Итерация 3250/3387. Loss: 0.14205558598041534\n",
            "\tЭпоха 4. Итерация 3300/3387. Loss: 0.07441110908985138\n",
            "\tЭпоха 4. Итерация 3350/3387. Loss: 0.07840423285961151\n",
            "Эпоха #4 train_loss: 0.005604889409316438, val_loss: 0.005126563803851605\n",
            "Потрачено 77.4 минут на 4 эпоху\n",
            "\tЭпоха 5. Итерация 0/3387. Loss: 0.11043357849121094\n",
            "\tЭпоха 5. Итерация 50/3387. Loss: 0.10003184527158737\n",
            "\tЭпоха 5. Итерация 100/3387. Loss: 0.08751209080219269\n",
            "\tЭпоха 5. Итерация 150/3387. Loss: 0.07736430317163467\n",
            "\tЭпоха 5. Итерация 200/3387. Loss: 0.10406886786222458\n",
            "\tЭпоха 5. Итерация 250/3387. Loss: 0.07873156666755676\n",
            "\tЭпоха 5. Итерация 300/3387. Loss: 0.09615469723939896\n",
            "\tЭпоха 5. Итерация 350/3387. Loss: 0.12829256057739258\n",
            "\tЭпоха 5. Итерация 400/3387. Loss: 0.0928274542093277\n",
            "\tЭпоха 5. Итерация 450/3387. Loss: 0.07026734948158264\n",
            "\tЭпоха 5. Итерация 500/3387. Loss: 0.0896499902009964\n",
            "\tЭпоха 5. Итерация 550/3387. Loss: 0.13303382694721222\n",
            "\tЭпоха 5. Итерация 600/3387. Loss: 0.07851165533065796\n",
            "\tЭпоха 5. Итерация 650/3387. Loss: 0.11531458050012589\n",
            "\tЭпоха 5. Итерация 700/3387. Loss: 0.07176968455314636\n",
            "\tЭпоха 5. Итерация 750/3387. Loss: 0.08690912276506424\n",
            "\tЭпоха 5. Итерация 800/3387. Loss: 0.07366175204515457\n",
            "\tЭпоха 5. Итерация 850/3387. Loss: 0.08483704924583435\n",
            "\tЭпоха 5. Итерация 900/3387. Loss: 0.0893249586224556\n",
            "\tЭпоха 5. Итерация 950/3387. Loss: 0.10252492874860764\n",
            "\tЭпоха 5. Итерация 1000/3387. Loss: 0.09391957521438599\n",
            "\tЭпоха 5. Итерация 1050/3387. Loss: 0.09872449934482574\n",
            "\tЭпоха 5. Итерация 1100/3387. Loss: 0.09894886612892151\n",
            "\tЭпоха 5. Итерация 1150/3387. Loss: 0.09085991978645325\n",
            "\tЭпоха 5. Итерация 1200/3387. Loss: 0.09614866226911545\n",
            "\tЭпоха 5. Итерация 1250/3387. Loss: 0.10549784451723099\n",
            "\tЭпоха 5. Итерация 1300/3387. Loss: 0.08832625299692154\n",
            "\tЭпоха 5. Итерация 1350/3387. Loss: 0.10099366307258606\n",
            "\tЭпоха 5. Итерация 1400/3387. Loss: 0.12304644286632538\n",
            "\tЭпоха 5. Итерация 1450/3387. Loss: 0.0771268755197525\n",
            "\tЭпоха 5. Итерация 1500/3387. Loss: 0.07430917769670486\n",
            "\tЭпоха 5. Итерация 1550/3387. Loss: 0.10674313455820084\n",
            "\tЭпоха 5. Итерация 1600/3387. Loss: 0.12111136317253113\n",
            "\tЭпоха 5. Итерация 1650/3387. Loss: 0.10397383570671082\n",
            "\tЭпоха 5. Итерация 1700/3387. Loss: 0.08170681446790695\n",
            "\tЭпоха 5. Итерация 1750/3387. Loss: 0.08216825872659683\n",
            "\tЭпоха 5. Итерация 1800/3387. Loss: 0.11820347607135773\n",
            "\tЭпоха 5. Итерация 1850/3387. Loss: 0.09545254707336426\n",
            "\tЭпоха 5. Итерация 1900/3387. Loss: 0.05972716957330704\n",
            "\tЭпоха 5. Итерация 1950/3387. Loss: 0.10126010328531265\n",
            "\tЭпоха 5. Итерация 2000/3387. Loss: 0.10328716039657593\n",
            "\tЭпоха 5. Итерация 2050/3387. Loss: 0.08272730559110641\n",
            "\tЭпоха 5. Итерация 2100/3387. Loss: 0.128114253282547\n",
            "\tЭпоха 5. Итерация 2150/3387. Loss: 0.08932490646839142\n",
            "\tЭпоха 5. Итерация 2200/3387. Loss: 0.10122840851545334\n",
            "\tЭпоха 5. Итерация 2250/3387. Loss: 0.11302843689918518\n",
            "\tЭпоха 5. Итерация 2300/3387. Loss: 0.06740493327379227\n",
            "\tЭпоха 5. Итерация 2350/3387. Loss: 0.0722433403134346\n",
            "\tЭпоха 5. Итерация 2400/3387. Loss: 0.08407176285982132\n",
            "\tЭпоха 5. Итерация 2450/3387. Loss: 0.07897794991731644\n",
            "\tЭпоха 5. Итерация 2500/3387. Loss: 0.07725340873003006\n",
            "\tЭпоха 5. Итерация 2550/3387. Loss: 0.0881546139717102\n",
            "\tЭпоха 5. Итерация 2600/3387. Loss: 0.06882338225841522\n",
            "\tЭпоха 5. Итерация 2650/3387. Loss: 0.07352511584758759\n",
            "\tЭпоха 5. Итерация 2700/3387. Loss: 0.08827140182256699\n",
            "\tЭпоха 5. Итерация 2750/3387. Loss: 0.06403612345457077\n",
            "\tЭпоха 5. Итерация 2800/3387. Loss: 0.07848846167325974\n",
            "\tЭпоха 5. Итерация 2850/3387. Loss: 0.07722825556993484\n",
            "\tЭпоха 5. Итерация 2900/3387. Loss: 0.0716768130660057\n",
            "\tЭпоха 5. Итерация 2950/3387. Loss: 0.0660458654165268\n",
            "\tЭпоха 5. Итерация 3000/3387. Loss: 0.0856732577085495\n",
            "\tЭпоха 5. Итерация 3050/3387. Loss: 0.08191581815481186\n",
            "\tЭпоха 5. Итерация 3100/3387. Loss: 0.08604485541582108\n",
            "\tЭпоха 5. Итерация 3150/3387. Loss: 0.08111005276441574\n",
            "\tЭпоха 5. Итерация 3200/3387. Loss: 0.08734871447086334\n",
            "\tЭпоха 5. Итерация 3250/3387. Loss: 0.0663626492023468\n",
            "\tЭпоха 5. Итерация 3300/3387. Loss: 0.08402350544929504\n",
            "\tЭпоха 5. Итерация 3350/3387. Loss: 0.07374858856201172\n",
            "Эпоха #5 train_loss: 0.0055833846000517134, val_loss: 0.005666033986210823\n",
            "Потрачено 77.7 минут на 5 эпоху\n",
            "\tЭпоха 6. Итерация 0/3387. Loss: 0.08567621558904648\n",
            "\tЭпоха 6. Итерация 50/3387. Loss: 0.12389116734266281\n",
            "\tЭпоха 6. Итерация 100/3387. Loss: 0.07015643268823624\n",
            "\tЭпоха 6. Итерация 150/3387. Loss: 0.0683545395731926\n",
            "\tЭпоха 6. Итерация 200/3387. Loss: 0.11463218927383423\n",
            "\tЭпоха 6. Итерация 250/3387. Loss: 0.09415289014577866\n",
            "\tЭпоха 6. Итерация 300/3387. Loss: 0.061202846467494965\n",
            "\tЭпоха 6. Итерация 350/3387. Loss: 0.11042959243059158\n",
            "\tЭпоха 6. Итерация 400/3387. Loss: 0.06912131607532501\n",
            "\tЭпоха 6. Итерация 450/3387. Loss: 0.09164624661207199\n",
            "\tЭпоха 6. Итерация 500/3387. Loss: 0.06414133310317993\n",
            "\tЭпоха 6. Итерация 550/3387. Loss: 0.08593379706144333\n",
            "\tЭпоха 6. Итерация 600/3387. Loss: 0.07069854438304901\n",
            "\tЭпоха 6. Итерация 650/3387. Loss: 0.07146618515253067\n",
            "\tЭпоха 6. Итерация 700/3387. Loss: 0.10772620141506195\n",
            "\tЭпоха 6. Итерация 750/3387. Loss: 0.07442203909158707\n",
            "\tЭпоха 6. Итерация 800/3387. Loss: 0.08928506076335907\n",
            "\tЭпоха 6. Итерация 850/3387. Loss: 0.08464695513248444\n",
            "\tЭпоха 6. Итерация 900/3387. Loss: 0.061752546578645706\n",
            "\tЭпоха 6. Итерация 950/3387. Loss: 0.06144189089536667\n",
            "\tЭпоха 6. Итерация 1000/3387. Loss: 0.07742565870285034\n",
            "\tЭпоха 6. Итерация 1050/3387. Loss: 0.10106947273015976\n",
            "\tЭпоха 6. Итерация 1100/3387. Loss: 0.09120506048202515\n",
            "\tЭпоха 6. Итерация 1150/3387. Loss: 0.08107585459947586\n",
            "\tЭпоха 6. Итерация 1200/3387. Loss: 0.09573089331388474\n",
            "\tЭпоха 6. Итерация 1250/3387. Loss: 0.0713609978556633\n",
            "\tЭпоха 6. Итерация 1300/3387. Loss: 0.09443525969982147\n",
            "\tЭпоха 6. Итерация 1350/3387. Loss: 0.0969569981098175\n",
            "\tЭпоха 6. Итерация 1400/3387. Loss: 0.08642382174730301\n",
            "\tЭпоха 6. Итерация 1450/3387. Loss: 0.06360834836959839\n",
            "\tЭпоха 6. Итерация 1500/3387. Loss: 0.08332508057355881\n",
            "\tЭпоха 6. Итерация 1550/3387. Loss: 0.09656171500682831\n",
            "\tЭпоха 6. Итерация 1600/3387. Loss: 0.06956389546394348\n",
            "\tЭпоха 6. Итерация 1650/3387. Loss: 0.08240890502929688\n",
            "\tЭпоха 6. Итерация 1700/3387. Loss: 0.07319962978363037\n",
            "\tЭпоха 6. Итерация 1750/3387. Loss: 0.0882166177034378\n",
            "\tЭпоха 6. Итерация 1800/3387. Loss: 0.0839359387755394\n",
            "\tЭпоха 6. Итерация 1850/3387. Loss: 0.07940192520618439\n",
            "\tЭпоха 6. Итерация 1900/3387. Loss: 0.10986340045928955\n",
            "\tЭпоха 6. Итерация 1950/3387. Loss: 0.08961669355630875\n",
            "\tЭпоха 6. Итерация 2000/3387. Loss: 0.09901341050863266\n",
            "\tЭпоха 6. Итерация 2050/3387. Loss: 0.08634732663631439\n",
            "\tЭпоха 6. Итерация 2100/3387. Loss: 0.07752593606710434\n",
            "\tЭпоха 6. Итерация 2150/3387. Loss: 0.08293741196393967\n",
            "\tЭпоха 6. Итерация 2200/3387. Loss: 0.0791519358754158\n",
            "\tЭпоха 6. Итерация 2250/3387. Loss: 0.08414807915687561\n",
            "\tЭпоха 6. Итерация 2300/3387. Loss: 0.08542382717132568\n",
            "\tЭпоха 6. Итерация 2350/3387. Loss: 0.13067369163036346\n",
            "\tЭпоха 6. Итерация 2400/3387. Loss: 0.0845036655664444\n",
            "\tЭпоха 6. Итерация 2450/3387. Loss: 0.09531064331531525\n",
            "\tЭпоха 6. Итерация 2500/3387. Loss: 0.08111739158630371\n",
            "\tЭпоха 6. Итерация 2550/3387. Loss: 0.10571761429309845\n",
            "\tЭпоха 6. Итерация 2600/3387. Loss: 0.07819857448339462\n",
            "\tЭпоха 6. Итерация 2650/3387. Loss: 0.09982295334339142\n",
            "\tЭпоха 6. Итерация 2700/3387. Loss: 0.06797226518392563\n",
            "\tЭпоха 6. Итерация 2750/3387. Loss: 0.08710420876741409\n",
            "\tЭпоха 6. Итерация 2800/3387. Loss: 0.07239101082086563\n",
            "\tЭпоха 6. Итерация 2850/3387. Loss: 0.07607673108577728\n",
            "\tЭпоха 6. Итерация 2900/3387. Loss: 0.06917265057563782\n",
            "\tЭпоха 6. Итерация 2950/3387. Loss: 0.09236875176429749\n",
            "\tЭпоха 6. Итерация 3000/3387. Loss: 0.0727715715765953\n",
            "\tЭпоха 6. Итерация 3050/3387. Loss: 0.1012926772236824\n",
            "\tЭпоха 6. Итерация 3100/3387. Loss: 0.09609273821115494\n",
            "\tЭпоха 6. Итерация 3150/3387. Loss: 0.0865151584148407\n",
            "\tЭпоха 6. Итерация 3200/3387. Loss: 0.06690851598978043\n",
            "\tЭпоха 6. Итерация 3250/3387. Loss: 0.12324976176023483\n",
            "\tЭпоха 6. Итерация 3300/3387. Loss: 0.06474371999502182\n",
            "\tЭпоха 6. Итерация 3350/3387. Loss: 0.0651879757642746\n",
            "Эпоха #6 train_loss: 0.005629228140847806, val_loss: 0.0051099161364138125\n",
            "Потрачено 76.5 минут на 6 эпоху\n",
            "\tЭпоха 7. Итерация 0/3387. Loss: 0.09844015538692474\n",
            "\tЭпоха 7. Итерация 50/3387. Loss: 0.07925985753536224\n",
            "\tЭпоха 7. Итерация 100/3387. Loss: 0.09219378232955933\n",
            "\tЭпоха 7. Итерация 150/3387. Loss: 0.1259264349937439\n",
            "\tЭпоха 7. Итерация 200/3387. Loss: 0.06174000725150108\n",
            "\tЭпоха 7. Итерация 250/3387. Loss: 0.09852565824985504\n",
            "\tЭпоха 7. Итерация 300/3387. Loss: 0.06544654816389084\n",
            "\tЭпоха 7. Итерация 350/3387. Loss: 0.09530916064977646\n",
            "\tЭпоха 7. Итерация 400/3387. Loss: 0.09381235390901566\n",
            "\tЭпоха 7. Итерация 450/3387. Loss: 0.081890769302845\n",
            "\tЭпоха 7. Итерация 500/3387. Loss: 0.10705902427434921\n",
            "\tЭпоха 7. Итерация 550/3387. Loss: 0.07691280543804169\n",
            "\tЭпоха 7. Итерация 600/3387. Loss: 0.060635048896074295\n",
            "\tЭпоха 7. Итерация 650/3387. Loss: 0.0852212905883789\n",
            "\tЭпоха 7. Итерация 700/3387. Loss: 0.08801119774580002\n",
            "\tЭпоха 7. Итерация 750/3387. Loss: 0.07398276031017303\n",
            "\tЭпоха 7. Итерация 800/3387. Loss: 0.10727456212043762\n",
            "\tЭпоха 7. Итерация 850/3387. Loss: 0.1101236641407013\n",
            "\tЭпоха 7. Итерация 900/3387. Loss: 0.1207384243607521\n",
            "\tЭпоха 7. Итерация 950/3387. Loss: 0.07752969115972519\n",
            "\tЭпоха 7. Итерация 1000/3387. Loss: 0.06361829489469528\n",
            "\tЭпоха 7. Итерация 1050/3387. Loss: 0.17871977388858795\n",
            "\tЭпоха 7. Итерация 1100/3387. Loss: 0.07063279300928116\n",
            "\tЭпоха 7. Итерация 1150/3387. Loss: 0.07743637263774872\n",
            "\tЭпоха 7. Итерация 1200/3387. Loss: 0.09267380833625793\n",
            "\tЭпоха 7. Итерация 1250/3387. Loss: 0.06952805072069168\n",
            "\tЭпоха 7. Итерация 1300/3387. Loss: 0.13022983074188232\n",
            "\tЭпоха 7. Итерация 1350/3387. Loss: 0.09407052397727966\n",
            "\tЭпоха 7. Итерация 1400/3387. Loss: 0.07649007439613342\n",
            "\tЭпоха 7. Итерация 1450/3387. Loss: 0.09874098747968674\n",
            "\tЭпоха 7. Итерация 1500/3387. Loss: 0.08737053722143173\n",
            "\tЭпоха 7. Итерация 1550/3387. Loss: 0.07311443239450455\n",
            "\tЭпоха 7. Итерация 1600/3387. Loss: 0.09881963580846786\n",
            "\tЭпоха 7. Итерация 1650/3387. Loss: 0.07810791581869125\n",
            "\tЭпоха 7. Итерация 1700/3387. Loss: 0.06885582953691483\n",
            "\tЭпоха 7. Итерация 1750/3387. Loss: 0.05761090666055679\n",
            "\tЭпоха 7. Итерация 1800/3387. Loss: 0.08999812602996826\n",
            "\tЭпоха 7. Итерация 1850/3387. Loss: 0.07258766889572144\n",
            "\tЭпоха 7. Итерация 1900/3387. Loss: 0.061876434832811356\n",
            "\tЭпоха 7. Итерация 1950/3387. Loss: 0.09285856783390045\n",
            "\tЭпоха 7. Итерация 2000/3387. Loss: 0.09522926807403564\n",
            "\tЭпоха 7. Итерация 2050/3387. Loss: 0.07785583287477493\n",
            "\tЭпоха 7. Итерация 2100/3387. Loss: 0.08957423269748688\n",
            "\tЭпоха 7. Итерация 2150/3387. Loss: 0.08262187242507935\n",
            "\tЭпоха 7. Итерация 2200/3387. Loss: 0.09134503453969955\n",
            "\tЭпоха 7. Итерация 2250/3387. Loss: 0.11807257682085037\n",
            "\tЭпоха 7. Итерация 2300/3387. Loss: 0.1337023675441742\n",
            "\tЭпоха 7. Итерация 2350/3387. Loss: 0.10483106970787048\n",
            "\tЭпоха 7. Итерация 2400/3387. Loss: 0.10585936903953552\n",
            "\tЭпоха 7. Итерация 2450/3387. Loss: 0.09723924845457077\n",
            "\tЭпоха 7. Итерация 2500/3387. Loss: 0.1218569278717041\n",
            "\tЭпоха 7. Итерация 2550/3387. Loss: 0.11955182999372482\n",
            "\tЭпоха 7. Итерация 2600/3387. Loss: 0.08451776951551437\n",
            "\tЭпоха 7. Итерация 2650/3387. Loss: 0.10231174528598785\n",
            "\tЭпоха 7. Итерация 2700/3387. Loss: 0.07867006957530975\n",
            "\tЭпоха 7. Итерация 2750/3387. Loss: 0.09719308465719223\n",
            "\tЭпоха 7. Итерация 2800/3387. Loss: 0.08802147209644318\n",
            "\tЭпоха 7. Итерация 2850/3387. Loss: 0.09597960859537125\n",
            "\tЭпоха 7. Итерация 2900/3387. Loss: 0.08316827565431595\n",
            "\tЭпоха 7. Итерация 2950/3387. Loss: 0.08619822561740875\n",
            "\tЭпоха 7. Итерация 3000/3387. Loss: 0.08860936760902405\n",
            "\tЭпоха 7. Итерация 3050/3387. Loss: 0.09869617968797684\n",
            "\tЭпоха 7. Итерация 3100/3387. Loss: 0.07909087836742401\n",
            "\tЭпоха 7. Итерация 3150/3387. Loss: 0.1348314732313156\n",
            "\tЭпоха 7. Итерация 3200/3387. Loss: 0.11446673423051834\n",
            "\tЭпоха 7. Итерация 3250/3387. Loss: 0.11177729070186615\n",
            "\tЭпоха 7. Итерация 3300/3387. Loss: 0.08022172749042511\n",
            "\tЭпоха 7. Итерация 3350/3387. Loss: 0.08702471852302551\n",
            "Эпоха #7 train_loss: 0.005547962800513081, val_loss: 0.005101670296490193\n",
            "Потрачено 78.3 минут на 7 эпоху\n",
            "\tЭпоха 8. Итерация 0/3387. Loss: 0.08133617788553238\n",
            "\tЭпоха 8. Итерация 50/3387. Loss: 0.0848928764462471\n",
            "\tЭпоха 8. Итерация 100/3387. Loss: 0.08345523476600647\n",
            "\tЭпоха 8. Итерация 150/3387. Loss: 0.09754069149494171\n",
            "\tЭпоха 8. Итерация 200/3387. Loss: 0.06776534020900726\n",
            "\tЭпоха 8. Итерация 250/3387. Loss: 0.07472396641969681\n",
            "\tЭпоха 8. Итерация 300/3387. Loss: 0.1090570017695427\n",
            "\tЭпоха 8. Итерация 350/3387. Loss: 0.11200587451457977\n",
            "\tЭпоха 8. Итерация 400/3387. Loss: 0.1005387231707573\n",
            "\tЭпоха 8. Итерация 450/3387. Loss: 0.07031060010194778\n",
            "\tЭпоха 8. Итерация 500/3387. Loss: 0.0901143029332161\n",
            "\tЭпоха 8. Итерация 550/3387. Loss: 0.09763544052839279\n",
            "\tЭпоха 8. Итерация 600/3387. Loss: 0.09191284328699112\n",
            "\tЭпоха 8. Итерация 650/3387. Loss: 0.10593290627002716\n",
            "\tЭпоха 8. Итерация 700/3387. Loss: 0.08102810382843018\n",
            "\tЭпоха 8. Итерация 750/3387. Loss: 0.1016358882188797\n",
            "\tЭпоха 8. Итерация 800/3387. Loss: 0.06611912697553635\n",
            "\tЭпоха 8. Итерация 850/3387. Loss: 0.07267773896455765\n",
            "\tЭпоха 8. Итерация 900/3387. Loss: 0.10687766224145889\n",
            "\tЭпоха 8. Итерация 950/3387. Loss: 0.09033676981925964\n",
            "\tЭпоха 8. Итерация 1000/3387. Loss: 0.0833202600479126\n",
            "\tЭпоха 8. Итерация 1050/3387. Loss: 0.12519477307796478\n",
            "\tЭпоха 8. Итерация 1100/3387. Loss: 0.08760024607181549\n",
            "\tЭпоха 8. Итерация 1150/3387. Loss: 0.08643431216478348\n",
            "\tЭпоха 8. Итерация 1200/3387. Loss: 0.07703791558742523\n",
            "\tЭпоха 8. Итерация 1250/3387. Loss: 0.08265695720911026\n",
            "\tЭпоха 8. Итерация 1300/3387. Loss: 0.09290611743927002\n",
            "\tЭпоха 8. Итерация 1350/3387. Loss: 0.09654511511325836\n",
            "\tЭпоха 8. Итерация 1400/3387. Loss: 0.11182679235935211\n",
            "\tЭпоха 8. Итерация 1450/3387. Loss: 0.10720915347337723\n",
            "\tЭпоха 8. Итерация 1500/3387. Loss: 0.07609868049621582\n",
            "\tЭпоха 8. Итерация 1550/3387. Loss: 0.11074618250131607\n",
            "\tЭпоха 8. Итерация 1600/3387. Loss: 0.07533090561628342\n",
            "\tЭпоха 8. Итерация 1650/3387. Loss: 0.0773683413863182\n",
            "\tЭпоха 8. Итерация 1700/3387. Loss: 0.07260306179523468\n",
            "\tЭпоха 8. Итерация 1750/3387. Loss: 0.10098008811473846\n",
            "\tЭпоха 8. Итерация 1800/3387. Loss: 0.07652341574430466\n",
            "\tЭпоха 8. Итерация 1850/3387. Loss: 0.10016510635614395\n",
            "\tЭпоха 8. Итерация 1900/3387. Loss: 0.09071463346481323\n",
            "\tЭпоха 8. Итерация 1950/3387. Loss: 0.12720254063606262\n",
            "\tЭпоха 8. Итерация 2000/3387. Loss: 0.07432261854410172\n",
            "\tЭпоха 8. Итерация 2050/3387. Loss: 0.0850788950920105\n",
            "\tЭпоха 8. Итерация 2100/3387. Loss: 0.07818034291267395\n",
            "\tЭпоха 8. Итерация 2150/3387. Loss: 0.1019202470779419\n",
            "\tЭпоха 8. Итерация 2200/3387. Loss: 0.07337435334920883\n",
            "\tЭпоха 8. Итерация 2250/3387. Loss: 0.11742212623357773\n",
            "\tЭпоха 8. Итерация 2300/3387. Loss: 0.10914626717567444\n",
            "\tЭпоха 8. Итерация 2350/3387. Loss: 0.09379075467586517\n",
            "\tЭпоха 8. Итерация 2400/3387. Loss: 0.11938084661960602\n",
            "\tЭпоха 8. Итерация 2450/3387. Loss: 0.09062882512807846\n",
            "\tЭпоха 8. Итерация 2500/3387. Loss: 0.07095489650964737\n",
            "\tЭпоха 8. Итерация 2550/3387. Loss: 0.0949590653181076\n",
            "\tЭпоха 8. Итерация 2600/3387. Loss: 0.11318124830722809\n",
            "\tЭпоха 8. Итерация 2650/3387. Loss: 0.08657974749803543\n",
            "\tЭпоха 8. Итерация 2700/3387. Loss: 0.11092869937419891\n",
            "\tЭпоха 8. Итерация 2750/3387. Loss: 0.10387169569730759\n",
            "\tЭпоха 8. Итерация 2800/3387. Loss: 0.11119237542152405\n",
            "\tЭпоха 8. Итерация 2850/3387. Loss: 0.11230441927909851\n",
            "\tЭпоха 8. Итерация 2900/3387. Loss: 0.09819179773330688\n",
            "\tЭпоха 8. Итерация 2950/3387. Loss: 0.07364877313375473\n",
            "\tЭпоха 8. Итерация 3000/3387. Loss: 0.10385376214981079\n",
            "\tЭпоха 8. Итерация 3050/3387. Loss: 0.06920980662107468\n",
            "\tЭпоха 8. Итерация 3100/3387. Loss: 0.09277177602052689\n",
            "\tЭпоха 8. Итерация 3150/3387. Loss: 0.10240308940410614\n",
            "\tЭпоха 8. Итерация 3200/3387. Loss: 0.0835493728518486\n",
            "\tЭпоха 8. Итерация 3250/3387. Loss: 0.07426190376281738\n",
            "\tЭпоха 8. Итерация 3300/3387. Loss: 0.06854217499494553\n",
            "\tЭпоха 8. Итерация 3350/3387. Loss: 0.08464068919420242\n",
            "Эпоха #8 train_loss: 0.00553514801933476, val_loss: 0.0051783937379717825\n",
            "Потрачено 77.5 минут на 8 эпоху\n",
            "\tЭпоха 9. Итерация 0/3387. Loss: 0.09199930727481842\n",
            "\tЭпоха 9. Итерация 50/3387. Loss: 0.09228729456663132\n",
            "\tЭпоха 9. Итерация 100/3387. Loss: 0.08712314069271088\n",
            "\tЭпоха 9. Итерация 150/3387. Loss: 0.08155935257673264\n",
            "\tЭпоха 9. Итерация 200/3387. Loss: 0.0642663761973381\n",
            "\tЭпоха 9. Итерация 250/3387. Loss: 0.09663046896457672\n",
            "\tЭпоха 9. Итерация 300/3387. Loss: 0.08410938084125519\n",
            "\tЭпоха 9. Итерация 350/3387. Loss: 0.07877297699451447\n",
            "\tЭпоха 9. Итерация 400/3387. Loss: 0.09594756364822388\n",
            "\tЭпоха 9. Итерация 450/3387. Loss: 0.06797294318675995\n",
            "\tЭпоха 9. Итерация 500/3387. Loss: 0.09131429344415665\n",
            "\tЭпоха 9. Итерация 550/3387. Loss: 0.06810283660888672\n",
            "\tЭпоха 9. Итерация 600/3387. Loss: 0.10819736868143082\n",
            "\tЭпоха 9. Итерация 650/3387. Loss: 0.07501061260700226\n",
            "\tЭпоха 9. Итерация 700/3387. Loss: 0.09427408874034882\n",
            "\tЭпоха 9. Итерация 750/3387. Loss: 0.06825724244117737\n",
            "\tЭпоха 9. Итерация 800/3387. Loss: 0.07218876481056213\n",
            "\tЭпоха 9. Итерация 850/3387. Loss: 0.08757037669420242\n",
            "\tЭпоха 9. Итерация 900/3387. Loss: 0.0704374611377716\n",
            "\tЭпоха 9. Итерация 950/3387. Loss: 0.06908915191888809\n",
            "\tЭпоха 9. Итерация 1000/3387. Loss: 0.06965930759906769\n",
            "\tЭпоха 9. Итерация 1050/3387. Loss: 0.06940215080976486\n",
            "\tЭпоха 9. Итерация 1100/3387. Loss: 0.08450539410114288\n",
            "\tЭпоха 9. Итерация 1150/3387. Loss: 0.09549173712730408\n",
            "\tЭпоха 9. Итерация 1200/3387. Loss: 0.07224994897842407\n",
            "\tЭпоха 9. Итерация 1250/3387. Loss: 0.055508315563201904\n",
            "\tЭпоха 9. Итерация 1300/3387. Loss: 0.08898742496967316\n",
            "\tЭпоха 9. Итерация 1350/3387. Loss: 0.05618787929415703\n",
            "\tЭпоха 9. Итерация 1400/3387. Loss: 0.068338543176651\n",
            "\tЭпоха 9. Итерация 1450/3387. Loss: 0.09358765929937363\n",
            "\tЭпоха 9. Итерация 1500/3387. Loss: 0.06952404230833054\n",
            "\tЭпоха 9. Итерация 1550/3387. Loss: 0.09347213804721832\n",
            "\tЭпоха 9. Итерация 1600/3387. Loss: 0.07136834412813187\n",
            "\tЭпоха 9. Итерация 1650/3387. Loss: 0.07647575438022614\n",
            "\tЭпоха 9. Итерация 1700/3387. Loss: 0.07017739117145538\n",
            "\tЭпоха 9. Итерация 1750/3387. Loss: 0.07162094861268997\n",
            "\tЭпоха 9. Итерация 1800/3387. Loss: 0.06462513655424118\n",
            "\tЭпоха 9. Итерация 1850/3387. Loss: 0.07077568769454956\n",
            "\tЭпоха 9. Итерация 1900/3387. Loss: 0.11966125667095184\n",
            "\tЭпоха 9. Итерация 1950/3387. Loss: 0.07059696316719055\n",
            "\tЭпоха 9. Итерация 2000/3387. Loss: 0.06352775543928146\n",
            "\tЭпоха 9. Итерация 2050/3387. Loss: 0.12054029107093811\n",
            "\tЭпоха 9. Итерация 2100/3387. Loss: 0.09983394294977188\n",
            "\tЭпоха 9. Итерация 2150/3387. Loss: 0.0814257562160492\n",
            "\tЭпоха 9. Итерация 2200/3387. Loss: 0.08067748695611954\n",
            "\tЭпоха 9. Итерация 2250/3387. Loss: 0.08222812414169312\n",
            "\tЭпоха 9. Итерация 2300/3387. Loss: 0.09961771965026855\n",
            "\tЭпоха 9. Итерация 2350/3387. Loss: 0.06904834508895874\n",
            "\tЭпоха 9. Итерация 2400/3387. Loss: 0.0685984268784523\n",
            "\tЭпоха 9. Итерация 2450/3387. Loss: 0.0958177000284195\n",
            "\tЭпоха 9. Итерация 2500/3387. Loss: 0.08879096060991287\n",
            "\tЭпоха 9. Итерация 2550/3387. Loss: 0.08143143355846405\n",
            "\tЭпоха 9. Итерация 2600/3387. Loss: 0.08976373821496964\n",
            "\tЭпоха 9. Итерация 2650/3387. Loss: 0.08961936086416245\n",
            "\tЭпоха 9. Итерация 2700/3387. Loss: 0.08130911737680435\n",
            "\tЭпоха 9. Итерация 2750/3387. Loss: 0.06520004570484161\n",
            "\tЭпоха 9. Итерация 2800/3387. Loss: 0.11921392381191254\n",
            "\tЭпоха 9. Итерация 2850/3387. Loss: 0.08132841438055038\n",
            "\tЭпоха 9. Итерация 2900/3387. Loss: 0.06266257911920547\n",
            "\tЭпоха 9. Итерация 2950/3387. Loss: 0.0874943733215332\n",
            "\tЭпоха 9. Итерация 3000/3387. Loss: 0.07738900184631348\n",
            "\tЭпоха 9. Итерация 3050/3387. Loss: 0.06370855867862701\n",
            "\tЭпоха 9. Итерация 3100/3387. Loss: 0.06914035230875015\n",
            "\tЭпоха 9. Итерация 3150/3387. Loss: 0.09203172475099564\n",
            "\tЭпоха 9. Итерация 3200/3387. Loss: 0.08349590748548508\n",
            "\tЭпоха 9. Итерация 3250/3387. Loss: 0.09178123623132706\n",
            "\tЭпоха 9. Итерация 3300/3387. Loss: 0.08104987442493439\n",
            "\tЭпоха 9. Итерация 3350/3387. Loss: 0.11209098249673843\n",
            "Эпоха #9 train_loss: 0.005502277377899647, val_loss: 0.005250982386618853\n",
            "Потрачено 77.2 минут на 9 эпоху\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    for epoch in range(last_epoch + 1, n_epochs):\n",
        "        start = time.time()\n",
        "        train_loss = train(train_data_loader, epoch)\n",
        "        val_loss = val(val_data_loader, epoch)\n",
        "        #lr_scheduler.step()\n",
        "        print(f\"Эпоха #{epoch} train_loss: {train_loss}, val_loss: {val_loss}\")  \n",
        "        end = time.time()\n",
        "        print(f\"Потрачено {round((end - start) / 60, 1)} минут на {epoch} эпоху\")\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
        "            'losses_train': train_losses,\n",
        "            'losses_val': val_losses\n",
        "            }, os.path.join(checkpoints_path, f'chkpt_{model_name}_{epoch}.pth'))\n",
        "    \n",
        "        torch.save(model, os.path.join(checkpoints_path, f'model_{model_name}_{epoch}.pth'))\n",
        "    \n",
        "except KeyboardInterrupt:\n",
        "    print('Прервано пользователем')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "M584PZv6tHIe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5cjIjiCtHIe",
        "outputId": "d53ebc6c-5b52-4f52-d764-5a971c93f463"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'../checkpoints'"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checkpoints_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tORE5beOtHIe"
      },
      "outputs": [],
      "source": [
        "torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
        "            'losses_train': train_losses,\n",
        "            'losses_val': val_losses\n",
        "            }, os.path.join(checkpoints_path, f'model_detector_resnet50_augmented_{epoch}.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cBAujDitHIf"
      },
      "outputs": [],
      "source": [
        "checkpoint2 = torch.load(os.path.join('checkpoints', f'model_detector_resnet50_augmented_2.pth'), map_location=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8aDrH5KtHIf"
      },
      "outputs": [],
      "source": [
        "checkpoint3 = torch.load(os.path.join('checkpoints', f'model_detector_resnet50_augmented_3.pth'), map_location=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEN2BXnStHIf",
        "outputId": "c266908e-04d3-492f-ad1a-ecd1639f5686"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.006822872471820277, 0.00596496530648258, 0.005799468892012618]"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checkpoint2['losses_train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ISihzJ_tHIf",
        "outputId": "b9ec9ea7-7482-40b0-81a1-fb85d4997a10"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.006822872471820277,\n",
              " 0.00596496530648258,\n",
              " 0.005799468892012618,\n",
              " 0.011846726517383244]"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checkpoint3['losses_train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJDKIgmPtHIf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8cjVoDX8HWt",
        "outputId": "87932dc9-f2a3-4376-c369-fc1397a65293"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([0.014400107387186303], [0.010745501028001309])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_losses, val_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24EVi0gp8HWt"
      },
      "outputs": [],
      "source": [
        "'''torch.save({\n",
        "    'epoch': epoch,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
        "    'losses_train': train_losses,\n",
        "    'losses_val': val_losses\n",
        "    }, os.path.join(dataset_path, f'../checkpoints/model_detector_resnet50_augmented_{epoch}.pth'))'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmcJUIyy8HWt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1pW07TQ8HWt"
      },
      "outputs": [],
      "source": [
        "def create_model(num_classes, pretrained=True):\n",
        "    model =torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights='FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT')\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "be07765f19b946589dd0ed7cc58b5ce8"
          ]
        },
        "id": "2uGdiZ6O8HWu",
        "outputId": "7e3127c3-f904-4cbc-9bde-94c97ed84b62"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_mobilenet_v3_large_fpn-fb6a3cc7.pth\" to C:\\Users\\User/.cache\\torch\\hub\\checkpoints\\fasterrcnn_mobilenet_v3_large_fpn-fb6a3cc7.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be07765f19b946589dd0ed7cc58b5ce8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/74.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = create_model(num_classes=2, pretrained=True).to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# parameters\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "#params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9, weight_decay=0.0005)\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.0005)\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
        "\n",
        "\n",
        "\n",
        "#optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "    # and a learning rate scheduler\n",
        "#lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
        "train_dataset = RTSD_dataset(os.path.join(dataset_path, 'train_anno_bin_class.json'),\n",
        "                                          dataset_path)\n",
        "val_dataset = RTSD_dataset(os.path.join(dataset_path, 'val_anno_bin_class.json'), dataset_path)\n",
        "\n",
        "train_data_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    #num_workers=4,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "val_data_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    #num_workers=4,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIr8FnYO8HWu"
      },
      "outputs": [],
      "source": [
        "def train(train_dataloader, epoch): \n",
        "    len_dataloader = len(train_dataloader)\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for i, data in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        images, targets = data[0], data[1]\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        loss_dict = model(images, targets)\n",
        "        loss = sum(loss for loss in loss_dict.values())\n",
        "        running_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if i % 50 == 0:\n",
        "            print(f\"\\tЭпоха {epoch}. Итерация {i}/{len_dataloader}. Loss: {loss}\")\n",
        "    train_loss = running_loss/len(train_dataloader.dataset)\n",
        "    return train_loss\n",
        "\n",
        "def val(val_dataloader, epoch):\n",
        "    running_loss = 0\n",
        "    for data in val_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        images, targets = data[0], data[1]\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        with torch.no_grad():\n",
        "            loss_dict = model(images, targets)\n",
        "        loss_dict = model(images, targets)\n",
        "        loss = sum(loss for loss in loss_dict.values())\n",
        "        running_loss += loss.item()\n",
        "    val_loss = running_loss/len(val_dataloader.dataset)\n",
        "    return val_loss "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2cNTn4Z8HWu",
        "outputId": "253e1ef9-5822-46f3-cf17-9a5eaea597e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tЭпоха 0. Итерация 0/6774. Loss: 0.8358988165855408\n",
            "\tЭпоха 0. Итерация 50/6774. Loss: 0.09858530014753342\n",
            "\tЭпоха 0. Итерация 100/6774. Loss: 0.2172754555940628\n",
            "\tЭпоха 0. Итерация 150/6774. Loss: 0.40015822649002075\n",
            "\tЭпоха 0. Итерация 200/6774. Loss: 0.13395972549915314\n",
            "\tЭпоха 0. Итерация 250/6774. Loss: 0.28839361667633057\n",
            "\tЭпоха 0. Итерация 300/6774. Loss: 0.2116183489561081\n",
            "\tЭпоха 0. Итерация 350/6774. Loss: 0.2833598256111145\n",
            "\tЭпоха 0. Итерация 400/6774. Loss: 0.2793215811252594\n",
            "\tЭпоха 0. Итерация 450/6774. Loss: 0.309714674949646\n",
            "\tЭпоха 0. Итерация 500/6774. Loss: 0.20718857645988464\n",
            "\tЭпоха 0. Итерация 550/6774. Loss: 0.26163995265960693\n",
            "\tЭпоха 0. Итерация 600/6774. Loss: 0.35022008419036865\n",
            "\tЭпоха 0. Итерация 650/6774. Loss: 0.25102320313453674\n",
            "\tЭпоха 0. Итерация 700/6774. Loss: 0.36677470803260803\n",
            "\tЭпоха 0. Итерация 750/6774. Loss: 0.34893864393234253\n",
            "\tЭпоха 0. Итерация 800/6774. Loss: 0.32514622807502747\n",
            "\tЭпоха 0. Итерация 850/6774. Loss: 0.2878253161907196\n",
            "\tЭпоха 0. Итерация 900/6774. Loss: 0.3401532471179962\n",
            "\tЭпоха 0. Итерация 950/6774. Loss: 0.38222745060920715\n",
            "\tЭпоха 0. Итерация 1000/6774. Loss: 0.18317118287086487\n",
            "\tЭпоха 0. Итерация 1050/6774. Loss: 0.3864421844482422\n",
            "\tЭпоха 0. Итерация 1100/6774. Loss: 0.38182953000068665\n",
            "\tЭпоха 0. Итерация 1150/6774. Loss: 0.32848238945007324\n",
            "\tЭпоха 0. Итерация 1200/6774. Loss: 0.2292160838842392\n",
            "\tЭпоха 0. Итерация 1250/6774. Loss: 0.357188880443573\n",
            "\tЭпоха 0. Итерация 1300/6774. Loss: 0.30188116431236267\n",
            "\tЭпоха 0. Итерация 1350/6774. Loss: 0.2532297372817993\n",
            "\tЭпоха 0. Итерация 1400/6774. Loss: 0.29030346870422363\n",
            "\tЭпоха 0. Итерация 1450/6774. Loss: 0.35013389587402344\n",
            "\tЭпоха 0. Итерация 1500/6774. Loss: 0.36888572573661804\n",
            "\tЭпоха 0. Итерация 1550/6774. Loss: 0.2636238634586334\n",
            "\tЭпоха 0. Итерация 1600/6774. Loss: 0.4155515730381012\n",
            "\tЭпоха 0. Итерация 1650/6774. Loss: 0.4899461269378662\n",
            "\tЭпоха 0. Итерация 1700/6774. Loss: 0.3157951235771179\n",
            "\tЭпоха 0. Итерация 1750/6774. Loss: 0.3650500774383545\n",
            "\tЭпоха 0. Итерация 1800/6774. Loss: 0.27937963604927063\n",
            "\tЭпоха 0. Итерация 1850/6774. Loss: 0.30610644817352295\n",
            "\tЭпоха 0. Итерация 1900/6774. Loss: 0.2988344430923462\n",
            "\tЭпоха 0. Итерация 1950/6774. Loss: 0.28896868228912354\n",
            "\tЭпоха 0. Итерация 2000/6774. Loss: 0.30254513025283813\n",
            "\tЭпоха 0. Итерация 2050/6774. Loss: 0.2099846452474594\n",
            "\tЭпоха 0. Итерация 2100/6774. Loss: 0.42602673172950745\n",
            "\tЭпоха 0. Итерация 2150/6774. Loss: 0.2616986930370331\n",
            "\tЭпоха 0. Итерация 2200/6774. Loss: 0.2928885221481323\n",
            "\tЭпоха 0. Итерация 2250/6774. Loss: 0.2699390649795532\n",
            "\tЭпоха 0. Итерация 2300/6774. Loss: 0.3184402287006378\n",
            "\tЭпоха 0. Итерация 2350/6774. Loss: 0.33588075637817383\n",
            "\tЭпоха 0. Итерация 2400/6774. Loss: 0.30526188015937805\n",
            "\tЭпоха 0. Итерация 2450/6774. Loss: 0.1943347305059433\n",
            "\tЭпоха 0. Итерация 2500/6774. Loss: 0.37360596656799316\n",
            "\tЭпоха 0. Итерация 2550/6774. Loss: 0.35080838203430176\n",
            "\tЭпоха 0. Итерация 2600/6774. Loss: 0.33217939734458923\n",
            "\tЭпоха 0. Итерация 2650/6774. Loss: 0.3111608624458313\n",
            "\tЭпоха 0. Итерация 2700/6774. Loss: 0.3208328187465668\n",
            "\tЭпоха 0. Итерация 2750/6774. Loss: 0.24740852415561676\n",
            "\tЭпоха 0. Итерация 2800/6774. Loss: 0.34518757462501526\n",
            "\tЭпоха 0. Итерация 2850/6774. Loss: 0.2022755891084671\n",
            "\tЭпоха 0. Итерация 2900/6774. Loss: 0.4684189558029175\n",
            "\tЭпоха 0. Итерация 2950/6774. Loss: 0.32487520575523376\n",
            "\tЭпоха 0. Итерация 3000/6774. Loss: 0.24739661812782288\n",
            "\tЭпоха 0. Итерация 3050/6774. Loss: 0.3808477222919464\n",
            "\tЭпоха 0. Итерация 3100/6774. Loss: 0.27301111817359924\n",
            "\tЭпоха 0. Итерация 3150/6774. Loss: 0.3859260380268097\n",
            "\tЭпоха 0. Итерация 3200/6774. Loss: 0.5423765778541565\n",
            "\tЭпоха 0. Итерация 3250/6774. Loss: 0.3773883581161499\n",
            "\tЭпоха 0. Итерация 3300/6774. Loss: 0.41587671637535095\n",
            "\tЭпоха 0. Итерация 3350/6774. Loss: 0.34305375814437866\n",
            "\tЭпоха 0. Итерация 3400/6774. Loss: 0.6083003282546997\n",
            "\tЭпоха 0. Итерация 3450/6774. Loss: 0.3301055133342743\n",
            "\tЭпоха 0. Итерация 3500/6774. Loss: 0.31640180945396423\n",
            "\tЭпоха 0. Итерация 3550/6774. Loss: 0.3548625707626343\n",
            "\tЭпоха 0. Итерация 3600/6774. Loss: 0.3046359419822693\n",
            "\tЭпоха 0. Итерация 3650/6774. Loss: 0.33320552110671997\n",
            "\tЭпоха 0. Итерация 3700/6774. Loss: 0.4526026248931885\n",
            "\tЭпоха 0. Итерация 3750/6774. Loss: 0.34792885184288025\n",
            "\tЭпоха 0. Итерация 3800/6774. Loss: 0.38482552766799927\n",
            "\tЭпоха 0. Итерация 3850/6774. Loss: 0.35100823640823364\n",
            "\tЭпоха 0. Итерация 3900/6774. Loss: 0.23835380375385284\n",
            "\tЭпоха 0. Итерация 3950/6774. Loss: 0.41030141711235046\n",
            "\tЭпоха 0. Итерация 4000/6774. Loss: 0.2948543131351471\n",
            "\tЭпоха 0. Итерация 4050/6774. Loss: 0.2638646960258484\n",
            "\tЭпоха 0. Итерация 4100/6774. Loss: 0.41812703013420105\n",
            "\tЭпоха 0. Итерация 4150/6774. Loss: 0.5085234642028809\n",
            "\tЭпоха 0. Итерация 4200/6774. Loss: 0.2097371220588684\n",
            "\tЭпоха 0. Итерация 4250/6774. Loss: 0.26442912220954895\n",
            "\tЭпоха 0. Итерация 4300/6774. Loss: 0.24154822528362274\n",
            "\tЭпоха 0. Итерация 4350/6774. Loss: 0.4022882878780365\n",
            "\tЭпоха 0. Итерация 4400/6774. Loss: 0.4389779567718506\n",
            "\tЭпоха 0. Итерация 4450/6774. Loss: 0.348263680934906\n",
            "\tЭпоха 0. Итерация 4500/6774. Loss: 0.28931429982185364\n",
            "\tЭпоха 0. Итерация 4550/6774. Loss: 0.3102140426635742\n",
            "\tЭпоха 0. Итерация 4600/6774. Loss: 0.20012907683849335\n",
            "\tЭпоха 0. Итерация 4650/6774. Loss: 0.35621610283851624\n",
            "\tЭпоха 0. Итерация 4700/6774. Loss: 0.291753351688385\n",
            "\tЭпоха 0. Итерация 4750/6774. Loss: 0.24610646069049835\n",
            "\tЭпоха 0. Итерация 4800/6774. Loss: 0.45314761996269226\n",
            "\tЭпоха 0. Итерация 4850/6774. Loss: 0.2888239920139313\n",
            "\tЭпоха 0. Итерация 4900/6774. Loss: 0.2444274127483368\n",
            "\tЭпоха 0. Итерация 4950/6774. Loss: 0.2979982793331146\n",
            "\tЭпоха 0. Итерация 5000/6774. Loss: 0.5813078284263611\n",
            "\tЭпоха 0. Итерация 5050/6774. Loss: 0.3875882923603058\n",
            "\tЭпоха 0. Итерация 5100/6774. Loss: 0.3512566387653351\n",
            "\tЭпоха 0. Итерация 5150/6774. Loss: 0.32033571600914\n",
            "\tЭпоха 0. Итерация 5200/6774. Loss: 0.4603501558303833\n",
            "\tЭпоха 0. Итерация 5250/6774. Loss: 0.43615829944610596\n",
            "\tЭпоха 0. Итерация 5300/6774. Loss: 0.5474340915679932\n",
            "\tЭпоха 0. Итерация 5350/6774. Loss: 0.36110055446624756\n",
            "\tЭпоха 0. Итерация 5400/6774. Loss: 0.31166237592697144\n",
            "\tЭпоха 0. Итерация 5450/6774. Loss: 0.2825218737125397\n",
            "\tЭпоха 0. Итерация 5500/6774. Loss: 0.2806028425693512\n",
            "\tЭпоха 0. Итерация 5550/6774. Loss: 0.31587040424346924\n",
            "\tЭпоха 0. Итерация 5600/6774. Loss: 0.6018278002738953\n",
            "\tЭпоха 0. Итерация 5650/6774. Loss: 0.48541325330734253\n",
            "\tЭпоха 0. Итерация 5700/6774. Loss: 0.1741761565208435\n",
            "\tЭпоха 0. Итерация 5750/6774. Loss: 0.3061968684196472\n",
            "\tЭпоха 0. Итерация 5800/6774. Loss: 0.3103734850883484\n",
            "\tЭпоха 0. Итерация 5850/6774. Loss: 0.46791693568229675\n",
            "\tЭпоха 0. Итерация 5900/6774. Loss: 0.42280831933021545\n",
            "\tЭпоха 0. Итерация 5950/6774. Loss: 0.30640551447868347\n",
            "\tЭпоха 0. Итерация 6000/6774. Loss: 0.42255547642707825\n",
            "\tЭпоха 0. Итерация 6050/6774. Loss: 0.3898535966873169\n",
            "\tЭпоха 0. Итерация 6100/6774. Loss: 0.25518718361854553\n",
            "\tЭпоха 0. Итерация 6150/6774. Loss: 0.3671203553676605\n",
            "\tЭпоха 0. Итерация 6200/6774. Loss: 0.27329227328300476\n",
            "\tЭпоха 0. Итерация 6250/6774. Loss: 0.3945823013782501\n",
            "\tЭпоха 0. Итерация 6300/6774. Loss: 0.38400527834892273\n",
            "\tЭпоха 0. Итерация 6350/6774. Loss: 0.20627932250499725\n",
            "\tЭпоха 0. Итерация 6400/6774. Loss: 0.4845552444458008\n",
            "\tЭпоха 0. Итерация 6450/6774. Loss: 0.3458143174648285\n",
            "\tЭпоха 0. Итерация 6500/6774. Loss: 0.37972334027290344\n",
            "\tЭпоха 0. Итерация 6550/6774. Loss: 0.41420891880989075\n",
            "\tЭпоха 0. Итерация 6600/6774. Loss: 0.34638547897338867\n",
            "\tЭпоха 0. Итерация 6650/6774. Loss: 0.19860349595546722\n",
            "\tЭпоха 0. Итерация 6700/6774. Loss: 0.481199711561203\n",
            "\tЭпоха 0. Итерация 6750/6774. Loss: 0.491936594247818\n",
            "Эпоха #0 train_loss: 0.041939963174074606, val_loss: 0.04971962125450373\n",
            "Потрачено 51.4 минут на 0 эпоху\n",
            "\tЭпоха 1. Итерация 0/6774. Loss: 0.41819116473197937\n",
            "\tЭпоха 1. Итерация 50/6774. Loss: 0.35619866847991943\n",
            "\tЭпоха 1. Итерация 100/6774. Loss: 0.25738027691841125\n",
            "\tЭпоха 1. Итерация 150/6774. Loss: 0.3240741491317749\n",
            "\tЭпоха 1. Итерация 200/6774. Loss: 0.5980733036994934\n",
            "\tЭпоха 1. Итерация 250/6774. Loss: 0.4684480130672455\n",
            "\tЭпоха 1. Итерация 300/6774. Loss: 0.4259324371814728\n",
            "\tЭпоха 1. Итерация 350/6774. Loss: 0.3969480097293854\n",
            "\tЭпоха 1. Итерация 400/6774. Loss: 0.23790697753429413\n",
            "\tЭпоха 1. Итерация 450/6774. Loss: 0.41913461685180664\n",
            "\tЭпоха 1. Итерация 500/6774. Loss: 0.2675626873970032\n",
            "\tЭпоха 1. Итерация 550/6774. Loss: 0.39752694964408875\n",
            "\tЭпоха 1. Итерация 600/6774. Loss: 0.33315804600715637\n",
            "\tЭпоха 1. Итерация 650/6774. Loss: 0.37485313415527344\n",
            "\tЭпоха 1. Итерация 700/6774. Loss: 0.3911009728908539\n",
            "\tЭпоха 1. Итерация 750/6774. Loss: 0.2956126928329468\n",
            "\tЭпоха 1. Итерация 800/6774. Loss: 0.29083970189094543\n",
            "\tЭпоха 1. Итерация 850/6774. Loss: 0.3838546872138977\n",
            "\tЭпоха 1. Итерация 900/6774. Loss: 0.33601146936416626\n",
            "\tЭпоха 1. Итерация 950/6774. Loss: 0.42298623919487\n",
            "\tЭпоха 1. Итерация 1000/6774. Loss: 0.20946303009986877\n",
            "\tЭпоха 1. Итерация 1050/6774. Loss: 0.25266873836517334\n",
            "\tЭпоха 1. Итерация 1100/6774. Loss: 0.36946460604667664\n",
            "\tЭпоха 1. Итерация 1150/6774. Loss: 0.23496907949447632\n",
            "\tЭпоха 1. Итерация 1200/6774. Loss: 0.5798377394676208\n",
            "\tЭпоха 1. Итерация 1250/6774. Loss: 0.36844193935394287\n",
            "\tЭпоха 1. Итерация 1300/6774. Loss: 0.24761724472045898\n",
            "\tЭпоха 1. Итерация 1350/6774. Loss: 0.5298421382904053\n",
            "\tЭпоха 1. Итерация 1400/6774. Loss: 0.4400467276573181\n",
            "\tЭпоха 1. Итерация 1450/6774. Loss: 0.24246622622013092\n",
            "\tЭпоха 1. Итерация 1500/6774. Loss: 0.2236536145210266\n",
            "\tЭпоха 1. Итерация 1550/6774. Loss: 0.6530472040176392\n",
            "\tЭпоха 1. Итерация 1600/6774. Loss: 0.15169815719127655\n",
            "\tЭпоха 1. Итерация 1650/6774. Loss: 0.6787804365158081\n",
            "\tЭпоха 1. Итерация 1700/6774. Loss: 0.4169812798500061\n",
            "\tЭпоха 1. Итерация 1750/6774. Loss: 0.35499730706214905\n",
            "\tЭпоха 1. Итерация 1800/6774. Loss: 0.4295102059841156\n",
            "\tЭпоха 1. Итерация 1850/6774. Loss: 0.30775511264801025\n",
            "\tЭпоха 1. Итерация 1900/6774. Loss: 0.35965874791145325\n",
            "\tЭпоха 1. Итерация 1950/6774. Loss: 0.4489986300468445\n",
            "\tЭпоха 1. Итерация 2000/6774. Loss: 0.43344366550445557\n",
            "\tЭпоха 1. Итерация 2050/6774. Loss: 0.342428594827652\n",
            "\tЭпоха 1. Итерация 2100/6774. Loss: 0.3849497437477112\n",
            "\tЭпоха 1. Итерация 2150/6774. Loss: 0.2826206088066101\n",
            "\tЭпоха 1. Итерация 2200/6774. Loss: 0.3037051558494568\n",
            "\tЭпоха 1. Итерация 2250/6774. Loss: 0.35941824316978455\n",
            "\tЭпоха 1. Итерация 2300/6774. Loss: 0.25653213262557983\n",
            "\tЭпоха 1. Итерация 2350/6774. Loss: 0.24246865510940552\n",
            "\tЭпоха 1. Итерация 2400/6774. Loss: 0.4544875919818878\n",
            "\tЭпоха 1. Итерация 2450/6774. Loss: 0.3290853798389435\n",
            "\tЭпоха 1. Итерация 2500/6774. Loss: 0.3703569769859314\n",
            "\tЭпоха 1. Итерация 2550/6774. Loss: 0.3514910042285919\n",
            "\tЭпоха 1. Итерация 2600/6774. Loss: 0.28617215156555176\n",
            "\tЭпоха 1. Итерация 2650/6774. Loss: 0.235402911901474\n",
            "\tЭпоха 1. Итерация 2700/6774. Loss: 0.2665899693965912\n",
            "\tЭпоха 1. Итерация 2750/6774. Loss: 0.3864673674106598\n",
            "\tЭпоха 1. Итерация 2800/6774. Loss: 0.36783862113952637\n",
            "\tЭпоха 1. Итерация 2850/6774. Loss: 0.3622267246246338\n",
            "\tЭпоха 1. Итерация 2900/6774. Loss: 0.3390768766403198\n",
            "\tЭпоха 1. Итерация 2950/6774. Loss: 0.5326998829841614\n",
            "\tЭпоха 1. Итерация 3000/6774. Loss: 0.2347417175769806\n",
            "\tЭпоха 1. Итерация 3050/6774. Loss: 0.28418806195259094\n",
            "\tЭпоха 1. Итерация 3100/6774. Loss: 0.29809242486953735\n",
            "\tЭпоха 1. Итерация 3150/6774. Loss: 0.3055286705493927\n",
            "\tЭпоха 1. Итерация 3200/6774. Loss: 0.35438743233680725\n",
            "\tЭпоха 1. Итерация 3250/6774. Loss: 0.22692017257213593\n",
            "\tЭпоха 1. Итерация 3300/6774. Loss: 0.27565062046051025\n",
            "\tЭпоха 1. Итерация 3350/6774. Loss: 0.3885766267776489\n",
            "\tЭпоха 1. Итерация 3400/6774. Loss: 0.3618251383304596\n",
            "\tЭпоха 1. Итерация 3450/6774. Loss: 0.4212157726287842\n",
            "\tЭпоха 1. Итерация 3500/6774. Loss: 0.3020871579647064\n",
            "\tЭпоха 1. Итерация 3550/6774. Loss: 0.3398270308971405\n",
            "\tЭпоха 1. Итерация 3600/6774. Loss: 0.4610428810119629\n",
            "\tЭпоха 1. Итерация 3650/6774. Loss: 0.24168196320533752\n",
            "\tЭпоха 1. Итерация 3700/6774. Loss: 0.3224095404148102\n",
            "\tЭпоха 1. Итерация 3750/6774. Loss: 0.4177107512950897\n",
            "\tЭпоха 1. Итерация 3800/6774. Loss: 0.3488139510154724\n",
            "\tЭпоха 1. Итерация 3850/6774. Loss: 0.29283517599105835\n",
            "\tЭпоха 1. Итерация 3900/6774. Loss: 0.6431142687797546\n",
            "\tЭпоха 1. Итерация 3950/6774. Loss: 0.46962928771972656\n",
            "\tЭпоха 1. Итерация 4000/6774. Loss: 0.41944026947021484\n",
            "\tЭпоха 1. Итерация 4050/6774. Loss: 0.47633877396583557\n",
            "\tЭпоха 1. Итерация 4100/6774. Loss: 0.2212451547384262\n",
            "\tЭпоха 1. Итерация 4150/6774. Loss: 0.303644597530365\n",
            "\tЭпоха 1. Итерация 4200/6774. Loss: 0.3483940660953522\n",
            "\tЭпоха 1. Итерация 4250/6774. Loss: 0.32347041368484497\n",
            "\tЭпоха 1. Итерация 4300/6774. Loss: 0.509441614151001\n",
            "\tЭпоха 1. Итерация 4350/6774. Loss: 0.6056998372077942\n",
            "\tЭпоха 1. Итерация 4400/6774. Loss: 0.36725109815597534\n",
            "\tЭпоха 1. Итерация 4450/6774. Loss: 0.5444537997245789\n",
            "\tЭпоха 1. Итерация 4500/6774. Loss: 0.3195989727973938\n",
            "\tЭпоха 1. Итерация 4550/6774. Loss: 0.23488301038742065\n",
            "\tЭпоха 1. Итерация 4600/6774. Loss: 0.37782320380210876\n",
            "\tЭпоха 1. Итерация 4650/6774. Loss: 0.21875934302806854\n",
            "\tЭпоха 1. Итерация 4700/6774. Loss: 0.47045019268989563\n",
            "\tЭпоха 1. Итерация 4750/6774. Loss: 0.34184154868125916\n",
            "\tЭпоха 1. Итерация 4800/6774. Loss: 0.4102199077606201\n",
            "\tЭпоха 1. Итерация 4850/6774. Loss: 0.2346402257680893\n",
            "\tЭпоха 1. Итерация 4900/6774. Loss: 0.4911821484565735\n",
            "\tЭпоха 1. Итерация 4950/6774. Loss: 0.4445916414260864\n",
            "\tЭпоха 1. Итерация 5000/6774. Loss: 0.3917425274848938\n",
            "\tЭпоха 1. Итерация 5050/6774. Loss: 0.4292667508125305\n",
            "\tЭпоха 1. Итерация 5100/6774. Loss: 0.4430619180202484\n",
            "\tЭпоха 1. Итерация 5150/6774. Loss: 0.498257040977478\n",
            "\tЭпоха 1. Итерация 5200/6774. Loss: 0.3104202151298523\n",
            "\tЭпоха 1. Итерация 5250/6774. Loss: 0.5381891131401062\n",
            "\tЭпоха 1. Итерация 5300/6774. Loss: 0.3701520562171936\n",
            "\tЭпоха 1. Итерация 5350/6774. Loss: 0.3301345705986023\n",
            "\tЭпоха 1. Итерация 5400/6774. Loss: 0.363673597574234\n",
            "\tЭпоха 1. Итерация 5450/6774. Loss: 0.41580185294151306\n",
            "\tЭпоха 1. Итерация 5500/6774. Loss: 0.30989667773246765\n",
            "\tЭпоха 1. Итерация 5550/6774. Loss: 0.2724064290523529\n",
            "\tЭпоха 1. Итерация 5600/6774. Loss: 0.5211115479469299\n",
            "\tЭпоха 1. Итерация 5650/6774. Loss: 0.42866191267967224\n",
            "\tЭпоха 1. Итерация 5700/6774. Loss: 0.4435442388057709\n",
            "\tЭпоха 1. Итерация 5750/6774. Loss: 0.44266247749328613\n",
            "\tЭпоха 1. Итерация 5800/6774. Loss: 0.22385050356388092\n",
            "\tЭпоха 1. Итерация 5850/6774. Loss: 0.37382107973098755\n",
            "\tЭпоха 1. Итерация 5900/6774. Loss: 0.40789803862571716\n",
            "\tЭпоха 1. Итерация 5950/6774. Loss: 0.28010573983192444\n",
            "\tЭпоха 1. Итерация 6000/6774. Loss: 0.23171432316303253\n",
            "\tЭпоха 1. Итерация 6050/6774. Loss: 0.27011892199516296\n",
            "\tЭпоха 1. Итерация 6100/6774. Loss: 0.3276524841785431\n",
            "\tЭпоха 1. Итерация 6150/6774. Loss: 0.3281562328338623\n",
            "\tЭпоха 1. Итерация 6200/6774. Loss: 0.41261714696884155\n",
            "\tЭпоха 1. Итерация 6250/6774. Loss: 0.3872956335544586\n",
            "\tЭпоха 1. Итерация 6300/6774. Loss: 0.5138572454452515\n",
            "\tЭпоха 1. Итерация 6350/6774. Loss: 0.26330676674842834\n",
            "\tЭпоха 1. Итерация 6400/6774. Loss: 0.3324149549007416\n",
            "\tЭпоха 1. Итерация 6450/6774. Loss: 0.6742364168167114\n",
            "\tЭпоха 1. Итерация 6500/6774. Loss: 0.23300595581531525\n",
            "\tЭпоха 1. Итерация 6550/6774. Loss: 0.3239038288593292\n",
            "\tЭпоха 1. Итерация 6600/6774. Loss: 0.35350754857063293\n",
            "\tЭпоха 1. Итерация 6650/6774. Loss: 0.2974702715873718\n",
            "\tЭпоха 1. Итерация 6700/6774. Loss: 0.3229328989982605\n",
            "\tЭпоха 1. Итерация 6750/6774. Loss: 0.3328906297683716\n",
            "Эпоха #1 train_loss: 0.04659779232391055, val_loss: 0.054402772963047026\n",
            "Потрачено 48.8 минут на 1 эпоху\n",
            "\tЭпоха 2. Итерация 0/6774. Loss: 0.336588054895401\n",
            "\tЭпоха 2. Итерация 50/6774. Loss: 0.3330010175704956\n",
            "\tЭпоха 2. Итерация 100/6774. Loss: 0.5946797728538513\n",
            "\tЭпоха 2. Итерация 150/6774. Loss: 0.17851068079471588\n",
            "\tЭпоха 2. Итерация 200/6774. Loss: 0.34251269698143005\n",
            "\tЭпоха 2. Итерация 250/6774. Loss: 0.32756441831588745\n",
            "\tЭпоха 2. Итерация 300/6774. Loss: 0.41554316878318787\n",
            "\tЭпоха 2. Итерация 350/6774. Loss: 0.41835808753967285\n",
            "\tЭпоха 2. Итерация 400/6774. Loss: 0.20596261322498322\n",
            "\tЭпоха 2. Итерация 450/6774. Loss: 0.32385000586509705\n",
            "\tЭпоха 2. Итерация 500/6774. Loss: 0.3988465666770935\n",
            "\tЭпоха 2. Итерация 550/6774. Loss: 0.3398951590061188\n",
            "\tЭпоха 2. Итерация 600/6774. Loss: 0.2937109172344208\n",
            "\tЭпоха 2. Итерация 650/6774. Loss: 0.4359780251979828\n",
            "\tЭпоха 2. Итерация 700/6774. Loss: 0.41071727871894836\n",
            "\tЭпоха 2. Итерация 750/6774. Loss: 0.5380074977874756\n",
            "\tЭпоха 2. Итерация 800/6774. Loss: 0.48733973503112793\n",
            "\tЭпоха 2. Итерация 850/6774. Loss: 0.6064988970756531\n",
            "\tЭпоха 2. Итерация 900/6774. Loss: 0.4436124563217163\n",
            "\tЭпоха 2. Итерация 950/6774. Loss: 0.24103766679763794\n",
            "\tЭпоха 2. Итерация 1000/6774. Loss: 0.45375728607177734\n",
            "\tЭпоха 2. Итерация 1050/6774. Loss: 0.6554536819458008\n",
            "\tЭпоха 2. Итерация 1100/6774. Loss: 0.2831757068634033\n",
            "\tЭпоха 2. Итерация 1150/6774. Loss: 0.24177606403827667\n",
            "\tЭпоха 2. Итерация 1200/6774. Loss: 0.37173810601234436\n",
            "\tЭпоха 2. Итерация 1250/6774. Loss: 0.3961319029331207\n",
            "\tЭпоха 2. Итерация 1300/6774. Loss: 0.3727187216281891\n",
            "\tЭпоха 2. Итерация 1350/6774. Loss: 0.16123799979686737\n",
            "\tЭпоха 2. Итерация 1400/6774. Loss: 0.37507233023643494\n",
            "\tЭпоха 2. Итерация 1450/6774. Loss: 0.3982768952846527\n",
            "\tЭпоха 2. Итерация 1500/6774. Loss: 0.4386429488658905\n",
            "\tЭпоха 2. Итерация 1550/6774. Loss: 0.48092517256736755\n",
            "\tЭпоха 2. Итерация 1600/6774. Loss: 0.5660461783409119\n",
            "\tЭпоха 2. Итерация 1650/6774. Loss: 0.3510037958621979\n",
            "\tЭпоха 2. Итерация 1700/6774. Loss: 0.1432289034128189\n",
            "\tЭпоха 2. Итерация 1750/6774. Loss: 0.3895244598388672\n",
            "\tЭпоха 2. Итерация 1800/6774. Loss: 0.2809964120388031\n",
            "\tЭпоха 2. Итерация 1850/6774. Loss: 0.5549435019493103\n",
            "\tЭпоха 2. Итерация 1900/6774. Loss: 0.2604682147502899\n",
            "\tЭпоха 2. Итерация 1950/6774. Loss: 0.5850344896316528\n",
            "\tЭпоха 2. Итерация 2000/6774. Loss: 0.1953851878643036\n",
            "\tЭпоха 2. Итерация 2050/6774. Loss: 0.3426191806793213\n",
            "\tЭпоха 2. Итерация 2100/6774. Loss: 0.43685826659202576\n",
            "\tЭпоха 2. Итерация 2150/6774. Loss: 0.16496910154819489\n",
            "\tЭпоха 2. Итерация 2200/6774. Loss: 0.4110396206378937\n",
            "\tЭпоха 2. Итерация 2250/6774. Loss: 0.5875741243362427\n",
            "\tЭпоха 2. Итерация 2300/6774. Loss: 0.22160044312477112\n",
            "\tЭпоха 2. Итерация 2350/6774. Loss: 0.4129514992237091\n",
            "\tЭпоха 2. Итерация 2400/6774. Loss: 0.3616454303264618\n",
            "\tЭпоха 2. Итерация 2450/6774. Loss: 0.3547167479991913\n",
            "\tЭпоха 2. Итерация 2500/6774. Loss: 0.2614678740501404\n",
            "\tЭпоха 2. Итерация 2550/6774. Loss: 0.4659675061702728\n",
            "\tЭпоха 2. Итерация 2600/6774. Loss: 0.4972081780433655\n",
            "\tЭпоха 2. Итерация 2650/6774. Loss: 0.39271044731140137\n",
            "\tЭпоха 2. Итерация 2700/6774. Loss: 0.31445273756980896\n",
            "\tЭпоха 2. Итерация 2750/6774. Loss: 0.26584190130233765\n",
            "\tЭпоха 2. Итерация 2800/6774. Loss: 0.4953067898750305\n",
            "\tЭпоха 2. Итерация 2850/6774. Loss: 0.42261165380477905\n",
            "\tЭпоха 2. Итерация 2900/6774. Loss: 0.2327653467655182\n",
            "\tЭпоха 2. Итерация 2950/6774. Loss: 0.34685325622558594\n",
            "\tЭпоха 2. Итерация 3000/6774. Loss: 0.31393012404441833\n",
            "\tЭпоха 2. Итерация 3050/6774. Loss: 0.2860867977142334\n",
            "\tЭпоха 2. Итерация 3100/6774. Loss: 0.22650018334388733\n",
            "\tЭпоха 2. Итерация 3150/6774. Loss: 0.36363524198532104\n",
            "\tЭпоха 2. Итерация 3200/6774. Loss: 0.3760872483253479\n",
            "\tЭпоха 2. Итерация 3250/6774. Loss: 0.21308332681655884\n",
            "\tЭпоха 2. Итерация 3300/6774. Loss: 0.30173927545547485\n",
            "\tЭпоха 2. Итерация 3350/6774. Loss: 0.25429007411003113\n",
            "\tЭпоха 2. Итерация 3400/6774. Loss: 0.3555644750595093\n",
            "\tЭпоха 2. Итерация 3450/6774. Loss: 0.540815532207489\n",
            "\tЭпоха 2. Итерация 3500/6774. Loss: 0.21265669167041779\n",
            "\tЭпоха 2. Итерация 3550/6774. Loss: 0.3760349452495575\n",
            "\tЭпоха 2. Итерация 3600/6774. Loss: 0.48704880475997925\n",
            "\tЭпоха 2. Итерация 3650/6774. Loss: 0.4031670391559601\n",
            "\tЭпоха 2. Итерация 3700/6774. Loss: 0.24516525864601135\n",
            "\tЭпоха 2. Итерация 3750/6774. Loss: 0.41909900307655334\n",
            "\tЭпоха 2. Итерация 3800/6774. Loss: 0.4468613266944885\n",
            "\tЭпоха 2. Итерация 3850/6774. Loss: 0.4713948965072632\n",
            "\tЭпоха 2. Итерация 3900/6774. Loss: 0.38938501477241516\n",
            "\tЭпоха 2. Итерация 3950/6774. Loss: 0.39138278365135193\n",
            "\tЭпоха 2. Итерация 4000/6774. Loss: 0.24815011024475098\n",
            "\tЭпоха 2. Итерация 4050/6774. Loss: 0.42394131422042847\n",
            "\tЭпоха 2. Итерация 4100/6774. Loss: 0.35972893238067627\n",
            "\tЭпоха 2. Итерация 4150/6774. Loss: 0.3560599386692047\n",
            "\tЭпоха 2. Итерация 4200/6774. Loss: 0.3321000039577484\n",
            "\tЭпоха 2. Итерация 4250/6774. Loss: 0.31412002444267273\n",
            "\tЭпоха 2. Итерация 4300/6774. Loss: 0.26035434007644653\n",
            "\tЭпоха 2. Итерация 4350/6774. Loss: 0.4388516843318939\n",
            "\tЭпоха 2. Итерация 4400/6774. Loss: 0.4107847213745117\n",
            "\tЭпоха 2. Итерация 4450/6774. Loss: 0.3075743317604065\n",
            "\tЭпоха 2. Итерация 4500/6774. Loss: 0.48476162552833557\n",
            "\tЭпоха 2. Итерация 4550/6774. Loss: 0.38758885860443115\n",
            "\tЭпоха 2. Итерация 4600/6774. Loss: 0.3181326389312744\n",
            "\tЭпоха 2. Итерация 4650/6774. Loss: 0.21759918332099915\n",
            "\tЭпоха 2. Итерация 4700/6774. Loss: 0.2582862079143524\n",
            "\tЭпоха 2. Итерация 4750/6774. Loss: 0.3067563772201538\n",
            "\tЭпоха 2. Итерация 4800/6774. Loss: 0.24123616516590118\n",
            "\tЭпоха 2. Итерация 4850/6774. Loss: 0.2566227614879608\n",
            "\tЭпоха 2. Итерация 4900/6774. Loss: 0.2622562646865845\n",
            "\tЭпоха 2. Итерация 4950/6774. Loss: 0.36167559027671814\n",
            "\tЭпоха 2. Итерация 5000/6774. Loss: 0.27921831607818604\n",
            "\tЭпоха 2. Итерация 5050/6774. Loss: 0.26007336378097534\n",
            "\tЭпоха 2. Итерация 5100/6774. Loss: 0.27689558267593384\n",
            "\tЭпоха 2. Итерация 5150/6774. Loss: 0.3927775025367737\n",
            "\tЭпоха 2. Итерация 5200/6774. Loss: 0.3948075473308563\n",
            "\tЭпоха 2. Итерация 5250/6774. Loss: 0.2905776798725128\n",
            "\tЭпоха 2. Итерация 5300/6774. Loss: 0.2829306721687317\n",
            "\tЭпоха 2. Итерация 5350/6774. Loss: 0.3205646276473999\n",
            "\tЭпоха 2. Итерация 5400/6774. Loss: 0.22966161370277405\n",
            "\tЭпоха 2. Итерация 5450/6774. Loss: 0.2031169980764389\n",
            "\tЭпоха 2. Итерация 5500/6774. Loss: 0.33866503834724426\n",
            "\tЭпоха 2. Итерация 5550/6774. Loss: 0.20330485701560974\n",
            "\tЭпоха 2. Итерация 5600/6774. Loss: 0.3879810869693756\n",
            "\tЭпоха 2. Итерация 5650/6774. Loss: 0.3210204839706421\n",
            "\tЭпоха 2. Итерация 5700/6774. Loss: 0.2465163916349411\n",
            "\tЭпоха 2. Итерация 5750/6774. Loss: 0.4214897155761719\n",
            "\tЭпоха 2. Итерация 5800/6774. Loss: 0.40318062901496887\n",
            "\tЭпоха 2. Итерация 5850/6774. Loss: 0.21728472411632538\n",
            "\tЭпоха 2. Итерация 5900/6774. Loss: 0.2742270529270172\n",
            "\tЭпоха 2. Итерация 5950/6774. Loss: 0.3801407516002655\n",
            "\tЭпоха 2. Итерация 6000/6774. Loss: 0.3961808681488037\n",
            "\tЭпоха 2. Итерация 6050/6774. Loss: 0.2840469181537628\n",
            "\tЭпоха 2. Итерация 6100/6774. Loss: 0.31434494256973267\n",
            "\tЭпоха 2. Итерация 6150/6774. Loss: 0.32145193219184875\n",
            "\tЭпоха 2. Итерация 6200/6774. Loss: 0.34708043932914734\n",
            "\tЭпоха 2. Итерация 6250/6774. Loss: 0.28148433566093445\n",
            "\tЭпоха 2. Итерация 6300/6774. Loss: 0.31410348415374756\n",
            "\tЭпоха 2. Итерация 6350/6774. Loss: 0.2635899782180786\n",
            "\tЭпоха 2. Итерация 6400/6774. Loss: 0.32705065608024597\n",
            "\tЭпоха 2. Итерация 6450/6774. Loss: 0.3229910135269165\n",
            "\tЭпоха 2. Итерация 6500/6774. Loss: 0.32964104413986206\n",
            "\tЭпоха 2. Итерация 6550/6774. Loss: 0.39438924193382263\n",
            "\tЭпоха 2. Итерация 6600/6774. Loss: 0.28204861283302307\n",
            "\tЭпоха 2. Итерация 6650/6774. Loss: 0.2737219035625458\n",
            "\tЭпоха 2. Итерация 6700/6774. Loss: 0.3442721366882324\n",
            "\tЭпоха 2. Итерация 6750/6774. Loss: 0.3330710530281067\n",
            "Эпоха #2 train_loss: 0.046035758322024375, val_loss: 0.056045305943489075\n",
            "Потрачено 49.0 минут на 2 эпоху\n",
            "\tЭпоха 3. Итерация 0/6774. Loss: 0.26013219356536865\n",
            "\tЭпоха 3. Итерация 50/6774. Loss: 0.2921954393386841\n",
            "\tЭпоха 3. Итерация 100/6774. Loss: 0.3363632261753082\n",
            "\tЭпоха 3. Итерация 150/6774. Loss: 0.32091623544692993\n",
            "\tЭпоха 3. Итерация 200/6774. Loss: 0.442366361618042\n",
            "\tЭпоха 3. Итерация 250/6774. Loss: 0.41991063952445984\n",
            "\tЭпоха 3. Итерация 300/6774. Loss: 0.45122766494750977\n",
            "\tЭпоха 3. Итерация 350/6774. Loss: 0.24042728543281555\n",
            "\tЭпоха 3. Итерация 400/6774. Loss: 0.4546917676925659\n",
            "\tЭпоха 3. Итерация 450/6774. Loss: 0.5091292858123779\n",
            "\tЭпоха 3. Итерация 500/6774. Loss: 0.259773313999176\n",
            "\tЭпоха 3. Итерация 550/6774. Loss: 0.33531010150909424\n",
            "\tЭпоха 3. Итерация 600/6774. Loss: 0.5151806473731995\n",
            "\tЭпоха 3. Итерация 650/6774. Loss: 0.2536592483520508\n",
            "\tЭпоха 3. Итерация 700/6774. Loss: 0.3358435034751892\n",
            "\tЭпоха 3. Итерация 750/6774. Loss: 0.35887816548347473\n",
            "\tЭпоха 3. Итерация 800/6774. Loss: 0.3093236982822418\n",
            "\tЭпоха 3. Итерация 850/6774. Loss: 0.4216674268245697\n",
            "\tЭпоха 3. Итерация 900/6774. Loss: 0.3329043388366699\n",
            "\tЭпоха 3. Итерация 950/6774. Loss: 0.23510444164276123\n",
            "\tЭпоха 3. Итерация 1000/6774. Loss: 0.2687258720397949\n",
            "\tЭпоха 3. Итерация 1050/6774. Loss: 0.39611613750457764\n",
            "\tЭпоха 3. Итерация 1100/6774. Loss: 0.259937047958374\n",
            "\tЭпоха 3. Итерация 1150/6774. Loss: 0.4489871561527252\n",
            "\tЭпоха 3. Итерация 1200/6774. Loss: 0.3329002857208252\n",
            "\tЭпоха 3. Итерация 1250/6774. Loss: 0.3072521686553955\n",
            "\tЭпоха 3. Итерация 1300/6774. Loss: 0.3512943983078003\n",
            "\tЭпоха 3. Итерация 1350/6774. Loss: 0.49363771080970764\n",
            "\tЭпоха 3. Итерация 1400/6774. Loss: 0.1956986039876938\n",
            "\tЭпоха 3. Итерация 1450/6774. Loss: 0.3655320107936859\n",
            "\tЭпоха 3. Итерация 1500/6774. Loss: 0.24633002281188965\n",
            "\tЭпоха 3. Итерация 1550/6774. Loss: 0.41871026158332825\n",
            "\tЭпоха 3. Итерация 1600/6774. Loss: 0.3817703127861023\n",
            "\tЭпоха 3. Итерация 1650/6774. Loss: 0.38074561953544617\n",
            "\tЭпоха 3. Итерация 1700/6774. Loss: 0.3349577784538269\n",
            "\tЭпоха 3. Итерация 1750/6774. Loss: 0.5713488459587097\n",
            "\tЭпоха 3. Итерация 1800/6774. Loss: 0.3159545361995697\n",
            "\tЭпоха 3. Итерация 1850/6774. Loss: 0.5425722599029541\n",
            "\tЭпоха 3. Итерация 1900/6774. Loss: 0.7488095760345459\n",
            "\tЭпоха 3. Итерация 1950/6774. Loss: 0.15683482587337494\n",
            "\tЭпоха 3. Итерация 2000/6774. Loss: 0.5029070973396301\n",
            "\tЭпоха 3. Итерация 2050/6774. Loss: 0.3272288143634796\n",
            "\tЭпоха 3. Итерация 2100/6774. Loss: 0.22893033921718597\n",
            "\tЭпоха 3. Итерация 2150/6774. Loss: 0.36053285002708435\n",
            "\tЭпоха 3. Итерация 2200/6774. Loss: 0.3038906455039978\n",
            "\tЭпоха 3. Итерация 2250/6774. Loss: 0.3064747750759125\n",
            "\tЭпоха 3. Итерация 2300/6774. Loss: 0.18578030169010162\n",
            "\tЭпоха 3. Итерация 2350/6774. Loss: 0.33937692642211914\n",
            "\tЭпоха 3. Итерация 2400/6774. Loss: 0.37697872519493103\n",
            "\tЭпоха 3. Итерация 2450/6774. Loss: 0.3261823058128357\n",
            "\tЭпоха 3. Итерация 2500/6774. Loss: 0.21585167944431305\n",
            "\tЭпоха 3. Итерация 2550/6774. Loss: 0.3302173614501953\n",
            "\tЭпоха 3. Итерация 2600/6774. Loss: 0.5426003932952881\n",
            "\tЭпоха 3. Итерация 2650/6774. Loss: 0.5118193626403809\n",
            "\tЭпоха 3. Итерация 2700/6774. Loss: 0.5426220297813416\n",
            "\tЭпоха 3. Итерация 2750/6774. Loss: 0.3890902101993561\n",
            "\tЭпоха 3. Итерация 2800/6774. Loss: 0.36140772700309753\n",
            "\tЭпоха 3. Итерация 2850/6774. Loss: 0.3281515836715698\n",
            "\tЭпоха 3. Итерация 2900/6774. Loss: 0.30975309014320374\n",
            "\tЭпоха 3. Итерация 2950/6774. Loss: 0.283447265625\n",
            "\tЭпоха 3. Итерация 3000/6774. Loss: 0.40519067645072937\n",
            "\tЭпоха 3. Итерация 3050/6774. Loss: 0.29493218660354614\n",
            "\tЭпоха 3. Итерация 3100/6774. Loss: 0.15981321036815643\n",
            "\tЭпоха 3. Итерация 3150/6774. Loss: 0.3260907530784607\n",
            "\tЭпоха 3. Итерация 3200/6774. Loss: 0.2953464686870575\n",
            "\tЭпоха 3. Итерация 3250/6774. Loss: 0.3647482991218567\n",
            "\tЭпоха 3. Итерация 3300/6774. Loss: 0.32170727849006653\n",
            "\tЭпоха 3. Итерация 3350/6774. Loss: 0.4548830986022949\n",
            "\tЭпоха 3. Итерация 3400/6774. Loss: 0.3153432011604309\n",
            "\tЭпоха 3. Итерация 3450/6774. Loss: 0.26069438457489014\n",
            "\tЭпоха 3. Итерация 3500/6774. Loss: 0.29421916604042053\n",
            "\tЭпоха 3. Итерация 3550/6774. Loss: 0.3952752947807312\n",
            "\tЭпоха 3. Итерация 3600/6774. Loss: 0.3227981925010681\n",
            "\tЭпоха 3. Итерация 3650/6774. Loss: 0.3187529146671295\n",
            "\tЭпоха 3. Итерация 3700/6774. Loss: 0.4134458601474762\n",
            "\tЭпоха 3. Итерация 3750/6774. Loss: 0.2345305234193802\n",
            "\tЭпоха 3. Итерация 3800/6774. Loss: 0.3798535466194153\n",
            "\tЭпоха 3. Итерация 3850/6774. Loss: 0.31444716453552246\n",
            "\tЭпоха 3. Итерация 3900/6774. Loss: 0.3496212661266327\n",
            "\tЭпоха 3. Итерация 3950/6774. Loss: 0.3950970768928528\n",
            "\tЭпоха 3. Итерация 4000/6774. Loss: 0.3425307273864746\n",
            "\tЭпоха 3. Итерация 4050/6774. Loss: 0.27806368470191956\n",
            "\tЭпоха 3. Итерация 4100/6774. Loss: 0.2500990033149719\n",
            "\tЭпоха 3. Итерация 4150/6774. Loss: 0.16900408267974854\n",
            "\tЭпоха 3. Итерация 4200/6774. Loss: 0.9071934819221497\n",
            "\tЭпоха 3. Итерация 4250/6774. Loss: 0.44397956132888794\n",
            "\tЭпоха 3. Итерация 4300/6774. Loss: 0.3117855489253998\n",
            "\tЭпоха 3. Итерация 4350/6774. Loss: 0.22881577908992767\n",
            "\tЭпоха 3. Итерация 4400/6774. Loss: 0.42901262640953064\n",
            "\tЭпоха 3. Итерация 4450/6774. Loss: 0.29033419489860535\n",
            "\tЭпоха 3. Итерация 4500/6774. Loss: 0.3512704372406006\n",
            "\tЭпоха 3. Итерация 4550/6774. Loss: 0.44271066784858704\n",
            "\tЭпоха 3. Итерация 4600/6774. Loss: 0.4998128414154053\n",
            "\tЭпоха 3. Итерация 4650/6774. Loss: 0.4320876896381378\n",
            "\tЭпоха 3. Итерация 4700/6774. Loss: 0.35433411598205566\n",
            "\tЭпоха 3. Итерация 4750/6774. Loss: 0.3213806450366974\n",
            "\tЭпоха 3. Итерация 4800/6774. Loss: 0.694284200668335\n",
            "\tЭпоха 3. Итерация 4850/6774. Loss: 0.5857424736022949\n",
            "\tЭпоха 3. Итерация 4900/6774. Loss: 0.3336128294467926\n",
            "\tЭпоха 3. Итерация 4950/6774. Loss: 0.3485924005508423\n",
            "\tЭпоха 3. Итерация 5000/6774. Loss: 0.7560489773750305\n",
            "\tЭпоха 3. Итерация 5050/6774. Loss: 0.21725793182849884\n",
            "\tЭпоха 3. Итерация 5100/6774. Loss: 0.4160654842853546\n",
            "\tЭпоха 3. Итерация 5150/6774. Loss: 0.4252774119377136\n",
            "\tЭпоха 3. Итерация 5200/6774. Loss: 0.14955422282218933\n",
            "\tЭпоха 3. Итерация 5250/6774. Loss: 0.35955771803855896\n",
            "\tЭпоха 3. Итерация 5300/6774. Loss: 0.24321848154067993\n",
            "\tЭпоха 3. Итерация 5350/6774. Loss: 0.3021199703216553\n",
            "\tЭпоха 3. Итерация 5400/6774. Loss: 0.3055301904678345\n",
            "\tЭпоха 3. Итерация 5450/6774. Loss: 0.3643554449081421\n",
            "\tЭпоха 3. Итерация 5500/6774. Loss: 0.41437458992004395\n",
            "\tЭпоха 3. Итерация 5550/6774. Loss: 0.36526817083358765\n",
            "\tЭпоха 3. Итерация 5600/6774. Loss: 0.23715458810329437\n",
            "\tЭпоха 3. Итерация 5650/6774. Loss: 0.3311896026134491\n",
            "\tЭпоха 3. Итерация 5700/6774. Loss: 0.4576210379600525\n",
            "\tЭпоха 3. Итерация 5750/6774. Loss: 0.37005332112312317\n",
            "\tЭпоха 3. Итерация 5800/6774. Loss: 0.3372334837913513\n",
            "\tЭпоха 3. Итерация 5850/6774. Loss: 0.31197842955589294\n",
            "\tЭпоха 3. Итерация 5900/6774. Loss: 0.3063225746154785\n",
            "\tЭпоха 3. Итерация 5950/6774. Loss: 0.4404281675815582\n",
            "\tЭпоха 3. Итерация 6000/6774. Loss: 0.3907303810119629\n",
            "\tЭпоха 3. Итерация 6050/6774. Loss: 0.23124217987060547\n",
            "\tЭпоха 3. Итерация 6100/6774. Loss: 0.364819198846817\n",
            "\tЭпоха 3. Итерация 6150/6774. Loss: 0.407914400100708\n",
            "\tЭпоха 3. Итерация 6200/6774. Loss: 0.4739909768104553\n",
            "\tЭпоха 3. Итерация 6250/6774. Loss: 0.5878323912620544\n",
            "\tЭпоха 3. Итерация 6300/6774. Loss: 0.22749990224838257\n",
            "\tЭпоха 3. Итерация 6350/6774. Loss: 0.34614047408103943\n",
            "\tЭпоха 3. Итерация 6400/6774. Loss: 0.2536269426345825\n",
            "\tЭпоха 3. Итерация 6450/6774. Loss: 0.3601311147212982\n",
            "\tЭпоха 3. Итерация 6500/6774. Loss: 0.42445865273475647\n",
            "\tЭпоха 3. Итерация 6550/6774. Loss: 0.26269257068634033\n",
            "\tЭпоха 3. Итерация 6600/6774. Loss: 0.5084567070007324\n",
            "\tЭпоха 3. Итерация 6650/6774. Loss: 0.36069032549858093\n",
            "\tЭпоха 3. Итерация 6700/6774. Loss: 0.4539448022842407\n",
            "\tЭпоха 3. Итерация 6750/6774. Loss: 0.4517667293548584\n",
            "Эпоха #3 train_loss: 0.04478149950139116, val_loss: 0.04986235699802637\n",
            "Потрачено 49.2 минут на 3 эпоху\n",
            "\tЭпоха 4. Итерация 0/6774. Loss: 0.4020726680755615\n",
            "\tЭпоха 4. Итерация 50/6774. Loss: 0.30905845761299133\n",
            "\tЭпоха 4. Итерация 100/6774. Loss: 0.5285887122154236\n",
            "\tЭпоха 4. Итерация 150/6774. Loss: 0.2678665816783905\n",
            "\tЭпоха 4. Итерация 200/6774. Loss: 0.4048873484134674\n",
            "\tЭпоха 4. Итерация 250/6774. Loss: 0.3988003134727478\n",
            "\tЭпоха 4. Итерация 300/6774. Loss: 0.43511536717414856\n",
            "\tЭпоха 4. Итерация 350/6774. Loss: 0.3894948661327362\n",
            "\tЭпоха 4. Итерация 400/6774. Loss: 0.5065560340881348\n",
            "\tЭпоха 4. Итерация 450/6774. Loss: 0.2763676345348358\n",
            "\tЭпоха 4. Итерация 500/6774. Loss: 0.296188622713089\n",
            "\tЭпоха 4. Итерация 550/6774. Loss: 0.30936604738235474\n",
            "\tЭпоха 4. Итерация 600/6774. Loss: 0.3085092306137085\n",
            "\tЭпоха 4. Итерация 650/6774. Loss: 0.3339523673057556\n",
            "\tЭпоха 4. Итерация 700/6774. Loss: 0.319060742855072\n",
            "\tЭпоха 4. Итерация 750/6774. Loss: 0.5044836401939392\n",
            "\tЭпоха 4. Итерация 800/6774. Loss: 0.2941506803035736\n",
            "\tЭпоха 4. Итерация 850/6774. Loss: 0.4303390383720398\n",
            "\tЭпоха 4. Итерация 900/6774. Loss: 0.3589863181114197\n",
            "\tЭпоха 4. Итерация 950/6774. Loss: 0.4100857973098755\n",
            "\tЭпоха 4. Итерация 1000/6774. Loss: 0.5014880895614624\n",
            "\tЭпоха 4. Итерация 1050/6774. Loss: 0.35582220554351807\n",
            "\tЭпоха 4. Итерация 1100/6774. Loss: 0.5150396823883057\n",
            "\tЭпоха 4. Итерация 1150/6774. Loss: 0.2859395444393158\n",
            "\tЭпоха 4. Итерация 1200/6774. Loss: 0.40352287888526917\n",
            "\tЭпоха 4. Итерация 1250/6774. Loss: 0.2746299207210541\n",
            "\tЭпоха 4. Итерация 1300/6774. Loss: 0.32259246706962585\n",
            "\tЭпоха 4. Итерация 1350/6774. Loss: 0.22694431245326996\n",
            "\tЭпоха 4. Итерация 1400/6774. Loss: 0.38527747988700867\n",
            "\tЭпоха 4. Итерация 1450/6774. Loss: 0.366534024477005\n",
            "\tЭпоха 4. Итерация 1500/6774. Loss: 0.3326556086540222\n",
            "\tЭпоха 4. Итерация 1550/6774. Loss: 0.2541566491127014\n",
            "\tЭпоха 4. Итерация 1600/6774. Loss: 0.4127250611782074\n",
            "\tЭпоха 4. Итерация 1650/6774. Loss: 0.3103575110435486\n",
            "\tЭпоха 4. Итерация 1700/6774. Loss: 0.398375928401947\n",
            "\tЭпоха 4. Итерация 1750/6774. Loss: 0.3259070813655853\n",
            "\tЭпоха 4. Итерация 1800/6774. Loss: 0.3744964003562927\n",
            "\tЭпоха 4. Итерация 1850/6774. Loss: 0.39910587668418884\n",
            "\tЭпоха 4. Итерация 1900/6774. Loss: 0.5379545092582703\n",
            "\tЭпоха 4. Итерация 1950/6774. Loss: 0.49190255999565125\n",
            "\tЭпоха 4. Итерация 2000/6774. Loss: 0.3478332757949829\n",
            "\tЭпоха 4. Итерация 2050/6774. Loss: 0.3286934494972229\n",
            "\tЭпоха 4. Итерация 2100/6774. Loss: 0.3219574987888336\n",
            "\tЭпоха 4. Итерация 2150/6774. Loss: 0.3653348982334137\n",
            "\tЭпоха 4. Итерация 2200/6774. Loss: 0.4589076340198517\n",
            "\tЭпоха 4. Итерация 2250/6774. Loss: 0.3711208701133728\n",
            "\tЭпоха 4. Итерация 2300/6774. Loss: 0.33842331171035767\n",
            "\tЭпоха 4. Итерация 2350/6774. Loss: 0.3257945477962494\n",
            "\tЭпоха 4. Итерация 2400/6774. Loss: 0.25155091285705566\n",
            "\tЭпоха 4. Итерация 2450/6774. Loss: 0.33648160099983215\n",
            "\tЭпоха 4. Итерация 2500/6774. Loss: 0.28473445773124695\n",
            "\tЭпоха 4. Итерация 2550/6774. Loss: 0.36038026213645935\n",
            "\tЭпоха 4. Итерация 2600/6774. Loss: 0.3031115233898163\n",
            "\tЭпоха 4. Итерация 2650/6774. Loss: 0.5799340009689331\n",
            "\tЭпоха 4. Итерация 2700/6774. Loss: 0.45697569847106934\n",
            "\tЭпоха 4. Итерация 2750/6774. Loss: 0.19417989253997803\n",
            "\tЭпоха 4. Итерация 2800/6774. Loss: 0.5111010074615479\n",
            "\tЭпоха 4. Итерация 2850/6774. Loss: 0.31696170568466187\n",
            "\tЭпоха 4. Итерация 2900/6774. Loss: 0.33059191703796387\n",
            "\tЭпоха 4. Итерация 2950/6774. Loss: 0.33908793330192566\n",
            "\tЭпоха 4. Итерация 3000/6774. Loss: 0.12629334628582\n",
            "\tЭпоха 4. Итерация 3050/6774. Loss: 0.330703467130661\n",
            "\tЭпоха 4. Итерация 3100/6774. Loss: 0.19992728531360626\n",
            "\tЭпоха 4. Итерация 3150/6774. Loss: 0.25653907656669617\n",
            "\tЭпоха 4. Итерация 3200/6774. Loss: 0.2424170821905136\n",
            "\tЭпоха 4. Итерация 3250/6774. Loss: 0.42700132727622986\n",
            "\tЭпоха 4. Итерация 3300/6774. Loss: 0.3248606026172638\n",
            "\tЭпоха 4. Итерация 3350/6774. Loss: 0.3647662401199341\n",
            "\tЭпоха 4. Итерация 3400/6774. Loss: 0.3920629322528839\n",
            "\tЭпоха 4. Итерация 3450/6774. Loss: 0.41353875398635864\n",
            "\tЭпоха 4. Итерация 3500/6774. Loss: 0.36867406964302063\n",
            "\tЭпоха 4. Итерация 3550/6774. Loss: 0.4166390597820282\n",
            "\tЭпоха 4. Итерация 3600/6774. Loss: 0.29477420449256897\n",
            "\tЭпоха 4. Итерация 3650/6774. Loss: 0.5487056374549866\n",
            "\tЭпоха 4. Итерация 3700/6774. Loss: 0.32399624586105347\n",
            "\tЭпоха 4. Итерация 3750/6774. Loss: 0.5384054183959961\n",
            "\tЭпоха 4. Итерация 3800/6774. Loss: 0.2858901917934418\n",
            "\tЭпоха 4. Итерация 3850/6774. Loss: 0.3325367271900177\n",
            "\tЭпоха 4. Итерация 3900/6774. Loss: 0.34382036328315735\n",
            "\tЭпоха 4. Итерация 3950/6774. Loss: 0.38186904788017273\n",
            "\tЭпоха 4. Итерация 4000/6774. Loss: 0.3601212501525879\n",
            "\tЭпоха 4. Итерация 4050/6774. Loss: 0.43305066227912903\n",
            "\tЭпоха 4. Итерация 4100/6774. Loss: 0.26439014077186584\n",
            "\tЭпоха 4. Итерация 4150/6774. Loss: 0.38069653511047363\n",
            "\tЭпоха 4. Итерация 4200/6774. Loss: 0.3850792348384857\n",
            "\tЭпоха 4. Итерация 4250/6774. Loss: 0.31490767002105713\n",
            "\tЭпоха 4. Итерация 4300/6774. Loss: 0.5650919079780579\n",
            "\tЭпоха 4. Итерация 4350/6774. Loss: 0.4215516448020935\n",
            "\tЭпоха 4. Итерация 4400/6774. Loss: 0.3098064064979553\n",
            "\tЭпоха 4. Итерация 4450/6774. Loss: 0.21397805213928223\n",
            "\tЭпоха 4. Итерация 4500/6774. Loss: 0.39834344387054443\n",
            "\tЭпоха 4. Итерация 4550/6774. Loss: 0.5519026517868042\n",
            "\tЭпоха 4. Итерация 4600/6774. Loss: 0.27125221490859985\n",
            "\tЭпоха 4. Итерация 4650/6774. Loss: 0.17657919228076935\n",
            "\tЭпоха 4. Итерация 4700/6774. Loss: 0.2882687449455261\n",
            "\tЭпоха 4. Итерация 4750/6774. Loss: 0.39238226413726807\n",
            "\tЭпоха 4. Итерация 4800/6774. Loss: 0.3493354618549347\n",
            "\tЭпоха 4. Итерация 4850/6774. Loss: 0.3810936510562897\n",
            "\tЭпоха 4. Итерация 4900/6774. Loss: 0.5927990674972534\n",
            "\tЭпоха 4. Итерация 4950/6774. Loss: 0.31019532680511475\n",
            "\tЭпоха 4. Итерация 5000/6774. Loss: 0.39705225825309753\n",
            "\tЭпоха 4. Итерация 5050/6774. Loss: 0.6522000432014465\n",
            "\tЭпоха 4. Итерация 5100/6774. Loss: 0.3071192502975464\n",
            "\tЭпоха 4. Итерация 5150/6774. Loss: 0.23154571652412415\n",
            "\tЭпоха 4. Итерация 5200/6774. Loss: 0.29525139927864075\n",
            "\tЭпоха 4. Итерация 5250/6774. Loss: 0.33664199709892273\n",
            "\tЭпоха 4. Итерация 5300/6774. Loss: 0.29689615964889526\n",
            "\tЭпоха 4. Итерация 5350/6774. Loss: 0.28180766105651855\n",
            "\tЭпоха 4. Итерация 5400/6774. Loss: 0.35744553804397583\n",
            "\tЭпоха 4. Итерация 5450/6774. Loss: 0.3217161297798157\n",
            "\tЭпоха 4. Итерация 5500/6774. Loss: 0.21070395410060883\n",
            "\tЭпоха 4. Итерация 5550/6774. Loss: 0.34076228737831116\n",
            "\tЭпоха 4. Итерация 5600/6774. Loss: 0.2828730344772339\n",
            "\tЭпоха 4. Итерация 5650/6774. Loss: 0.29030004143714905\n",
            "\tЭпоха 4. Итерация 5700/6774. Loss: 0.43148717284202576\n",
            "\tЭпоха 4. Итерация 5750/6774. Loss: 0.23357713222503662\n",
            "\tЭпоха 4. Итерация 5800/6774. Loss: 0.3529087007045746\n",
            "\tЭпоха 4. Итерация 5850/6774. Loss: 0.2931641936302185\n",
            "\tЭпоха 4. Итерация 5900/6774. Loss: 0.40897876024246216\n",
            "\tЭпоха 4. Итерация 5950/6774. Loss: 0.4007856845855713\n",
            "\tЭпоха 4. Итерация 6000/6774. Loss: 0.2715589106082916\n",
            "\tЭпоха 4. Итерация 6050/6774. Loss: 0.41973555088043213\n",
            "\tЭпоха 4. Итерация 6100/6774. Loss: 0.41283443570137024\n",
            "\tЭпоха 4. Итерация 6150/6774. Loss: 0.3250635862350464\n",
            "\tЭпоха 4. Итерация 6200/6774. Loss: 0.27974650263786316\n",
            "\tЭпоха 4. Итерация 6250/6774. Loss: 0.4157582223415375\n",
            "\tЭпоха 4. Итерация 6300/6774. Loss: 0.31663745641708374\n",
            "\tЭпоха 4. Итерация 6350/6774. Loss: 0.3239412307739258\n",
            "\tЭпоха 4. Итерация 6400/6774. Loss: 0.31198588013648987\n",
            "\tЭпоха 4. Итерация 6450/6774. Loss: 0.5214074850082397\n",
            "\tЭпоха 4. Итерация 6500/6774. Loss: 0.29540151357650757\n",
            "\tЭпоха 4. Итерация 6550/6774. Loss: 0.39318230748176575\n",
            "\tЭпоха 4. Итерация 6600/6774. Loss: 0.13011229038238525\n",
            "\tЭпоха 4. Итерация 6650/6774. Loss: 0.3366398215293884\n",
            "\tЭпоха 4. Итерация 6700/6774. Loss: 0.17710621654987335\n",
            "\tЭпоха 4. Итерация 6750/6774. Loss: 0.2222580909729004\n",
            "Эпоха #4 train_loss: 0.044319538132561793, val_loss: 0.04664341243952513\n",
            "Потрачено 49.1 минут на 4 эпоху\n",
            "\tЭпоха 5. Итерация 0/6774. Loss: 0.3758436441421509\n",
            "\tЭпоха 5. Итерация 50/6774. Loss: 0.20925109088420868\n",
            "\tЭпоха 5. Итерация 100/6774. Loss: 0.31020069122314453\n",
            "\tЭпоха 5. Итерация 150/6774. Loss: 0.3548519015312195\n",
            "\tЭпоха 5. Итерация 200/6774. Loss: 0.3416458070278168\n",
            "\tЭпоха 5. Итерация 250/6774. Loss: 0.35284313559532166\n",
            "\tЭпоха 5. Итерация 300/6774. Loss: 0.2637600302696228\n",
            "\tЭпоха 5. Итерация 350/6774. Loss: 0.44618648290634155\n",
            "\tЭпоха 5. Итерация 400/6774. Loss: 0.42541733384132385\n",
            "\tЭпоха 5. Итерация 450/6774. Loss: 0.34441274404525757\n",
            "\tЭпоха 5. Итерация 500/6774. Loss: 0.25524604320526123\n",
            "\tЭпоха 5. Итерация 550/6774. Loss: 0.2739761471748352\n",
            "\tЭпоха 5. Итерация 600/6774. Loss: 0.4133385419845581\n",
            "\tЭпоха 5. Итерация 650/6774. Loss: 0.4922897219657898\n",
            "\tЭпоха 5. Итерация 700/6774. Loss: 0.34666189551353455\n",
            "\tЭпоха 5. Итерация 750/6774. Loss: 0.5139595866203308\n",
            "\tЭпоха 5. Итерация 800/6774. Loss: 0.2571624517440796\n",
            "\tЭпоха 5. Итерация 850/6774. Loss: 0.3133081793785095\n",
            "\tЭпоха 5. Итерация 900/6774. Loss: 0.2509181797504425\n",
            "\tЭпоха 5. Итерация 950/6774. Loss: 0.4494488835334778\n",
            "\tЭпоха 5. Итерация 1000/6774. Loss: 0.4951154291629791\n",
            "\tЭпоха 5. Итерация 1050/6774. Loss: 0.24608610570430756\n",
            "\tЭпоха 5. Итерация 1100/6774. Loss: 0.35248643159866333\n",
            "\tЭпоха 5. Итерация 1150/6774. Loss: 0.25678369402885437\n",
            "\tЭпоха 5. Итерация 1200/6774. Loss: 0.3748866617679596\n",
            "\tЭпоха 5. Итерация 1250/6774. Loss: 0.18879029154777527\n",
            "\tЭпоха 5. Итерация 1300/6774. Loss: 0.3002004325389862\n",
            "\tЭпоха 5. Итерация 1350/6774. Loss: 0.22156275808811188\n",
            "\tЭпоха 5. Итерация 1400/6774. Loss: 0.3121890723705292\n",
            "\tЭпоха 5. Итерация 1450/6774. Loss: 0.3947472870349884\n",
            "\tЭпоха 5. Итерация 1500/6774. Loss: 0.6102020144462585\n",
            "\tЭпоха 5. Итерация 1550/6774. Loss: 0.3328612148761749\n",
            "\tЭпоха 5. Итерация 1600/6774. Loss: 0.5092991590499878\n",
            "\tЭпоха 5. Итерация 1650/6774. Loss: 0.3982607126235962\n",
            "\tЭпоха 5. Итерация 1700/6774. Loss: 0.3170596957206726\n",
            "\tЭпоха 5. Итерация 1750/6774. Loss: 0.237863689661026\n",
            "\tЭпоха 5. Итерация 1800/6774. Loss: 0.36173003911972046\n",
            "\tЭпоха 5. Итерация 1850/6774. Loss: 0.43014875054359436\n",
            "\tЭпоха 5. Итерация 1900/6774. Loss: 0.4032423496246338\n",
            "\tЭпоха 5. Итерация 1950/6774. Loss: 0.27086514234542847\n",
            "\tЭпоха 5. Итерация 2000/6774. Loss: 0.3761328160762787\n",
            "\tЭпоха 5. Итерация 2050/6774. Loss: 0.2834015488624573\n",
            "\tЭпоха 5. Итерация 2100/6774. Loss: 0.34518998861312866\n",
            "\tЭпоха 5. Итерация 2150/6774. Loss: 0.3185557425022125\n",
            "\tЭпоха 5. Итерация 2200/6774. Loss: 0.42732399702072144\n",
            "\tЭпоха 5. Итерация 2250/6774. Loss: 0.45419812202453613\n",
            "\tЭпоха 5. Итерация 2300/6774. Loss: 0.44904956221580505\n",
            "\tЭпоха 5. Итерация 2350/6774. Loss: 0.3622377812862396\n",
            "\tЭпоха 5. Итерация 2400/6774. Loss: 0.321633905172348\n",
            "\tЭпоха 5. Итерация 2450/6774. Loss: 0.4258248507976532\n",
            "\tЭпоха 5. Итерация 2500/6774. Loss: 0.6045528054237366\n",
            "\tЭпоха 5. Итерация 2550/6774. Loss: 0.27839258313179016\n",
            "\tЭпоха 5. Итерация 2600/6774. Loss: 0.16753160953521729\n",
            "\tЭпоха 5. Итерация 2650/6774. Loss: 0.3047199547290802\n",
            "\tЭпоха 5. Итерация 2700/6774. Loss: 0.5919455289840698\n",
            "\tЭпоха 5. Итерация 2750/6774. Loss: 0.26927101612091064\n",
            "\tЭпоха 5. Итерация 2800/6774. Loss: 0.25250113010406494\n",
            "\tЭпоха 5. Итерация 2850/6774. Loss: 0.338623583316803\n",
            "\tЭпоха 5. Итерация 2900/6774. Loss: 0.2337440401315689\n",
            "\tЭпоха 5. Итерация 2950/6774. Loss: 0.2721908688545227\n",
            "\tЭпоха 5. Итерация 3000/6774. Loss: 0.22042563557624817\n",
            "\tЭпоха 5. Итерация 3050/6774. Loss: 0.41575488448143005\n",
            "\tЭпоха 5. Итерация 3100/6774. Loss: 0.2641777992248535\n",
            "\tЭпоха 5. Итерация 3150/6774. Loss: 0.2613683044910431\n",
            "\tЭпоха 5. Итерация 3200/6774. Loss: 0.31185904145240784\n",
            "\tЭпоха 5. Итерация 3250/6774. Loss: 0.4130120277404785\n",
            "\tЭпоха 5. Итерация 3300/6774. Loss: 0.39156657457351685\n",
            "\tЭпоха 5. Итерация 3350/6774. Loss: 0.4080267548561096\n",
            "\tЭпоха 5. Итерация 3400/6774. Loss: 0.4089934527873993\n",
            "\tЭпоха 5. Итерация 3450/6774. Loss: 0.15290743112564087\n",
            "\tЭпоха 5. Итерация 3500/6774. Loss: 0.3482365310192108\n",
            "\tЭпоха 5. Итерация 3550/6774. Loss: 0.6660540103912354\n",
            "\tЭпоха 5. Итерация 3600/6774. Loss: 0.5285826325416565\n",
            "\tЭпоха 5. Итерация 3650/6774. Loss: 0.42071256041526794\n",
            "\tЭпоха 5. Итерация 3700/6774. Loss: 0.34599050879478455\n",
            "\tЭпоха 5. Итерация 3750/6774. Loss: 0.2776377499103546\n",
            "\tЭпоха 5. Итерация 3800/6774. Loss: 0.3321521580219269\n",
            "\tЭпоха 5. Итерация 3850/6774. Loss: 0.45775410532951355\n",
            "\tЭпоха 5. Итерация 3900/6774. Loss: 0.39899638295173645\n",
            "\tЭпоха 5. Итерация 3950/6774. Loss: 0.2989654242992401\n",
            "\tЭпоха 5. Итерация 4000/6774. Loss: 0.4866493344306946\n",
            "\tЭпоха 5. Итерация 4050/6774. Loss: 0.36065417528152466\n",
            "\tЭпоха 5. Итерация 4100/6774. Loss: 0.20080316066741943\n",
            "\tЭпоха 5. Итерация 4150/6774. Loss: 0.2772567570209503\n",
            "\tЭпоха 5. Итерация 4200/6774. Loss: 0.31326910853385925\n",
            "\tЭпоха 5. Итерация 4250/6774. Loss: 0.4038548171520233\n",
            "\tЭпоха 5. Итерация 4300/6774. Loss: 0.20003527402877808\n",
            "\tЭпоха 5. Итерация 4350/6774. Loss: 0.3376860022544861\n",
            "\tЭпоха 5. Итерация 4400/6774. Loss: 0.46480754017829895\n",
            "\tЭпоха 5. Итерация 4450/6774. Loss: 0.42101556062698364\n",
            "\tЭпоха 5. Итерация 4500/6774. Loss: 0.28100132942199707\n",
            "\tЭпоха 5. Итерация 4550/6774. Loss: 0.3320327401161194\n",
            "\tЭпоха 5. Итерация 4600/6774. Loss: 0.2943950593471527\n",
            "\tЭпоха 5. Итерация 4650/6774. Loss: 0.3515743315219879\n",
            "\tЭпоха 5. Итерация 4700/6774. Loss: 0.4177195727825165\n",
            "\tЭпоха 5. Итерация 4750/6774. Loss: 0.5276017785072327\n",
            "\tЭпоха 5. Итерация 4800/6774. Loss: 0.19488145411014557\n",
            "\tЭпоха 5. Итерация 4850/6774. Loss: 0.2404385805130005\n",
            "\tЭпоха 5. Итерация 4900/6774. Loss: 0.38848769664764404\n",
            "\tЭпоха 5. Итерация 4950/6774. Loss: 0.2208322137594223\n",
            "\tЭпоха 5. Итерация 5000/6774. Loss: 0.4205673933029175\n",
            "\tЭпоха 5. Итерация 5050/6774. Loss: 0.4369054138660431\n",
            "\tЭпоха 5. Итерация 5100/6774. Loss: 0.3078545033931732\n",
            "\tЭпоха 5. Итерация 5150/6774. Loss: 0.327869713306427\n",
            "\tЭпоха 5. Итерация 5200/6774. Loss: 0.3453092873096466\n",
            "\tЭпоха 5. Итерация 5250/6774. Loss: 0.3151472806930542\n",
            "\tЭпоха 5. Итерация 5300/6774. Loss: 0.36041495203971863\n",
            "\tЭпоха 5. Итерация 5350/6774. Loss: 0.2327437400817871\n",
            "\tЭпоха 5. Итерация 5400/6774. Loss: 0.2085142433643341\n",
            "\tЭпоха 5. Итерация 5450/6774. Loss: 0.26668840646743774\n",
            "\tЭпоха 5. Итерация 5500/6774. Loss: 0.3001214861869812\n",
            "\tЭпоха 5. Итерация 5550/6774. Loss: 0.25886112451553345\n",
            "\tЭпоха 5. Итерация 5600/6774. Loss: 0.3083285093307495\n",
            "\tЭпоха 5. Итерация 5650/6774. Loss: 0.20533129572868347\n",
            "\tЭпоха 5. Итерация 5700/6774. Loss: 0.40928536653518677\n",
            "\tЭпоха 5. Итерация 5750/6774. Loss: 0.3948586583137512\n",
            "\tЭпоха 5. Итерация 5800/6774. Loss: 0.19596463441848755\n",
            "\tЭпоха 5. Итерация 5850/6774. Loss: 0.4160163104534149\n",
            "\tЭпоха 5. Итерация 5900/6774. Loss: 0.2365046590566635\n",
            "\tЭпоха 5. Итерация 5950/6774. Loss: 0.40685606002807617\n",
            "\tЭпоха 5. Итерация 6000/6774. Loss: 0.19018928706645966\n",
            "\tЭпоха 5. Итерация 6050/6774. Loss: 0.3353915214538574\n",
            "\tЭпоха 5. Итерация 6100/6774. Loss: 0.4287140667438507\n",
            "\tЭпоха 5. Итерация 6150/6774. Loss: 0.3505866527557373\n",
            "\tЭпоха 5. Итерация 6200/6774. Loss: 0.5158790946006775\n",
            "\tЭпоха 5. Итерация 6250/6774. Loss: 0.29351818561553955\n",
            "\tЭпоха 5. Итерация 6300/6774. Loss: 0.38936370611190796\n",
            "\tЭпоха 5. Итерация 6350/6774. Loss: 0.27787142992019653\n",
            "\tЭпоха 5. Итерация 6400/6774. Loss: 0.22605493664741516\n",
            "\tЭпоха 5. Итерация 6450/6774. Loss: 0.1885300576686859\n",
            "\tЭпоха 5. Итерация 6500/6774. Loss: 0.35441526770591736\n",
            "\tЭпоха 5. Итерация 6550/6774. Loss: 0.3572927415370941\n",
            "\tЭпоха 5. Итерация 6600/6774. Loss: 0.38069114089012146\n",
            "\tЭпоха 5. Итерация 6650/6774. Loss: 0.19737660884857178\n",
            "\tЭпоха 5. Итерация 6700/6774. Loss: 0.4273449778556824\n",
            "\tЭпоха 5. Итерация 6750/6774. Loss: 0.24095502495765686\n",
            "Эпоха #5 train_loss: 0.043181144488510535, val_loss: 0.05141824644804001\n",
            "Потрачено 49.3 минут на 5 эпоху\n",
            "\tЭпоха 6. Итерация 0/6774. Loss: 0.2815762460231781\n",
            "\tЭпоха 6. Итерация 50/6774. Loss: 0.3093547821044922\n",
            "\tЭпоха 6. Итерация 100/6774. Loss: 0.368071973323822\n",
            "\tЭпоха 6. Итерация 150/6774. Loss: 0.527187705039978\n",
            "\tЭпоха 6. Итерация 200/6774. Loss: 0.3633479177951813\n",
            "\tЭпоха 6. Итерация 250/6774. Loss: 0.2766014039516449\n",
            "\tЭпоха 6. Итерация 300/6774. Loss: 0.28390976786613464\n",
            "\tЭпоха 6. Итерация 350/6774. Loss: 0.3166266083717346\n",
            "\tЭпоха 6. Итерация 400/6774. Loss: 0.37596601247787476\n",
            "\tЭпоха 6. Итерация 450/6774. Loss: 0.2783449590206146\n",
            "\tЭпоха 6. Итерация 500/6774. Loss: 0.4291166663169861\n",
            "\tЭпоха 6. Итерация 550/6774. Loss: 0.31071457266807556\n",
            "\tЭпоха 6. Итерация 600/6774. Loss: 0.34582430124282837\n",
            "\tЭпоха 6. Итерация 650/6774. Loss: 0.3842138350009918\n",
            "\tЭпоха 6. Итерация 700/6774. Loss: 0.32373255491256714\n",
            "\tЭпоха 6. Итерация 750/6774. Loss: 0.4188317656517029\n",
            "\tЭпоха 6. Итерация 800/6774. Loss: 0.3462704122066498\n",
            "\tЭпоха 6. Итерация 850/6774. Loss: 0.42798900604248047\n",
            "\tЭпоха 6. Итерация 900/6774. Loss: 0.27616485953330994\n",
            "\tЭпоха 6. Итерация 950/6774. Loss: 0.3927874267101288\n",
            "\tЭпоха 6. Итерация 1000/6774. Loss: 0.36866769194602966\n",
            "\tЭпоха 6. Итерация 1050/6774. Loss: 0.44455596804618835\n",
            "\tЭпоха 6. Итерация 1100/6774. Loss: 0.22118890285491943\n",
            "\tЭпоха 6. Итерация 1150/6774. Loss: 0.3953229486942291\n",
            "\tЭпоха 6. Итерация 1200/6774. Loss: 0.257404625415802\n",
            "\tЭпоха 6. Итерация 1250/6774. Loss: 0.22356680035591125\n",
            "\tЭпоха 6. Итерация 1300/6774. Loss: 0.25055989623069763\n",
            "\tЭпоха 6. Итерация 1350/6774. Loss: 0.3513164222240448\n",
            "\tЭпоха 6. Итерация 1400/6774. Loss: 0.4412425756454468\n",
            "\tЭпоха 6. Итерация 1450/6774. Loss: 0.3337138891220093\n",
            "\tЭпоха 6. Итерация 1500/6774. Loss: 0.4161282181739807\n",
            "\tЭпоха 6. Итерация 1550/6774. Loss: 0.26319512724876404\n",
            "\tЭпоха 6. Итерация 1600/6774. Loss: 0.29373908042907715\n",
            "\tЭпоха 6. Итерация 1650/6774. Loss: 0.2990269660949707\n",
            "\tЭпоха 6. Итерация 1700/6774. Loss: 0.2785620391368866\n",
            "\tЭпоха 6. Итерация 1750/6774. Loss: 0.37717974185943604\n",
            "\tЭпоха 6. Итерация 1800/6774. Loss: 0.4461340010166168\n",
            "\tЭпоха 6. Итерация 1850/6774. Loss: 0.4559444785118103\n",
            "\tЭпоха 6. Итерация 1900/6774. Loss: 0.2874388098716736\n",
            "\tЭпоха 6. Итерация 1950/6774. Loss: 0.30789288878440857\n",
            "\tЭпоха 6. Итерация 2000/6774. Loss: 0.25539785623550415\n",
            "\tЭпоха 6. Итерация 2050/6774. Loss: 0.4668411910533905\n",
            "\tЭпоха 6. Итерация 2100/6774. Loss: 0.3740527331829071\n",
            "\tЭпоха 6. Итерация 2150/6774. Loss: 0.3470463156700134\n",
            "\tЭпоха 6. Итерация 2200/6774. Loss: 0.22206033766269684\n",
            "\tЭпоха 6. Итерация 2250/6774. Loss: 0.2979929447174072\n",
            "\tЭпоха 6. Итерация 2300/6774. Loss: 0.41100695729255676\n",
            "\tЭпоха 6. Итерация 2350/6774. Loss: 0.28053244948387146\n",
            "\tЭпоха 6. Итерация 2400/6774. Loss: 0.4579372704029083\n",
            "\tЭпоха 6. Итерация 2450/6774. Loss: 0.3410255014896393\n",
            "\tЭпоха 6. Итерация 2500/6774. Loss: 0.42660394310951233\n",
            "\tЭпоха 6. Итерация 2550/6774. Loss: 0.40758469700813293\n",
            "\tЭпоха 6. Итерация 2600/6774. Loss: 0.5545458197593689\n",
            "\tЭпоха 6. Итерация 2650/6774. Loss: 0.14815981686115265\n",
            "\tЭпоха 6. Итерация 2700/6774. Loss: 0.3443581163883209\n",
            "\tЭпоха 6. Итерация 2750/6774. Loss: 0.3910912573337555\n",
            "\tЭпоха 6. Итерация 2800/6774. Loss: 0.3325731158256531\n",
            "\tЭпоха 6. Итерация 2850/6774. Loss: 0.3267570734024048\n",
            "\tЭпоха 6. Итерация 2900/6774. Loss: 0.1822260618209839\n",
            "\tЭпоха 6. Итерация 2950/6774. Loss: 0.2622627019882202\n",
            "\tЭпоха 6. Итерация 3000/6774. Loss: 0.3022352159023285\n",
            "\tЭпоха 6. Итерация 3050/6774. Loss: 0.27413883805274963\n",
            "\tЭпоха 6. Итерация 3100/6774. Loss: 0.33339300751686096\n",
            "\tЭпоха 6. Итерация 3150/6774. Loss: 0.26043763756752014\n",
            "\tЭпоха 6. Итерация 3200/6774. Loss: 0.4299323558807373\n",
            "\tЭпоха 6. Итерация 3250/6774. Loss: 0.2643512487411499\n",
            "\tЭпоха 6. Итерация 3300/6774. Loss: 0.24888716638088226\n",
            "\tЭпоха 6. Итерация 3350/6774. Loss: 0.5260334610939026\n",
            "\tЭпоха 6. Итерация 3400/6774. Loss: 0.38892126083374023\n",
            "\tЭпоха 6. Итерация 3450/6774. Loss: 0.41982588171958923\n",
            "\tЭпоха 6. Итерация 3500/6774. Loss: 0.27358072996139526\n",
            "\tЭпоха 6. Итерация 3550/6774. Loss: 0.27822163701057434\n",
            "\tЭпоха 6. Итерация 3600/6774. Loss: 0.311320960521698\n",
            "\tЭпоха 6. Итерация 3650/6774. Loss: 0.4211319088935852\n",
            "\tЭпоха 6. Итерация 3700/6774. Loss: 0.5171109437942505\n",
            "\tЭпоха 6. Итерация 3750/6774. Loss: 0.41996753215789795\n",
            "\tЭпоха 6. Итерация 3800/6774. Loss: 0.47138985991477966\n",
            "\tЭпоха 6. Итерация 3850/6774. Loss: 0.41854068636894226\n",
            "\tЭпоха 6. Итерация 3900/6774. Loss: 0.23451685905456543\n",
            "\tЭпоха 6. Итерация 3950/6774. Loss: 0.4565872848033905\n",
            "\tЭпоха 6. Итерация 4000/6774. Loss: 0.5792747735977173\n",
            "\tЭпоха 6. Итерация 4050/6774. Loss: 0.515129029750824\n",
            "\tЭпоха 6. Итерация 4100/6774. Loss: 0.4118044674396515\n",
            "\tЭпоха 6. Итерация 4150/6774. Loss: 0.40423086285591125\n",
            "\tЭпоха 6. Итерация 4200/6774. Loss: 0.22917860746383667\n",
            "\tЭпоха 6. Итерация 4250/6774. Loss: 0.3607453405857086\n",
            "\tЭпоха 6. Итерация 4300/6774. Loss: 0.39687031507492065\n",
            "\tЭпоха 6. Итерация 4350/6774. Loss: 0.5367519855499268\n",
            "\tЭпоха 6. Итерация 4400/6774. Loss: 0.4897233247756958\n",
            "\tЭпоха 6. Итерация 4450/6774. Loss: 0.25511908531188965\n",
            "\tЭпоха 6. Итерация 4500/6774. Loss: 0.22429467737674713\n",
            "\tЭпоха 6. Итерация 4550/6774. Loss: 0.25955742597579956\n",
            "\tЭпоха 6. Итерация 4600/6774. Loss: 0.2764075696468353\n",
            "\tЭпоха 6. Итерация 4650/6774. Loss: 0.20817211270332336\n",
            "\tЭпоха 6. Итерация 4700/6774. Loss: 0.3698633015155792\n",
            "\tЭпоха 6. Итерация 4750/6774. Loss: 0.24040962755680084\n",
            "\tЭпоха 6. Итерация 4800/6774. Loss: 0.15634988248348236\n",
            "\tЭпоха 6. Итерация 4850/6774. Loss: 0.23612931370735168\n",
            "\tЭпоха 6. Итерация 4900/6774. Loss: 0.3337491750717163\n",
            "\tЭпоха 6. Итерация 4950/6774. Loss: 0.3706175684928894\n",
            "\tЭпоха 6. Итерация 5000/6774. Loss: 0.3580123782157898\n",
            "\tЭпоха 6. Итерация 5050/6774. Loss: 0.6116147041320801\n",
            "\tЭпоха 6. Итерация 5100/6774. Loss: 0.2435351312160492\n",
            "\tЭпоха 6. Итерация 5150/6774. Loss: 0.4575245678424835\n",
            "\tЭпоха 6. Итерация 5200/6774. Loss: 0.3982146084308624\n",
            "\tЭпоха 6. Итерация 5250/6774. Loss: 0.3581211566925049\n",
            "\tЭпоха 6. Итерация 5300/6774. Loss: 0.3829176425933838\n",
            "\tЭпоха 6. Итерация 5350/6774. Loss: 0.5162315964698792\n",
            "\tЭпоха 6. Итерация 5400/6774. Loss: 0.43951207399368286\n",
            "\tЭпоха 6. Итерация 5450/6774. Loss: 0.15971776843070984\n",
            "\tЭпоха 6. Итерация 5500/6774. Loss: 0.270867258310318\n",
            "\tЭпоха 6. Итерация 5550/6774. Loss: 0.5428766012191772\n",
            "\tЭпоха 6. Итерация 5600/6774. Loss: 0.4024479389190674\n",
            "\tЭпоха 6. Итерация 5650/6774. Loss: 0.34170398116111755\n",
            "\tЭпоха 6. Итерация 5700/6774. Loss: 0.28876161575317383\n",
            "\tЭпоха 6. Итерация 5750/6774. Loss: 0.4046212136745453\n",
            "\tЭпоха 6. Итерация 5800/6774. Loss: 0.15490707755088806\n",
            "\tЭпоха 6. Итерация 5850/6774. Loss: 0.3631691336631775\n",
            "\tЭпоха 6. Итерация 5900/6774. Loss: 0.2362084984779358\n",
            "\tЭпоха 6. Итерация 5950/6774. Loss: 0.3113146424293518\n",
            "\tЭпоха 6. Итерация 6000/6774. Loss: 0.1837039291858673\n",
            "\tЭпоха 6. Итерация 6050/6774. Loss: 0.3482518792152405\n",
            "\tЭпоха 6. Итерация 6100/6774. Loss: 0.2632095217704773\n",
            "\tЭпоха 6. Итерация 6150/6774. Loss: 0.32301968336105347\n",
            "\tЭпоха 6. Итерация 6200/6774. Loss: 0.1212543249130249\n",
            "\tЭпоха 6. Итерация 6250/6774. Loss: 0.374509334564209\n",
            "\tЭпоха 6. Итерация 6300/6774. Loss: 0.2997051477432251\n",
            "\tЭпоха 6. Итерация 6350/6774. Loss: 0.6042682528495789\n",
            "\tЭпоха 6. Итерация 6400/6774. Loss: 0.3707127273082733\n",
            "\tЭпоха 6. Итерация 6450/6774. Loss: 0.2891433835029602\n",
            "\tЭпоха 6. Итерация 6500/6774. Loss: 0.2717711925506592\n",
            "\tЭпоха 6. Итерация 6550/6774. Loss: 0.3523465692996979\n",
            "\tЭпоха 6. Итерация 6600/6774. Loss: 0.33931002020835876\n",
            "\tЭпоха 6. Итерация 6650/6774. Loss: 0.4030422270298004\n",
            "\tЭпоха 6. Итерация 6700/6774. Loss: 0.28314709663391113\n",
            "\tЭпоха 6. Итерация 6750/6774. Loss: 0.37837541103363037\n",
            "Эпоха #6 train_loss: 0.04290648898413795, val_loss: 0.03822999240756035\n",
            "Потрачено 49.2 минут на 6 эпоху\n",
            "\tЭпоха 7. Итерация 0/6774. Loss: 0.17787861824035645\n",
            "\tЭпоха 7. Итерация 50/6774. Loss: 0.24732424318790436\n",
            "\tЭпоха 7. Итерация 100/6774. Loss: 0.23013268411159515\n",
            "\tЭпоха 7. Итерация 150/6774. Loss: 0.19508780539035797\n",
            "\tЭпоха 7. Итерация 200/6774. Loss: 0.21198555827140808\n",
            "\tЭпоха 7. Итерация 250/6774. Loss: 0.2446693480014801\n",
            "\tЭпоха 7. Итерация 300/6774. Loss: 0.2994915544986725\n",
            "\tЭпоха 7. Итерация 350/6774. Loss: 0.18778131902217865\n",
            "\tЭпоха 7. Итерация 400/6774. Loss: 0.19966241717338562\n",
            "\tЭпоха 7. Итерация 450/6774. Loss: 0.25558456778526306\n",
            "\tЭпоха 7. Итерация 500/6774. Loss: 0.42258986830711365\n",
            "\tЭпоха 7. Итерация 550/6774. Loss: 0.2682879567146301\n",
            "\tЭпоха 7. Итерация 600/6774. Loss: 0.27877676486968994\n",
            "\tЭпоха 7. Итерация 650/6774. Loss: 0.428467333316803\n",
            "\tЭпоха 7. Итерация 700/6774. Loss: 0.35359135270118713\n",
            "\tЭпоха 7. Итерация 750/6774. Loss: 0.590308427810669\n",
            "\tЭпоха 7. Итерация 800/6774. Loss: 0.29293060302734375\n",
            "\tЭпоха 7. Итерация 850/6774. Loss: 0.4063366949558258\n",
            "\tЭпоха 7. Итерация 900/6774. Loss: 0.5137606859207153\n",
            "\tЭпоха 7. Итерация 950/6774. Loss: 0.3440510928630829\n",
            "\tЭпоха 7. Итерация 1000/6774. Loss: 0.8028509020805359\n",
            "\tЭпоха 7. Итерация 1050/6774. Loss: 0.41508543491363525\n",
            "\tЭпоха 7. Итерация 1100/6774. Loss: 0.2143482267856598\n",
            "\tЭпоха 7. Итерация 1150/6774. Loss: 0.3388473391532898\n",
            "\tЭпоха 7. Итерация 1200/6774. Loss: 0.30958855152130127\n",
            "\tЭпоха 7. Итерация 1250/6774. Loss: 0.4824536442756653\n",
            "\tЭпоха 7. Итерация 1300/6774. Loss: 0.43264710903167725\n",
            "\tЭпоха 7. Итерация 1350/6774. Loss: 0.24775980412960052\n",
            "\tЭпоха 7. Итерация 1400/6774. Loss: 0.41023534536361694\n",
            "\tЭпоха 7. Итерация 1450/6774. Loss: 0.4310215413570404\n",
            "\tЭпоха 7. Итерация 1500/6774. Loss: 0.34612634778022766\n",
            "\tЭпоха 7. Итерация 1550/6774. Loss: 0.21133282780647278\n",
            "\tЭпоха 7. Итерация 1600/6774. Loss: 0.2886044383049011\n",
            "\tЭпоха 7. Итерация 1650/6774. Loss: 0.3029744625091553\n",
            "\tЭпоха 7. Итерация 1700/6774. Loss: 0.2792109251022339\n",
            "\tЭпоха 7. Итерация 1750/6774. Loss: 0.3783576786518097\n",
            "\tЭпоха 7. Итерация 1800/6774. Loss: 0.3206368386745453\n",
            "\tЭпоха 7. Итерация 1850/6774. Loss: 0.6279773116111755\n",
            "\tЭпоха 7. Итерация 1900/6774. Loss: 0.42977139353752136\n",
            "\tЭпоха 7. Итерация 1950/6774. Loss: 0.23786547780036926\n",
            "\tЭпоха 7. Итерация 2000/6774. Loss: 0.23952753841876984\n",
            "\tЭпоха 7. Итерация 2050/6774. Loss: 0.38712427020072937\n",
            "\tЭпоха 7. Итерация 2100/6774. Loss: 0.30968567728996277\n",
            "\tЭпоха 7. Итерация 2150/6774. Loss: 0.4593641459941864\n",
            "\tЭпоха 7. Итерация 2200/6774. Loss: 0.5444754362106323\n",
            "\tЭпоха 7. Итерация 2250/6774. Loss: 0.4296751916408539\n",
            "\tЭпоха 7. Итерация 2300/6774. Loss: 0.24340224266052246\n",
            "\tЭпоха 7. Итерация 2350/6774. Loss: 0.2471139281988144\n",
            "\tЭпоха 7. Итерация 2400/6774. Loss: 0.3023747205734253\n",
            "\tЭпоха 7. Итерация 2450/6774. Loss: 0.18692409992218018\n",
            "\tЭпоха 7. Итерация 2500/6774. Loss: 0.2129918485879898\n",
            "\tЭпоха 7. Итерация 2550/6774. Loss: 0.3499275743961334\n",
            "\tЭпоха 7. Итерация 2600/6774. Loss: 0.5384036898612976\n",
            "\tЭпоха 7. Итерация 2650/6774. Loss: 0.23542317748069763\n",
            "\tЭпоха 7. Итерация 2700/6774. Loss: 0.42447930574417114\n",
            "\tЭпоха 7. Итерация 2750/6774. Loss: 0.29735639691352844\n",
            "\tЭпоха 7. Итерация 2800/6774. Loss: 0.25397437810897827\n",
            "\tЭпоха 7. Итерация 2850/6774. Loss: 0.27972137928009033\n",
            "\tЭпоха 7. Итерация 2900/6774. Loss: 0.4095824360847473\n",
            "\tЭпоха 7. Итерация 2950/6774. Loss: 0.19879640638828278\n",
            "\tЭпоха 7. Итерация 3000/6774. Loss: 0.31048280000686646\n",
            "\tЭпоха 7. Итерация 3050/6774. Loss: 0.39479389786720276\n",
            "\tЭпоха 7. Итерация 3100/6774. Loss: 0.383653849363327\n",
            "\tЭпоха 7. Итерация 3150/6774. Loss: 0.3161453604698181\n",
            "\tЭпоха 7. Итерация 3200/6774. Loss: 0.35496827960014343\n",
            "\tЭпоха 7. Итерация 3250/6774. Loss: 0.26438724994659424\n",
            "\tЭпоха 7. Итерация 3300/6774. Loss: 0.39666488766670227\n",
            "\tЭпоха 7. Итерация 3350/6774. Loss: 0.20700456202030182\n",
            "\tЭпоха 7. Итерация 3400/6774. Loss: 0.38619741797447205\n",
            "\tЭпоха 7. Итерация 3450/6774. Loss: 0.24168317019939423\n",
            "\tЭпоха 7. Итерация 3500/6774. Loss: 0.2944640815258026\n",
            "\tЭпоха 7. Итерация 3550/6774. Loss: 0.26464080810546875\n",
            "\tЭпоха 7. Итерация 3600/6774. Loss: 0.26128503680229187\n",
            "\tЭпоха 7. Итерация 3650/6774. Loss: 0.31747403740882874\n",
            "\tЭпоха 7. Итерация 3700/6774. Loss: 0.26607269048690796\n",
            "\tЭпоха 7. Итерация 3750/6774. Loss: 0.4543347954750061\n",
            "\tЭпоха 7. Итерация 3800/6774. Loss: 0.2967997193336487\n",
            "\tЭпоха 7. Итерация 3850/6774. Loss: 0.18545348942279816\n",
            "\tЭпоха 7. Итерация 3900/6774. Loss: 0.5092456340789795\n",
            "\tЭпоха 7. Итерация 3950/6774. Loss: 0.3567163646221161\n",
            "\tЭпоха 7. Итерация 4000/6774. Loss: 0.30447274446487427\n",
            "\tЭпоха 7. Итерация 4050/6774. Loss: 0.31312453746795654\n",
            "\tЭпоха 7. Итерация 4100/6774. Loss: 0.34785017371177673\n",
            "\tЭпоха 7. Итерация 4150/6774. Loss: 0.21607916057109833\n",
            "\tЭпоха 7. Итерация 4200/6774. Loss: 0.2605229616165161\n",
            "\tЭпоха 7. Итерация 4250/6774. Loss: 0.23504579067230225\n",
            "\tЭпоха 7. Итерация 4300/6774. Loss: 0.40416449308395386\n",
            "\tЭпоха 7. Итерация 4350/6774. Loss: 0.4697451591491699\n",
            "\tЭпоха 7. Итерация 4400/6774. Loss: 0.220526784658432\n",
            "\tЭпоха 7. Итерация 4450/6774. Loss: 0.23194444179534912\n",
            "\tЭпоха 7. Итерация 4500/6774. Loss: 0.3219800889492035\n",
            "\tЭпоха 7. Итерация 4550/6774. Loss: 0.39621347188949585\n",
            "\tЭпоха 7. Итерация 4600/6774. Loss: 0.3049112856388092\n",
            "\tЭпоха 7. Итерация 4650/6774. Loss: 0.28178104758262634\n",
            "\tЭпоха 7. Итерация 4700/6774. Loss: 0.4269463121891022\n",
            "\tЭпоха 7. Итерация 4750/6774. Loss: 0.21553343534469604\n",
            "\tЭпоха 7. Итерация 4800/6774. Loss: 0.3827556371688843\n",
            "\tЭпоха 7. Итерация 4850/6774. Loss: 0.27445361018180847\n",
            "\tЭпоха 7. Итерация 4900/6774. Loss: 0.3182171881198883\n",
            "\tЭпоха 7. Итерация 4950/6774. Loss: 0.3526778221130371\n",
            "\tЭпоха 7. Итерация 5000/6774. Loss: 0.5893970727920532\n",
            "\tЭпоха 7. Итерация 5050/6774. Loss: 0.2979761064052582\n",
            "\tЭпоха 7. Итерация 5100/6774. Loss: 0.3936857581138611\n",
            "\tЭпоха 7. Итерация 5150/6774. Loss: 0.32585516571998596\n",
            "\tЭпоха 7. Итерация 5200/6774. Loss: 0.3351379632949829\n",
            "\tЭпоха 7. Итерация 5250/6774. Loss: 0.21199481189250946\n",
            "\tЭпоха 7. Итерация 5300/6774. Loss: 0.6569564342498779\n",
            "\tЭпоха 7. Итерация 5350/6774. Loss: 0.20381934940814972\n",
            "\tЭпоха 7. Итерация 5400/6774. Loss: 0.4148431718349457\n",
            "\tЭпоха 7. Итерация 5450/6774. Loss: 0.4003753364086151\n",
            "\tЭпоха 7. Итерация 5500/6774. Loss: 0.30662283301353455\n",
            "\tЭпоха 7. Итерация 5550/6774. Loss: 0.2727898955345154\n",
            "\tЭпоха 7. Итерация 5600/6774. Loss: 0.6428386569023132\n",
            "\tЭпоха 7. Итерация 5650/6774. Loss: 0.20476706326007843\n",
            "\tЭпоха 7. Итерация 5700/6774. Loss: 0.3937765657901764\n",
            "\tЭпоха 7. Итерация 5750/6774. Loss: 0.3243197798728943\n",
            "\tЭпоха 7. Итерация 5800/6774. Loss: 0.3981897830963135\n",
            "\tЭпоха 7. Итерация 5850/6774. Loss: 0.20579823851585388\n",
            "\tЭпоха 7. Итерация 5900/6774. Loss: 0.21678081154823303\n",
            "\tЭпоха 7. Итерация 5950/6774. Loss: 0.17438283562660217\n",
            "\tЭпоха 7. Итерация 6000/6774. Loss: 0.37581756711006165\n",
            "\tЭпоха 7. Итерация 6050/6774. Loss: 0.2144772708415985\n",
            "\tЭпоха 7. Итерация 6100/6774. Loss: 0.45948323607444763\n",
            "\tЭпоха 7. Итерация 6150/6774. Loss: 0.24678966403007507\n",
            "\tЭпоха 7. Итерация 6200/6774. Loss: 0.2871553301811218\n",
            "\tЭпоха 7. Итерация 6250/6774. Loss: 0.39742031693458557\n",
            "\tЭпоха 7. Итерация 6300/6774. Loss: 0.2767331898212433\n",
            "\tЭпоха 7. Итерация 6350/6774. Loss: 0.43364179134368896\n",
            "\tЭпоха 7. Итерация 6400/6774. Loss: 0.34528955817222595\n",
            "\tЭпоха 7. Итерация 6450/6774. Loss: 0.34553322196006775\n",
            "\tЭпоха 7. Итерация 6500/6774. Loss: 0.25798898935317993\n",
            "\tЭпоха 7. Итерация 6550/6774. Loss: 0.312826007604599\n",
            "\tЭпоха 7. Итерация 6600/6774. Loss: 0.3393411636352539\n",
            "\tЭпоха 7. Итерация 6650/6774. Loss: 0.20760278403759003\n",
            "\tЭпоха 7. Итерация 6700/6774. Loss: 0.28417080640792847\n",
            "\tЭпоха 7. Итерация 6750/6774. Loss: 0.3810056149959564\n",
            "Эпоха #7 train_loss: 0.04273540742330695, val_loss: 0.03755336822867394\n",
            "Потрачено 49.8 минут на 7 эпоху\n",
            "\tЭпоха 8. Итерация 0/6774. Loss: 0.3087793290615082\n",
            "\tЭпоха 8. Итерация 50/6774. Loss: 0.2770646810531616\n",
            "\tЭпоха 8. Итерация 100/6774. Loss: 0.27907299995422363\n",
            "\tЭпоха 8. Итерация 150/6774. Loss: 0.40588343143463135\n",
            "\tЭпоха 8. Итерация 200/6774. Loss: 0.15150605142116547\n",
            "\tЭпоха 8. Итерация 250/6774. Loss: 0.4203213155269623\n",
            "\tЭпоха 8. Итерация 300/6774. Loss: 0.43791934847831726\n",
            "\tЭпоха 8. Итерация 350/6774. Loss: 0.44016486406326294\n",
            "\tЭпоха 8. Итерация 400/6774. Loss: 0.28959542512893677\n",
            "\tЭпоха 8. Итерация 450/6774. Loss: 0.37859460711479187\n",
            "\tЭпоха 8. Итерация 500/6774. Loss: 0.5605816841125488\n",
            "\tЭпоха 8. Итерация 550/6774. Loss: 0.17495596408843994\n",
            "\tЭпоха 8. Итерация 600/6774. Loss: 0.19886843860149384\n",
            "\tЭпоха 8. Итерация 650/6774. Loss: 0.34400349855422974\n",
            "\tЭпоха 8. Итерация 700/6774. Loss: 0.3204250633716583\n",
            "\tЭпоха 8. Итерация 750/6774. Loss: 0.22683316469192505\n",
            "\tЭпоха 8. Итерация 800/6774. Loss: 0.18365702033042908\n",
            "\tЭпоха 8. Итерация 850/6774. Loss: 0.4947041869163513\n",
            "\tЭпоха 8. Итерация 900/6774. Loss: 0.49165216088294983\n",
            "\tЭпоха 8. Итерация 950/6774. Loss: 0.4433760941028595\n",
            "\tЭпоха 8. Итерация 1000/6774. Loss: 0.2606891691684723\n",
            "\tЭпоха 8. Итерация 1050/6774. Loss: 0.42981886863708496\n",
            "\tЭпоха 8. Итерация 1100/6774. Loss: 0.30721989274024963\n",
            "\tЭпоха 8. Итерация 1150/6774. Loss: 0.2715396583080292\n",
            "\tЭпоха 8. Итерация 1200/6774. Loss: 0.276582807302475\n",
            "\tЭпоха 8. Итерация 1250/6774. Loss: 0.33587682247161865\n",
            "\tЭпоха 8. Итерация 1300/6774. Loss: 0.4459364712238312\n",
            "\tЭпоха 8. Итерация 1350/6774. Loss: 0.34243765473365784\n",
            "\tЭпоха 8. Итерация 1400/6774. Loss: 0.24366073310375214\n",
            "\tЭпоха 8. Итерация 1450/6774. Loss: 0.5508409738540649\n",
            "\tЭпоха 8. Итерация 1500/6774. Loss: 0.3650822639465332\n",
            "\tЭпоха 8. Итерация 1550/6774. Loss: 0.557246744632721\n",
            "\tЭпоха 8. Итерация 1600/6774. Loss: 0.27577027678489685\n",
            "\tЭпоха 8. Итерация 1650/6774. Loss: 0.32627183198928833\n",
            "\tЭпоха 8. Итерация 1700/6774. Loss: 0.3934969902038574\n",
            "\tЭпоха 8. Итерация 1750/6774. Loss: 0.19731509685516357\n",
            "\tЭпоха 8. Итерация 1800/6774. Loss: 0.24939796328544617\n",
            "\tЭпоха 8. Итерация 1850/6774. Loss: 0.24801769852638245\n",
            "\tЭпоха 8. Итерация 1900/6774. Loss: 0.30377355217933655\n",
            "\tЭпоха 8. Итерация 1950/6774. Loss: 0.2903670072555542\n",
            "\tЭпоха 8. Итерация 2000/6774. Loss: 0.5217910408973694\n",
            "\tЭпоха 8. Итерация 2050/6774. Loss: 0.31292232871055603\n",
            "\tЭпоха 8. Итерация 2100/6774. Loss: 0.459179162979126\n",
            "\tЭпоха 8. Итерация 2150/6774. Loss: 0.45062363147735596\n",
            "\tЭпоха 8. Итерация 2200/6774. Loss: 0.17982986569404602\n",
            "\tЭпоха 8. Итерация 2250/6774. Loss: 0.1996675580739975\n",
            "\tЭпоха 8. Итерация 2300/6774. Loss: 0.5081531405448914\n",
            "\tЭпоха 8. Итерация 2350/6774. Loss: 0.29823675751686096\n",
            "\tЭпоха 8. Итерация 2400/6774. Loss: 0.21884621679782867\n",
            "\tЭпоха 8. Итерация 2450/6774. Loss: 0.28784453868865967\n",
            "\tЭпоха 8. Итерация 2500/6774. Loss: 0.4277581572532654\n",
            "\tЭпоха 8. Итерация 2550/6774. Loss: 0.2589273750782013\n",
            "\tЭпоха 8. Итерация 2600/6774. Loss: 0.19387686252593994\n",
            "\tЭпоха 8. Итерация 2650/6774. Loss: 0.49134665727615356\n",
            "\tЭпоха 8. Итерация 2700/6774. Loss: 0.5977230072021484\n",
            "\tЭпоха 8. Итерация 2750/6774. Loss: 0.20181424915790558\n",
            "\tЭпоха 8. Итерация 2800/6774. Loss: 0.27912914752960205\n",
            "\tЭпоха 8. Итерация 2850/6774. Loss: 0.3072664141654968\n",
            "\tЭпоха 8. Итерация 2900/6774. Loss: 0.3130740225315094\n",
            "\tЭпоха 8. Итерация 2950/6774. Loss: 0.38467520475387573\n",
            "\tЭпоха 8. Итерация 3000/6774. Loss: 0.26430559158325195\n",
            "\tЭпоха 8. Итерация 3050/6774. Loss: 0.5860268473625183\n",
            "\tЭпоха 8. Итерация 3100/6774. Loss: 0.21089062094688416\n",
            "\tЭпоха 8. Итерация 3150/6774. Loss: 0.30349117517471313\n",
            "\tЭпоха 8. Итерация 3200/6774. Loss: 0.3094622492790222\n",
            "\tЭпоха 8. Итерация 3250/6774. Loss: 0.3362242579460144\n",
            "\tЭпоха 8. Итерация 3300/6774. Loss: 0.5301047563552856\n",
            "\tЭпоха 8. Итерация 3350/6774. Loss: 0.6960862874984741\n",
            "\tЭпоха 8. Итерация 3400/6774. Loss: 0.2097790241241455\n",
            "\tЭпоха 8. Итерация 3450/6774. Loss: 0.1967444121837616\n",
            "\tЭпоха 8. Итерация 3500/6774. Loss: 0.31319713592529297\n",
            "\tЭпоха 8. Итерация 3550/6774. Loss: 0.4644884765148163\n",
            "\tЭпоха 8. Итерация 3600/6774. Loss: 0.30415982007980347\n",
            "\tЭпоха 8. Итерация 3650/6774. Loss: 0.3519026041030884\n",
            "\tЭпоха 8. Итерация 3700/6774. Loss: 0.4554121792316437\n",
            "\tЭпоха 8. Итерация 3750/6774. Loss: 0.2998788058757782\n",
            "\tЭпоха 8. Итерация 3800/6774. Loss: 0.25544261932373047\n",
            "\tЭпоха 8. Итерация 3850/6774. Loss: 0.2370937019586563\n",
            "\tЭпоха 8. Итерация 3900/6774. Loss: 0.25204354524612427\n",
            "\tЭпоха 8. Итерация 3950/6774. Loss: 0.2651786208152771\n",
            "\tЭпоха 8. Итерация 4000/6774. Loss: 0.19850938022136688\n",
            "\tЭпоха 8. Итерация 4050/6774. Loss: 0.44091349840164185\n",
            "\tЭпоха 8. Итерация 4100/6774. Loss: 0.3394954204559326\n",
            "\tЭпоха 8. Итерация 4150/6774. Loss: 0.2067784070968628\n",
            "\tЭпоха 8. Итерация 4200/6774. Loss: 0.2964450716972351\n",
            "\tЭпоха 8. Итерация 4250/6774. Loss: 0.2212892323732376\n",
            "\tЭпоха 8. Итерация 4300/6774. Loss: 0.4205445349216461\n",
            "\tЭпоха 8. Итерация 4350/6774. Loss: 0.4032455086708069\n",
            "\tЭпоха 8. Итерация 4400/6774. Loss: 0.2948364317417145\n",
            "\tЭпоха 8. Итерация 4450/6774. Loss: 0.29760637879371643\n",
            "\tЭпоха 8. Итерация 4500/6774. Loss: 0.47210386395454407\n",
            "\tЭпоха 8. Итерация 4550/6774. Loss: 0.2712062895298004\n",
            "\tЭпоха 8. Итерация 4600/6774. Loss: 0.26130977272987366\n",
            "\tЭпоха 8. Итерация 4650/6774. Loss: 0.23108811676502228\n",
            "\tЭпоха 8. Итерация 4700/6774. Loss: 0.34335097670555115\n",
            "\tЭпоха 8. Итерация 4750/6774. Loss: 0.16381704807281494\n",
            "\tЭпоха 8. Итерация 4800/6774. Loss: 0.38141942024230957\n",
            "\tЭпоха 8. Итерация 4850/6774. Loss: 0.3889533579349518\n",
            "\tЭпоха 8. Итерация 4900/6774. Loss: 0.3853711187839508\n",
            "\tЭпоха 8. Итерация 4950/6774. Loss: 0.16779330372810364\n",
            "\tЭпоха 8. Итерация 5000/6774. Loss: 0.5735883712768555\n",
            "\tЭпоха 8. Итерация 5050/6774. Loss: 0.5618215203285217\n",
            "\tЭпоха 8. Итерация 5100/6774. Loss: 0.198208287358284\n",
            "\tЭпоха 8. Итерация 5150/6774. Loss: 0.3206581473350525\n",
            "\tЭпоха 8. Итерация 5200/6774. Loss: 0.46944084763526917\n",
            "\tЭпоха 8. Итерация 5250/6774. Loss: 0.22152890264987946\n",
            "\tЭпоха 8. Итерация 5300/6774. Loss: 0.46255868673324585\n",
            "\tЭпоха 8. Итерация 5350/6774. Loss: 0.38694360852241516\n",
            "\tЭпоха 8. Итерация 5400/6774. Loss: 0.23398636281490326\n",
            "\tЭпоха 8. Итерация 5450/6774. Loss: 0.29497647285461426\n",
            "\tЭпоха 8. Итерация 5500/6774. Loss: 0.3175090253353119\n",
            "\tЭпоха 8. Итерация 5550/6774. Loss: 0.25300222635269165\n",
            "\tЭпоха 8. Итерация 5600/6774. Loss: 0.2322303056716919\n",
            "\tЭпоха 8. Итерация 5650/6774. Loss: 0.4266231656074524\n",
            "\tЭпоха 8. Итерация 5700/6774. Loss: 0.1495191752910614\n",
            "\tЭпоха 8. Итерация 5750/6774. Loss: 0.2162950038909912\n",
            "\tЭпоха 8. Итерация 5800/6774. Loss: 0.22368420660495758\n",
            "\tЭпоха 8. Итерация 5850/6774. Loss: 0.2924843430519104\n",
            "\tЭпоха 8. Итерация 5900/6774. Loss: 0.2578950524330139\n",
            "\tЭпоха 8. Итерация 5950/6774. Loss: 0.27602845430374146\n",
            "\tЭпоха 8. Итерация 6000/6774. Loss: 0.3554413318634033\n",
            "\tЭпоха 8. Итерация 6050/6774. Loss: 0.5716952681541443\n",
            "\tЭпоха 8. Итерация 6100/6774. Loss: 0.527855634689331\n",
            "\tЭпоха 8. Итерация 6150/6774. Loss: 0.3757840394973755\n",
            "\tЭпоха 8. Итерация 6200/6774. Loss: 0.17700381577014923\n",
            "\tЭпоха 8. Итерация 6250/6774. Loss: 0.5280628204345703\n",
            "\tЭпоха 8. Итерация 6300/6774. Loss: 0.479316383600235\n",
            "\tЭпоха 8. Итерация 6350/6774. Loss: 0.40291649103164673\n",
            "\tЭпоха 8. Итерация 6400/6774. Loss: 0.1268724799156189\n",
            "\tЭпоха 8. Итерация 6450/6774. Loss: 0.28423625230789185\n",
            "\tЭпоха 8. Итерация 6500/6774. Loss: 0.33360555768013\n",
            "\tЭпоха 8. Итерация 6550/6774. Loss: 0.16676783561706543\n",
            "\tЭпоха 8. Итерация 6600/6774. Loss: 0.3572913110256195\n",
            "\tЭпоха 8. Итерация 6650/6774. Loss: 0.24917347729206085\n",
            "\tЭпоха 8. Итерация 6700/6774. Loss: 0.2877543270587921\n",
            "\tЭпоха 8. Итерация 6750/6774. Loss: 0.23832951486110687\n",
            "Эпоха #8 train_loss: 0.042460512706871456, val_loss: 0.04190236104726791\n",
            "Потрачено 49.1 минут на 8 эпоху\n",
            "\tЭпоха 9. Итерация 0/6774. Loss: 0.35998788475990295\n",
            "\tЭпоха 9. Итерация 50/6774. Loss: 0.28076156973838806\n",
            "\tЭпоха 9. Итерация 100/6774. Loss: 0.36414435505867004\n",
            "\tЭпоха 9. Итерация 150/6774. Loss: 0.25380459427833557\n",
            "\tЭпоха 9. Итерация 200/6774. Loss: 0.45265084505081177\n",
            "\tЭпоха 9. Итерация 250/6774. Loss: 0.2573263943195343\n",
            "\tЭпоха 9. Итерация 300/6774. Loss: 0.17451035976409912\n",
            "\tЭпоха 9. Итерация 350/6774. Loss: 0.3219403922557831\n",
            "\tЭпоха 9. Итерация 400/6774. Loss: 0.23690439760684967\n",
            "\tЭпоха 9. Итерация 450/6774. Loss: 0.3823149800300598\n",
            "\tЭпоха 9. Итерация 500/6774. Loss: 0.15386809408664703\n",
            "\tЭпоха 9. Итерация 550/6774. Loss: 0.3035529851913452\n",
            "\tЭпоха 9. Итерация 600/6774. Loss: 0.3509402275085449\n",
            "\tЭпоха 9. Итерация 650/6774. Loss: 0.27245160937309265\n",
            "\tЭпоха 9. Итерация 700/6774. Loss: 0.5526943206787109\n",
            "\tЭпоха 9. Итерация 750/6774. Loss: 0.20630109310150146\n",
            "\tЭпоха 9. Итерация 800/6774. Loss: 0.35723549127578735\n",
            "\tЭпоха 9. Итерация 850/6774. Loss: 0.6268380284309387\n",
            "\tЭпоха 9. Итерация 900/6774. Loss: 0.2861962616443634\n",
            "\tЭпоха 9. Итерация 950/6774. Loss: 0.516858696937561\n",
            "\tЭпоха 9. Итерация 1000/6774. Loss: 0.37698623538017273\n",
            "\tЭпоха 9. Итерация 1050/6774. Loss: 0.3601531386375427\n",
            "\tЭпоха 9. Итерация 1100/6774. Loss: 0.3149510324001312\n",
            "\tЭпоха 9. Итерация 1150/6774. Loss: 0.2642801105976105\n",
            "\tЭпоха 9. Итерация 1200/6774. Loss: 0.3406507670879364\n",
            "\tЭпоха 9. Итерация 1250/6774. Loss: 0.5823288559913635\n",
            "\tЭпоха 9. Итерация 1300/6774. Loss: 0.15252655744552612\n",
            "\tЭпоха 9. Итерация 1350/6774. Loss: 0.31300872564315796\n",
            "\tЭпоха 9. Итерация 1400/6774. Loss: 0.3177851140499115\n",
            "\tЭпоха 9. Итерация 1450/6774. Loss: 0.3899063766002655\n",
            "\tЭпоха 9. Итерация 1500/6774. Loss: 0.27790671586990356\n",
            "\tЭпоха 9. Итерация 1550/6774. Loss: 0.34411606192588806\n",
            "\tЭпоха 9. Итерация 1600/6774. Loss: 0.49255144596099854\n",
            "\tЭпоха 9. Итерация 1650/6774. Loss: 0.21597205102443695\n",
            "\tЭпоха 9. Итерация 1700/6774. Loss: 0.3244514465332031\n",
            "\tЭпоха 9. Итерация 1750/6774. Loss: 0.3004091680049896\n",
            "\tЭпоха 9. Итерация 1800/6774. Loss: 0.21613629162311554\n",
            "\tЭпоха 9. Итерация 1850/6774. Loss: 0.33720114827156067\n",
            "\tЭпоха 9. Итерация 1900/6774. Loss: 0.2870434522628784\n",
            "\tЭпоха 9. Итерация 1950/6774. Loss: 0.43351536989212036\n",
            "\tЭпоха 9. Итерация 2000/6774. Loss: 0.22777576744556427\n",
            "\tЭпоха 9. Итерация 2050/6774. Loss: 0.481184184551239\n",
            "\tЭпоха 9. Итерация 2100/6774. Loss: 0.39619970321655273\n",
            "\tЭпоха 9. Итерация 2150/6774. Loss: 0.13947178423404694\n",
            "\tЭпоха 9. Итерация 2200/6774. Loss: 0.19485503435134888\n",
            "\tЭпоха 9. Итерация 2250/6774. Loss: 0.3987400531768799\n",
            "\tЭпоха 9. Итерация 2300/6774. Loss: 0.30692967772483826\n",
            "\tЭпоха 9. Итерация 2350/6774. Loss: 0.5470045208930969\n",
            "\tЭпоха 9. Итерация 2400/6774. Loss: 0.3133790194988251\n",
            "\tЭпоха 9. Итерация 2450/6774. Loss: 0.42250046133995056\n",
            "\tЭпоха 9. Итерация 2500/6774. Loss: 0.2884996235370636\n",
            "\tЭпоха 9. Итерация 2550/6774. Loss: 0.3313884735107422\n",
            "\tЭпоха 9. Итерация 2600/6774. Loss: 0.21619801223278046\n",
            "\tЭпоха 9. Итерация 2650/6774. Loss: 0.32184216380119324\n",
            "\tЭпоха 9. Итерация 2700/6774. Loss: 0.2020002156496048\n",
            "\tЭпоха 9. Итерация 2750/6774. Loss: 0.3641761243343353\n",
            "\tЭпоха 9. Итерация 2800/6774. Loss: 0.35036784410476685\n",
            "\tЭпоха 9. Итерация 2850/6774. Loss: 0.37314024567604065\n",
            "\tЭпоха 9. Итерация 2900/6774. Loss: 0.3302936851978302\n",
            "\tЭпоха 9. Итерация 2950/6774. Loss: 0.43114620447158813\n",
            "\tЭпоха 9. Итерация 3000/6774. Loss: 0.2775346040725708\n",
            "\tЭпоха 9. Итерация 3050/6774. Loss: 0.299007773399353\n",
            "\tЭпоха 9. Итерация 3100/6774. Loss: 0.40716031193733215\n",
            "\tЭпоха 9. Итерация 3150/6774. Loss: 0.17501601576805115\n",
            "\tЭпоха 9. Итерация 3200/6774. Loss: 0.2890874147415161\n",
            "\tЭпоха 9. Итерация 3250/6774. Loss: 0.2779110074043274\n",
            "\tЭпоха 9. Итерация 3300/6774. Loss: 0.20858722925186157\n",
            "\tЭпоха 9. Итерация 3350/6774. Loss: 0.3238360285758972\n",
            "\tЭпоха 9. Итерация 3400/6774. Loss: 0.2689952850341797\n",
            "\tЭпоха 9. Итерация 3450/6774. Loss: 0.4786565899848938\n",
            "\tЭпоха 9. Итерация 3500/6774. Loss: 0.29750528931617737\n",
            "\tЭпоха 9. Итерация 3550/6774. Loss: 0.3759250044822693\n",
            "\tЭпоха 9. Итерация 3600/6774. Loss: 0.22327622771263123\n",
            "\tЭпоха 9. Итерация 3650/6774. Loss: 0.34839797019958496\n",
            "\tЭпоха 9. Итерация 3700/6774. Loss: 0.3823589086532593\n",
            "\tЭпоха 9. Итерация 3750/6774. Loss: 0.2641250491142273\n",
            "\tЭпоха 9. Итерация 3800/6774. Loss: 0.21845929324626923\n",
            "\tЭпоха 9. Итерация 3850/6774. Loss: 0.23222370445728302\n",
            "\tЭпоха 9. Итерация 3900/6774. Loss: 0.32654091715812683\n",
            "\tЭпоха 9. Итерация 3950/6774. Loss: 0.3160858154296875\n",
            "\tЭпоха 9. Итерация 4000/6774. Loss: 0.4110124707221985\n",
            "\tЭпоха 9. Итерация 4050/6774. Loss: 0.3294179141521454\n",
            "\tЭпоха 9. Итерация 4100/6774. Loss: 0.22256489098072052\n",
            "\tЭпоха 9. Итерация 4150/6774. Loss: 0.4413994252681732\n",
            "\tЭпоха 9. Итерация 4200/6774. Loss: 0.4940735995769501\n",
            "\tЭпоха 9. Итерация 4250/6774. Loss: 0.3730546534061432\n",
            "\tЭпоха 9. Итерация 4300/6774. Loss: 0.3392286002635956\n",
            "\tЭпоха 9. Итерация 4350/6774. Loss: 0.48646244406700134\n",
            "\tЭпоха 9. Итерация 4400/6774. Loss: 0.28456681966781616\n",
            "\tЭпоха 9. Итерация 4450/6774. Loss: 0.23954427242279053\n",
            "\tЭпоха 9. Итерация 4500/6774. Loss: 0.47004157304763794\n",
            "\tЭпоха 9. Итерация 4550/6774. Loss: 0.45033353567123413\n",
            "\tЭпоха 9. Итерация 4600/6774. Loss: 0.48263058066368103\n",
            "\tЭпоха 9. Итерация 4650/6774. Loss: 0.6118025183677673\n",
            "\tЭпоха 9. Итерация 4700/6774. Loss: 0.37011703848838806\n",
            "\tЭпоха 9. Итерация 4750/6774. Loss: 0.5519284009933472\n",
            "\tЭпоха 9. Итерация 4800/6774. Loss: 0.22373300790786743\n",
            "\tЭпоха 9. Итерация 4850/6774. Loss: 0.3843504786491394\n",
            "\tЭпоха 9. Итерация 4900/6774. Loss: 0.2646239399909973\n",
            "\tЭпоха 9. Итерация 4950/6774. Loss: 0.2199602872133255\n",
            "\tЭпоха 9. Итерация 5000/6774. Loss: 0.2671646177768707\n",
            "\tЭпоха 9. Итерация 5050/6774. Loss: 0.49198415875434875\n",
            "\tЭпоха 9. Итерация 5100/6774. Loss: 0.20201468467712402\n",
            "\tЭпоха 9. Итерация 5150/6774. Loss: 0.18883995711803436\n",
            "\tЭпоха 9. Итерация 5200/6774. Loss: 0.3475421369075775\n",
            "\tЭпоха 9. Итерация 5250/6774. Loss: 0.26428258419036865\n",
            "\tЭпоха 9. Итерация 5300/6774. Loss: 0.3904266059398651\n",
            "\tЭпоха 9. Итерация 5350/6774. Loss: 0.21149009466171265\n",
            "\tЭпоха 9. Итерация 5400/6774. Loss: 0.22221027314662933\n",
            "\tЭпоха 9. Итерация 5450/6774. Loss: 0.37807801365852356\n",
            "\tЭпоха 9. Итерация 5500/6774. Loss: 0.33879372477531433\n",
            "\tЭпоха 9. Итерация 5550/6774. Loss: 0.28139910101890564\n",
            "\tЭпоха 9. Итерация 5600/6774. Loss: 0.39998242259025574\n",
            "\tЭпоха 9. Итерация 5650/6774. Loss: 0.3076038062572479\n",
            "\tЭпоха 9. Итерация 5700/6774. Loss: 0.484745055437088\n",
            "\tЭпоха 9. Итерация 5750/6774. Loss: 0.2945946455001831\n",
            "\tЭпоха 9. Итерация 5800/6774. Loss: 0.3160901665687561\n",
            "\tЭпоха 9. Итерация 5850/6774. Loss: 0.371288537979126\n",
            "\tЭпоха 9. Итерация 5900/6774. Loss: 0.5453469157218933\n",
            "\tЭпоха 9. Итерация 5950/6774. Loss: 0.31279024481773376\n",
            "\tЭпоха 9. Итерация 6000/6774. Loss: 0.2428692728281021\n",
            "\tЭпоха 9. Итерация 6050/6774. Loss: 0.3138088285923004\n",
            "\tЭпоха 9. Итерация 6100/6774. Loss: 0.5456010103225708\n",
            "\tЭпоха 9. Итерация 6150/6774. Loss: 0.44459742307662964\n",
            "\tЭпоха 9. Итерация 6200/6774. Loss: 0.2638959586620331\n",
            "\tЭпоха 9. Итерация 6250/6774. Loss: 0.1713700145483017\n",
            "\tЭпоха 9. Итерация 6300/6774. Loss: 0.32451125979423523\n",
            "\tЭпоха 9. Итерация 6350/6774. Loss: 0.40607956051826477\n",
            "\tЭпоха 9. Итерация 6400/6774. Loss: 0.34293532371520996\n",
            "\tЭпоха 9. Итерация 6450/6774. Loss: 0.4134407341480255\n",
            "\tЭпоха 9. Итерация 6500/6774. Loss: 0.1880922168493271\n",
            "\tЭпоха 9. Итерация 6550/6774. Loss: 0.4122134745121002\n",
            "\tЭпоха 9. Итерация 6600/6774. Loss: 0.4416220188140869\n",
            "\tЭпоха 9. Итерация 6650/6774. Loss: 0.4661807119846344\n",
            "\tЭпоха 9. Итерация 6700/6774. Loss: 0.39660677313804626\n",
            "\tЭпоха 9. Итерация 6750/6774. Loss: 0.3210393488407135\n",
            "Эпоха #9 train_loss: 0.04243292869874747, val_loss: 0.040583115349709986\n",
            "Потрачено 49.3 минут на 9 эпоху\n"
          ]
        }
      ],
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "try:\n",
        "    for epoch in range(n_epochs):\n",
        "        start = time.time()\n",
        "        train_loss = train(train_data_loader, epoch)\n",
        "        val_loss = val(val_data_loader, epoch)\n",
        "        #lr_scheduler.step()\n",
        "        print(f\"Эпоха #{epoch} train_loss: {train_loss}, val_loss: {val_loss}\")  \n",
        "        end = time.time()\n",
        "        print(f\"Потрачено {round((end - start) / 60, 1)} минут на {epoch} эпоху\")\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
        "            'losses_train': train_losses,\n",
        "            'losses_val': val_losses\n",
        "            }, os.path.join(dataset_path, f'../checkpoints/model_detector_resnet50_augmented_{epoch}.pth'))\n",
        "    torch.save({\n",
        "    'epoch': epoch,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
        "    'losses_train': train_losses,\n",
        "    'losses_val': val_losses\n",
        "    }, os.path.join(dataset_path, f'../checkpoints/model_detector_resnet50_augmented_full.pth'))\n",
        "except KeyboardInterrupt:\n",
        "    print('Прервано пользователем')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92r83slt8HWu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYImJAxe8HWu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "RLL0C1078HWu",
        "outputId": "d92014cd-dea2-4e7e-ee4f-d54968066738"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEGCAYAAACzYDhlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU1fnH8c+ThYQ9EAIhLBIhgIAJYERRQBBFVAJqUUHburUUi0vtT/25dLHW9ldb26p1r0utVZGiKK6gIIILQlBA9n0JBAgBwpY9z++PcwMhBkhgJncmed6vV16ZOXPvnWfygnxzzz33HFFVjDHGmGCK8LsAY4wxdZ+FjTHGmKCzsDHGGBN0FjbGGGOCzsLGGGNM0EX5XUAoatWqlXbq1MnvMowxJqwsWLBgp6omVPWahU0VOnXqRGZmpt9lGGNMWBGRjUd7zbrRjDHGBJ2FjTHGmKCzsDHGGBN0ds3GGGMCoLi4mKysLAoKCvwuJehiY2Np37490dHR1d7HwsYYYwIgKyuLpk2b0qlTJ0TE73KCRlXJzc0lKyuL5OTkau9n3WjGGBMABQUFxMfH1+mgARAR4uPja3wGZ2FjjDEBUteDptyJfE4LmwDauief//tgOTv21f0+W2OMqQkLmwA6UFjCs7PX8eF32/wuxRhTz+Tm5tK7d2969+5NYmIi7dq1O/S8qKjomPtmZmZy2223BbU+GyAQQCltmtI9sSnvLtrKded08rscY0w9Eh8fz8KFCwF44IEHaNKkCXfeeeeh10tKSoiKqvpXfnp6Ounp6UGtz85sAiwjLYnMjbvZsiff71KMMfXc9ddfz/jx4znrrLO4++67mTdvHv3796dPnz6cc845rFy5EoBZs2YxYsQIwAXVjTfeyODBgzn11FN5/PHHA1KLndkE2IjUtvxl2kreX7yVcYM6+12OMcYHv3t3Kcu27g3oMXskNeO3GT1rvF9WVhZffvklkZGR7N27lzlz5hAVFcUnn3zCfffdx5tvvvm9fVasWMGnn37Kvn376NatGzfffHON7qmpioVNgJ0S35jU9s15b3G2hY0xxndXXnklkZGRAOTl5XHdddexevVqRITi4uIq97n00kuJiYkhJiaG1q1bs337dtq3b39SdQQ1bERkOPAYEAk8r6p/qvR6DPBv4AwgF7haVTd4r90L3ASUArep6jSvPQ54HugFKHCjqn4lIm8A3bxDxwF7VLW3iHQClgMrvdfmqur4oHxgT0ZqEn/4YDkbdh6gU6vGwXwrY0wIOpEzkGBp3Pjw76Bf//rXDBkyhClTprBhwwYGDx5c5T4xMTGHHkdGRlJSUnLSdQTtmo2IRAJPAhcDPYCxItKj0mY3AbtVtQvwd+Bhb98ewBigJzAceMo7Hrjw+khVuwNpuCBBVa9W1d6q2ht4E3irwvusLX8t2EEDcGlqWwDeW7w12G9ljDHVlpeXR7t27QD417/+VavvHcwBAv2ANaq6TlWLgInAqErbjAJe9h5PBoaKu1toFDBRVQtVdT2wBugnIs2BQcALAKpapKp7Kh7Q2/8q4PUgfa7jSopryJmdWvDuomy/SjDGmO+5++67uffee+nTp09AzlZqIpjdaO2AzRWeZwFnHW0bVS0RkTwg3mufW2nfdkA+kAO8JCJpwALgdlU9UGHbgcB2VV1doS1ZRL4F9gK/UtU5J/vhjicjLYnfvLOUldv20S2xabDfzhhjDnnggQeqbO/fvz+rVq069Pyhhx4CYPDgwYe61Crvu2TJkoDUFG5Dn6OAvsDTqtoHOADcU2mbsRx5VpMNdPS2/yXwmog0q3xgERknIpkikpmTk3PShV7cqy0RYl1pxhgDwQ2bLUCHCs/be21VbiMiUUBz3ECBo+2bBWSp6tde+2Rc+FDhGFcAb5S3eV1xud7jBcBaoGvlYlX1OVVNV9X0hIQql9CukYSmMZzTuRXvLtqKqp708YwxJpwFM2zmAykikiwiDXAX/KdW2mYqcJ33eDQwU91v5qnAGBGJEZFkIAWYp6rbgM0iUj7qbCiwrMLxLgBWqGpWeYOIJJQPLhCRU71jrQvkBz2aEalt2ZB7kKUBHm9vjDHhJmhho6olwC3ANNyIsUmqulREHhSRkd5mLwDxIrIG18V1j7fvUmASLkg+Aiaoaqm3z63AqyKyGOgN/LHC247h+wMDBgGLRWQh7kxovKruCuynrdrwXolERQjvLrKuNGNM/SbWxfN96enpmpmZGZBj3fiv+azcto85dw8hIqJ+TD9uTH20fPlyTjvtNL/LqDVVfV4RWaCqVU6yFm4DBMJORlpbtuzJ59vNu/0uxRhjfGNhE2QXnNaGmKgIu+fGGBNUQ4YMYdq0aUe0Pfroo9x8881Vbj948GAC1YNTHRY2QdY0Nprzu7fmvcXZlJZZl6UxJjjGjh3LxIkTj2ibOHEiY8eO9amiI1nY1IKMtCR27i/k63W5fpdijKmjRo8ezfvvv39oobQNGzawdetWXn/9ddLT0+nZsye//e1vfavPZn2uBUO6taZxg0jeXbyVc7q08rscY0ywfXgPbPsusMdMPB0u/tNRX27ZsiX9+vXjww8/ZNSoUUycOJGrrrqK++67j5YtW1JaWsrQoUNZvHgxqampga2tGuzMphY0bBDJBT3a8OGSbRSXlvldjjGmjqrYlVbehTZp0iT69u1Lnz59WLp0KcuWLTvOUYLDzmxqSUZqEu8s3Mrna3YypFtrv8sxxgTTMc5AgmnUqFHccccdfPPNNxw8eJCWLVvyyCOPMH/+fFq0aMH1119PQUGBL7XZmU0tGdi1Fc1io+wGT2NM0DRp0oQhQ4Zw4403MnbsWPbu3Uvjxo1p3rw527dv58MPP/StNjuzqSUxUZEM75XIB99to6C4lNjoyOPvZIwxNTR27Fguv/xyJk6cSPfu3enTpw/du3enQ4cOnHvuub7VZWFTizLSkpiUmcWslTkM75XodznGmDrosssuO2Ly36MtkjZr1qzaKchj3Wi1qP+p8cQ3bsC7tuyAMaaesbCpRVGREVxyeltmLN/OgcLaXSXPGGP8ZGFTyzLSkigoLmPGih1+l2KMCbD6MrHxiXxOC5taln5KCxKbxdqoNGPqmNjYWHJzc+t84Kgqubm5xMbG1mg/GyBQyyIihEtT2/LKVxvJyy+mecNov0syxgRA+/btycrKIhDLyoe62NhY2rdvX6N9LGx8kJGWxAufr2f60m1cmd7h+DsYY0JedHQ0ycnJfpcRsqwbzQdp7ZvToWVD3l1syw4YY+oHCxsfiAgZqUl8sWYnufsL/S7HGGOCzsLGJxlpSZSWKR8u2eZ3KcYYE3QWNj7pntiULq2b2Kg0Y0y9ENSwEZHhIrJSRNaIyD1VvB4jIm94r38tIp0qvHav175SRC6q0B4nIpNFZIWILBeR/l77AyKyRUQWel+XHO9YfhIRRqS2Zd6GXWzf688srMYYU1uCFjYiEgk8CVwM9ADGikiPSpvdBOxW1S7A34GHvX17AGOAnsBw4CnveACPAR+pancgDVhe4Xh/V9Xe3tcH1TiWr0akJqEK79tAAWNMHRfMM5t+wBpVXaeqRcBEYFSlbUYBL3uPJwNDRUS89omqWqiq64E1QD8RaQ4MAl4AUNUiVd1znDqqPFYAPt9J69K6CT3aNrO50owxdV4ww6YdsLnC8yyvrcptVLUEyAPij7FvMpADvCQi34rI8yLSuMJ2t4jIYhF5UURa1KAORGSciGSKSGZt3pSVkZbEt5v2sHnXwVp7T2OMqW3hNkAgCugLPK2qfYADQPm1oKeBzkBvIBv4a00OrKrPqWq6qqYnJCQEsORjG5HaFoD3rCvNGFOHBTNstgAVb49v77VVuY2IRAHNgdxj7JsFZKnq1177ZFz4oKrbVbVUVcuAf3K4q6w6dfimQ8tG9OkYZ6PSjDF1WjDDZj6QIiLJItIAd5F+aqVtpgLXeY9HAzPVzWI3FRjjjVZLBlKAeaq6DdgsIt28fYYCywBEpG2F414OLKnwHt87ViA/6MnKSE1iWfZe1ubs97sUY4wJiqCFjXcN5hZgGm7E2CRVXSoiD4rISG+zF4B4EVkD/BKvS0xVlwKTcEHyETBBVUu9fW4FXhWRxbgusz967X8Wke+89iHAHdU4Vki4NLUtIvDeIutKM8bUTVLXp8M+Eenp6ZqZmVmr73n1s1+xc38hn/zyPNyAPGOMCS8iskBV06t6LdwGCNRZGWlJrM05wIpt+/wuxRhjAs7CJkRc3CuRyAixgQLGmDrJwiZExDeJ4dwurXh38dY6v9KfMab+sbAJIRmpbdm8K59FWXl+l2KMMQFlYRNChvVMpEFkhHWlGWPqHAubENK8YTTndUvg/cXZlJVZV5oxpu6wsAkxGWlJbNtbQObG3X6XYowxAWNhE2KGdm9NbLR1pRlj6hYLmxDTOCaKoae14YPvsikpLfO7HGOMCQgLmxCUkZpE7oEivlqX63cpxhgTEBY2IWhwtwSaxERZV5oxps6wsAlBsdGRDOvZho+WbKOwJKTmDDXGmBNiYROiMtKS2FtQwpxVO/0uxRhjTpqFTYga0KUVcY2ieW+xdaUZY8KfhU2Iio6M4OJeiXy8bDv5RdaVZowJbxY2ISwjNYkDRaV8unKH36UYY8xJsbAJYWedGk+rJjE2Ks0YE/YsbEJYZIQwIrUtM1fsYF9Bsd/lGGPMCbOwCXEZaW0pLCnjk+Xb/S7FGGNOmIVNiOvToQXt4hry7qJsv0sxxpgTFtSwEZHhIrJSRNaIyD1VvB4jIm94r38tIp0qvHav175SRC6q0B4nIpNFZIWILBeR/l77X7y2xSIyRUTivPZOIpIvIgu9r2eC+ZkDLcLrSpu9Koc9B4v8LscYY05I0MJGRCKBJ4GLgR7AWBHpUWmzm4DdqtoF+DvwsLdvD2AM0BMYDjzlHQ/gMeAjVe0OpAHLvfaPgV6qmgqsAu6t8D5rVbW39zU+wB816DLSkigpU6Yt3eZ3KcYYc0KCeWbTD1ijqutUtQiYCIyqtM0o4GXv8WRgqIiI1z5RVQtVdT2wBugnIs2BQcALAKpapKp7vMfTVbXEO9ZcoH0QP1ut6pnUjE7xjawrzRgTtoIZNu2AzRWeZ3ltVW7jBUUeEH+MfZOBHOAlEflWRJ4XkcZVvPeNwIcVnid7238mIgOrKlZExolIpohk5uTkVPtD1gYRISMtiS/X7iRnX6Hf5RhjTI2F2wCBKKAv8LSq9gEOAEdcCxKR+4ES4FWvKRvo6G3/S+A1EWlW+cCq+pyqpqtqekJCQjA/wwnJSEuiTOHDJXZ2Y4wJP8EMmy1AhwrP23ttVW4jIlFAcyD3GPtmAVmq+rXXPhkXPnjHuB4YAVyrqgrgdcXleo8XAGuBrif/8WpX1zZN6damqd3gaYwJS8EMm/lAiogki0gD3AX/qZW2mQpc5z0eDcz0QmIqMMYbrZYMpADzVHUbsFlEunn7DAWWgRv5BtwNjFTVg+VvICIJ5YMLRORU71jrAv9xgy8jrS3zN+xm6558v0sxxpgaCVrYeNdgbgGm4UaMTVLVpSLyoIiM9DZ7AYgXkTW4Lq57vH2XApNwQfIRMEFVy2ejvBV4VUQWA72BP3rtTwBNgY8rDXEeBCwWkYW4M6HxqrorWJ87mEakJgHw/mLrSjPGhBfxeptMBenp6ZqZmel3GVUa+cTnCPDOLQP8LsUYY44gIgtUNb2q18JtgEC9l5GaxKKsPDbmHvC7FGOMqTYLmzBzaWpbAN6zrjRjTBixsAkzSXENST+lhY1KM8aEFQubMJSRlsSKbftYtX2f36UYY0y1WNiEoYtPTyRC4D07uzHGhAkLmzDUumks/TvH8+7ibGw0oTEmHFjYhKmM1CTW7zzA0q17/S7FGGOOy8ImTA3vlUhUhPDuYutKM8aEPgubMBXXqAEDU1rx3iLrSjPGhD4LmzCWkZbElj35fLNpj9+lGGPMMVnYhLELe7ShQVSE3XNjjAl5FjZhrGlsNOd3a83732VTWmZdacaY0GVhE+Yy0pLI2VfI1+tz/S7FGGOOysImzJ3fvTWNGkTy7iKbK80YE7osbMJcwwaRXNijDR8tyaa4tMzvcowxpkoWNnVARmoSuw8W88WanX6XYowxVbKwqQMGdm1F09go60ozxoQsC5s6ICYqkuE9E5m+dBsFxaXH38EYY2qZhU0dkZGWxL7CEj5bleN3KcYY8z3VChsRaSwiEd7jriIyUkSig1uaqYlzOsfTsnEDu8HTGBOSqntmMxuIFZF2wHTgR8C/jreTiAwXkZUiskZE7qni9RgRecN7/WsR6VThtXu99pUiclGF9jgRmSwiK0RkuYj099pbisjHIrLa+97CaxcRedw71mIR6VvNzxxWoiIjuOT0RGYs38HBohK/yzHGmCNUN2xEVQ8CVwBPqeqVQM9j7iASCTwJXAz0AMaKSI9Km90E7FbVLsDfgYe9fXsAY7z3GA485R0P4DHgI1XtDqQBy732e4AZqpoCzPCe471/ivc1Dni6mp857GSkJpFfXMony3f4XYoxxhyh2mHjnUFcC7zvtUUeY3uAfsAaVV2nqkXARGBUpW1GAS97jycDQ0VEvPaJqlqoquuBNUA/EWkODAJeAFDVIlXdU8WxXgYuq9D+b3XmAnEi0raanzusnNmpJW2axdgKnsaYkFPdsPkFcC8wRVWXisipwKfH2acdsLnC8yyvrcptVLUEyAPij7FvMpADvCQi34rI8yLS2NumjaqWj/3dBrSpQR2IyDgRyRSRzJyc8LzIHhEhjEhNYtbKHPYWFPtdjjHGHFKtsFHVz1R1pKo+7A0U2KmqtwW5tqpEAX2Bp1W1D3CAw91lh6hb4KVGM1Oq6nOqmq6q6QkJCQEp1g8jUttSVFrG9KXb/S7FGGMOqe5otNdEpJl3FrEEWCYidx1nty1AhwrP23ttVW4jIlFAcyD3GPtmAVmq+rXXPhkXPgDby7vHvO/lFy6qU0ed0btDHO1bNLRRacaYkFLdbrQeqroXdx3kQ1x31o+Os898IEVEkkWkAe6C/9RK20wFrvMejwZmemclU4Ex3mi1ZNzF/Xmqug3YLCLdvH2GAsuqONZ1wDsV2n/sjUo7G8ir0N1W54gIGWlJfL5mJ/+cvY7CErvJ0xjjv6hqbhft3VdzGfCEqhaLyDG7qVS1RERuAabhBhO86F3veRDIVNWpuAv9r4jIGmAXLpDwtpuEC5ISYIKqlv/WvBV41QuwdcANXvufgEkichOwEbjKa/8AuAQ3yOBghe3rrJ8MSGZ59l7+8MFyXpm7kXsv7s7wXom4sRfGGFP7pDrr14vIbcD/AouAS4GOwH9UdWBwy/NHenq6ZmZm+l3GSftsVQ5/fH85K7fv48xOLfjVpT1I6xDnd1nGmDpKRBaoanqVr1UnbI5y0ChvBFmdU1fCBqCktIz/Lsjir9NXsnN/EZf1TuKu4d1pF9fQ79KMMXXMscKmugMEmovI38qHBovIX4HGx93R+C4qMoKx/Try6Z2DmTCkMx8s2cb5j8zikWkr2V9YJ/9WMMaEoOoOEHgR2Ie7DnIVsBd4KVhFmcBrGhvNXRd1Z+b/nMfwXok88ekaBv9lFhPnbaK07MTObo0xprqqe81moar2Pl5bXVGXutGO5ttNu3no/eUs2Lib7olNuf/S0xiYEr73Fxlj/HfS3WhAvogMqHDAc4H8QBRn/NGnYwsmj+/PU9f25UBRCT96YR43vDSPNTv2+V2aMaYOqu6ZTRrwb9xNlwC7getUdXEQa/NNfTizqaiwpJSXv9zAP2as4WBxKdf068gvLkghvkmM36UZY8JIwEajiUgzAFXdKyK/UNVHA1RjSKlvYVMud38hj81Yzatfb6JRdCS3nN+F687pRGz08eZcNcaY4A193qSqHU+qshBVX8Om3Jod+/jjByuYuWIHHVo25J7hp3HJ6XZTqDHm2AJxzabK457EviaEdWndlBevP5P/3HQWjRtEMeG1bxj9zFd8u2m336UZY8LUyYSNjZet4waktOL92wby8A9OZ9Oug1z+1Jfc9vq3ZO0+6Hdpxpgwc8xuNBHZR9WhIkBDVa3u3Gphpb53o1Vlf2EJz362ludmr0Nx86/dPLgzTWOj/S7NGBMignLNpi6zsDm6rXvyeWTaSt76dgutmjTglxd246r09kRFnsxJsjGmLgjWNRtTDyXFNeRvV/dm6i3ncmqrJtw35TsueXwOn60Kz9VNjTG1w8LGnJDU9nG88bOzeeaHfSksKeO6F+dx3YvzWLXdbgo1xnyfhY05YSLC8F5tmX7HIH516Wl8u2k3wx+dzf1TvmPn/kK/yzPGhBALG3PSYqIi+cnAU/nsriH8uH8n3pi/mcF/mcVTs9ZQUGwrhRpjbIBAlWyAwMlZm7Of//tgBZ8s305C0xhGpLZlVO92pLVvbjeGGlOH2Wi0GrKwCYwv1+7k5S838OmKHIpKyzglvhEZqUmM6p1ESpumfpdnjAkwC5sasrAJrLz8YqYt3ca7i7byxZqdlCl0T2zKyN5JZKQm0aFlI79LNMYEgG9hIyLDgceASOB5Vf1TpddjcLNJnwHkAler6gbvtXuBm4BS4DZVnea1b8At5FYKlJR/MBF5A+jmHToO2KOqvUWkE7AcWOm9NldVxx+rbgub4Nmxr4APFmczddFWvtm0B4AzTmnByLQkLjm9LQlNbaZpY8KVL2EjIpHAKuBCIAuYD4xV1WUVtvk5kKqq40VkDHC5ql4tIj2A14F+QBLwCdBVVUu9sElX1Z3HeO+/Anmq+qAXNu+paq/q1m5hUzs27zrI1EVbeXfRVlZs20eEwLldWjEyLYmLeiXSzGYnMCasHCtsgjndTD9gjaqu84qYCIwCllXYZhTwgPd4MvCEuCvIo4CJqloIrBeRNd7xvjrem3r7XwWcH6DPYYKkQ8tGTBjShQlDurBy2z6mLtrC1EVbuWvyYu5/ewnnd2vNyN5JnN+9tS1zYEyYC2bYtAM2V3ieBZx1tG1UtURE8oB4r31upX3beY8VmC4iCjyrqs9VOuZAYLuqrq7Qliwi3wJ7gV+p6pzKxYrIOGAcQMeOdXLlhJDWLbEpdyV2585h3fh28x6mLtzKe4uz+WjpNprERDGsRxtG9k7i3C6tiLapcYwJO+E4keYAVd0iIq2Bj0VkharOrvD6WFwXXLlsoKOq5orIGcDbItJTVfdWPKgXWs+B60YL8mcwRyEi9O3Ygr4dW/DrET34am0uUxdt4cMl23jr2y20bNyAS05PZFTvdpzRsQURETaU2phwEMyw2QJ0qPC8vddW1TZZIhKFW3Y691j7qmr59x0iMgXXvTYbwDvGFbgBB3jbFQKF3uMFIrIW6ArYRZkQFxkhDEhpxYCUVvz+sl58tjKHdxZtZfKCLP4zdxNJzWPJSEsiIy2JnknN7B4eY0JYMMNmPpAiIsm4oBgDXFNpm6nAdbhrMaOBmaqqIjIVeE1E/oYbIJACzBORxkCEqu7zHg8DHqxwvAuAFaqaVd4gIgnALm9wwanesdYF4fOaIIqJimRYz0SG9Uxkf2EJnyzbzjsLt/DC5+t5dvY6Oic0ZmRaO0b2TiK5VWO/yzXGVBK0sPGuwdwCTMMNfX5RVZeKyINApqpOBV4AXvEGAOzCBRLedpNwgwlKgAleWLQBpnh/wUYBr6nqRxXedgxHdqEBDAIeFJFioAwYr6q7gvSxTS1oEhPFZX3acVmfduw6UMSHS7KZunArj85Yxd8/WUVq++aMTEtiRGoSic1j/S7XGIPd1FklG/ocnrLz8nlvkbuH57steYhAv04tGdW7HRf3SqRF4wZ+l2hMnWYzCNSQhU34W5ezn6mLtjJ10VbW5RwgQqBfckuG9Ujkwh5tbNYCY4LAwqaGLGzqDlVl6da9fLRkGx8v285Kb72d09o2Y1iPNgzr2YYebW1wgTGBYGFTQxY2ddeGnQf4eNl2pi/bRubG3ahCu7iGDOvZhmE9EjmzUwtb4tqYE2RhU0MWNvXDzv2FzFi+nelLtzNnzU6KSsqIaxTN0O7ujGdQSgING9SxmQtKCuHzR6H3WIizm5dNYFnY1JCFTf1zoLCE2aty+HjZdmas2EFefjExUREMTElgWM82DO3emvgmdWCS0K+ehGn3QbdLYGzlgZvGnBwLmxqysKnfikvLmL9+F9OXbWf60m1szSsgQiC9U0t3nadHIh3jw3CAQf4eeLw3lBZD0X64cTp0rDyDlDEnzsKmhixsTLnyAQbTl25j+rLtrNjmBhh0T2zqbjLt0SZ8Zi/4+LfwxWNw40fwxo+gVQpc/z6EQ+0mLFjY1JCFjTmaTbkHmb7MBU/mhl2UeQMMLuzRhmE92nBmcsvQnCg0Lwse7ws9L4crnoV5/4QP7oRr34SUC2qtjNIyZeuefNbk7CdnbyF9OsbRpXWT8Ahrc1wWNjV0UmGjan8p1hO5+wuZsWKHG2CwOofCkjKaN4xmaPfWboBB1wQaNQiRuW7f/jl8NxluzXQDA0qK4Il0iG0G42ZDRGAD8kBhCet3HmBtzn7W7tjP2hz3eN3OAxSVlB2xbdvmsQxMacWgrgkM6NKKuEZ28224srCpoRMOmx0rYMo4+MELrovC1BsHi0qYs3on05duZ8aK7ew5WD7AoBUX9mjD0NPa0MqvAQbblsAzA+CcW2HY7w+3L54Eb/0URr8IvX5Q48OqKtv3FrpAqRQq2XkFh7aLEOjYshGdE5pwakJjOic0oXPrJrRo1ID5G3Yxe1UOn6/Zyb6CEkQgtX0c53nh07tDnA1FDyMWNjV0wmGTtwWeHQhNEuEnn0CDMLyIbE5aSWkZ8zfsdt1tS7ezZU8+IpB+Sgu6JzajRaNo4ho1IK5RNC0qfW8WGx34ZRP+8wPIyoTbF0LDFofby8pcCJXkw4R5EFn1yqgFxaVszD1YIVDcGcraHfs5UFR6aLsmMVF0TmjMqQlN6FwhVE6Jb0RM1LGHkJeUlrEoK4/Zq3KYszqHhZv3UKbQNCaKc7rEM6hrAoNSEmzmhxBnYVNDJ9WNtvoTePUH0OeHMOrJwBZmwo6qsix7rxtSvXwHm3cfJC+/mKP9t4sQaN6wUhh5z11IlaCYzNsAAB1GSURBVD9u4D122zRqEFn1dY91s+Dfo2DYQ+7MprKVH8HrV6MjHmVX92sOd3flHD5L2bzrIGUV6k1qHkvn1k1cmFQIldZNYwJ27SXvYDFfrN3JnNU5zF61ky178gFIbtXYdbmlJNC/czyNY0Kkm9IAFjY1dtIDBGb8HuY8Apc9426eM/5SBS2DiNC4QbOsTNlbUMzug8XsOVjEnoPF7Pa+7zlYxG7veV6++777QDF5+cXsLyw56jEbREbQvFH04bOmhtG0bBjF7evH0aRsLx+e9y7NmjalRaNomsRGkb2nwDtT2ceNq26mZXE2Awv+TiHueklMVATJrRp/L1ROTWhc69ehVJW1OQe84Mlh7rpd5BeXEh3pFtob1DWB87om0KNtM1tMz2cWNjV00mFTWuL+mtz6Dfx0JrQ+LXDFmZrJ3w2vXAExTeBHb4dM4JyIopIy9uQXkXew+HAged+rCq5++2fw+9JHub3o57xTNqDKY7ZqEsOlzdfzu9w7mdflFxw8cwKdE5rQLq5hyP7iLiwpZcGG3Xy2Ooc5q3ayLNstuhvfuAEDvLOegSmtaN3MlpeobRY2NRSQoc/7trn+8IYtXeDENAlMcab6CvfBK5e76xUoXPxnOOtnfldVO0oK4R/paMM48m+YwZ780kMhtDe/mDbNY+ncqgnNG3nXaf4zGrLmw+2LoGGcv7XX0I59BXy+eidzVrtut537iwB3L9R5XRMYmJJAeqcWxEaH7x8a4cLCpoYCdp/Nulnw78sg9Wq4/BkbEl2bivPh1Sth45dw1cuQ+RJsmgsT5taPOcG+fAKm3+/O5joPOf722Yvd4JaBd8LQXwe/viApK3PXyOas3snsVTlkbtxFcakSGx3BWcnxXpdbKzon2L09wWBhU0MBvalz1p9g1v/ByH9A3x8H5pjm2EqK4I0fwurpcPmzkHY17NkET54Np/SHayfX7eDP3w2P9YZ2Z8CP3qr+fpNvhJUfwm0LoWmb4NVXiw4UlvD1+lxmr9rJ7NU5rMs5ALhBDgNTEhjU1Z31NIuNJjY6wgLoJFnY1FBAw6asFP5zhfur+iczILFXYI5rqlZWCm/eBEunwIi/Q/qNh1/7+ln48G4vgMb4V2Owffwb+OJxGD8HEk+v/n65a+HJfnDGDXDpI8Grz0ebdx081N1Wfm9PORFoGB1JowaRNGwQSaPoKBrFeM+jo2jU4PBrjRtEuW0OtUXRKDrS2z7K26f89ah6E2QWNjUU8Olq9u+AZwa66zbjZkFM08Ad2xxWVgZTb4WF/4ELfw/n3lbp9VJ4cTjkroYJ86FJgj91BtOezfCPM6DXFa7rtqbe/QV8+x+4ZT60TA58fSHE3duzh6Vb93KgsJT8ohIOFpVysLiUg4XucX5xqWsrcq8fKColv6iUg0UlRwwHPx4RaBTthdJRQiuuYTSJzRvStnksbZvHkhTXkNbNYo57j1IosbCpoaDMjbbhc3g5w81N9YMX6nY3jh9U4cP/hXnPwnn/C0Puq3q7HSvctYnTMtyd83XNlPGw5C24dQHEdaj5/nuz3czQPUbBFc8Fvr46QlUpLClzwVMhnFxAVXhcVMqBohIvoA6H1sFDzw+H2q4DRUecaZVr1STmUAC1bR5L27jyQHLf2zSLpUFUaMyycKywCeqAeREZDjwGRALPq+qfKr0eA/wbOAPIBa5W1Q3ea/cCNwGlwG2qOs1r3wDs89pLyj+YiDwA/BTI8Q5/n6p+cKxj1apOA2DI/TDz93DKOXDmT2q9hDpt5u9d0Jw9AQbfe/TtWnd3F8Fn/RFOvxK6XVx7NQbbtu9g0UR3RnciQQPQrC2cNd7NDn3u7dCmZ2BrrCNEhNjoSGKjI2lx/M2r7UBhCdl5BWTn5ZO9p+Dw47wCNuQe4Kt1ud8LJJHKgdSwUii5QPJ7gtigndmISCSwCrgQyALmA2NVdVmFbX4OpKrqeBEZA1yuqleLSA/gdaAfkAR8AnRV1VIvbNJVdWel93sA2K+qj1RqP+qxjlZ70GZ9LiuD166E9bPhpo8hqXfg36M+mvM3mPE7NwAj4/HjnzWWFMFz57n1XSbMhdjmtVNnsL1yhbu367aFJzd8OX83PJrm/ii6ZmLg6jMBsb+whOw9+UcEUfaeArL3Fhxqr3wDsAgkHAqkhrSN+34wtW4ac9KB5NeZTT9gjaqu84qYCIwCllXYZhTwgPd4MvCEuKtoo4CJqloIrBeRNd7xvjqBOgJ5rJMTEQGXP+e6cf57Hfxsdt35ReeXr59zQdNrNIx4tHrdk1ENYOQT8MIF8MkDbiBBuFs7E9bOgGF/OPn7ZBq2gAG3w4wHYdPXtsBaiGkSE0VKm6aktDn6td99BcVeGLkA2ppXwDYvmNbk7GfO6pwj5rUDN1VSQtMYRqQm8esRPQJedzDDph2wucLzLKDyv9pD26hqiYjkAfFe+9xK+7bzHiswXUQUeFZVK3Ys3yIiPwYygf9R1d3HOdYhIjIOGAfQsWMQ78NoHA+jX4J/XQLvTICrXrHrNyfq21fhw7vcEseXP1Oz2QHanwFn3Qxzn3RB1enc4NUZbGVlbmG0uI7Q76eBOeZZ42HuMy7IbYG1sNM0NpqmsdF0PUogqSr7CkvI3lPA1rx8tuUdPitqF9cwKDWFxlWlmhmgqn2Bi4EJIjLIa38a6Az0BrKBv9bkoKr6nKqmq2p6QkKQRyl1PAuG/haWv+uG45qaWzoFpt4Cyee58D7KjMXHdP79EHeKG8FWnB/4GmvLksmwbTGc/xuICtAyBg0aw3l3w8YvYM2MwBzThAwRoVlsNN0SmzKkW2vG9uvIL4d14y9XpnHjgOCMQgxm2GwBKl6lbO+1VbmNiEQBzXEDBY66r6qWf98BTMF1iaGq21W1VFXLgH+Wt1ezjtp3zq3Q9WKY/ivIWuB3NeFl1XR486fQ/kwY+zpEn+AcWA0aQ8ZjsGstfPZwYGusLcUFbuLXtmkntCbNMfW9zoXxjAfc2ZMxJyGYYTMfSBGRZBFpAIwBplbaZipwnfd4NDBT3YiFqcAYEYkRkWQgBZgnIo1FpCmAiDQGhgFLvOdtKxz38vL2ox0rwJ+15kTgsqegaVv47/VwcJffFYWH9XNg0o/c5KbXTHKBcTI6D4HeP3Q3QWYvCkyNtWn+PyFvE1z4YMBX2ySqAZz/KzfKbWkNZiIwpgpBCxtVLQFuAaYBy4FJqrpURB4UkZHeZi8A8d5F+18C93j7LgUm4QYTfARM8EaPtQE+F5FFuMB4X1U/8o71ZxH5TkQWA0OAO45zLP81aglX/gv2ZbvrN3bP07FlZcLrY9xf2z+aErgJIy96CBrFwzu3uBm7w0X+bpj9CHS5AE4dHJz36DUaWveET/8ApcXBeQ9TL9hNnVUI2tDno5n7NHx0z9EXuDJuaeN/XeoC5oaP3P0ggbTsHZj0Y7jgARhwR2CPHSzTfw1f/gPGfx7caZC8Bda+N/2PMZUca+hzOA4QqHvOGu/uaP/4t26oqTnSztXwymUQ3Qh+PDXwQQPujvnuI9zEqblrA3/8QNuzyQ0uSRsb/Pn2ul4EHc6Cz/4MRQeD+16mzrKwCQUibgnpuA4w+QY4kOt3RaFj90a3EJ0q/PgdaHFK8N7rkkcgMgam3hb6F8Rn/sF9P//+4L+XiDvj25cN82wKG3NiLGxCRWxzuPJlOJADU8aF/i+72rBvmwuaov3w47choWtw369ZWxj2e9j4OXzzcnDf62RkL4bFb8DZN0Pz9rXznqecAynD4PO/u5kXjKkhC5tQktQbhv8frPkEvqgDd7WfjAO5Lmj274Br36zZVPkno++PodNAN03/3q2185419clv3bWr2r62dP6voWAPfPl47b6vqRMsbEJN+k3Q8wqY+ZCbKbo+KsiD/1wOuze4ubk6nFl77y0CIx93I6/e/5/QGyG4ZoabmmbQXbW/fHPbVDc6be7TsG977b63CXsWNqFGxN1o2CIZJt/k/rKvT4oOwKtXwfalcNW/IXnQ8fcJtJanuiUKVn7gZioIFRWnpfFr1vAh90FpEcz+iz/vb8KWhU0oim0GV73suize+qlb9Ks+KC6AiddC1jy44p9uFJRfzv45JPVxK3uGyg23302C7d+5qY4CNS1NTcV3dl2NC16CXev9qcGEJQubUJV4Olz8Z1g3y924V9eVFsPkG2HdpzDyH26lST9FRrmZofN3w7SjLMRWm4oLXNdq296um9VPg+6GiGiY9X/+1mHCioVNKOv7Y0i92v2nXjfL72qCp6wM3v45rHzfBWyfH/pdkZPYC879BSx63Q3a8NO85yBvc3CmpampZm3hrJ/B4kmuu9OYarCwCWUicOnfoFVXePMnbihwXaMK7//SdRGd/2v3SyyUDLoL4lPg3TugcL8/NRzcBXMegS4Xwqnn+VNDZQN+ATHN3CSgxlSDhU2oi2nirt8UHXADBsJp7q7jUXWzXi94yQ3jHXSn3xV9X3QsjHrCnVXMfMifGub8FQr2woW/8+f9q1K+wNqqD2HT3ONvb+o9C5tw0Po0uPSv7mbDutRP/tnD8NUT0G+cu+gdqjqe7UZ/ff0MbK7lCcN3b3RdaL2vhTY9a/e9j+es8dC4NXzyu9AbIm5CjoVNuOh9jbuWMeev/l8/CIQvn3DB2ftaGP5w6K8EecFvoVk7t9BaSWHtve+nfwCJcEOOQ035Amubvqwb/yZNUFnYhJOL/+LOct4aB3n+r/92wjJfgun3u8kvMx73/4J3dcQ0dbMe56yAOX+rnffMXlRhWprvrWQeGg4tsPY7m2LJHFMY/C83hzRo5OZPKy5ww4TDcX2RxZPgvTvcPFtXPO+GGIeLrsPg9Kvc2eX2ZcF/v49/Cw1bhvaSB7bAmqkmC5twk9DVzTCweS7MDLORQMvfgynjodMANztAVAO/K6q54X9yN91OvTW4N9uumeHuOTrvbjdJayjrNRra9HIDKMLxDyBTKyxswlHqlXDGDfDFY25hq3CwdqZbPiGpD4x9HaIb+l3RiWkc764xbcl068kEQ1mpNy3NKeGxWFlEhBu2vns9fPuK39WYEGVhE66G/8nNMjDlZ24hrVC28St4/Rp3v9APJ7vrH+Hs9NGQcpE7s9y9IfDHX1w+Lc1v/JuWpqa6XgQdzoZZD9sCa6ZKFjbhKjrWXb8pK4X/3gAlRX5XVLWt38JrV7kL3D+a4u7PCHciMOJvIJHw7u2BHfZbPi1NUh//p6WpCRE3Ym//NltgzVTJwiacxXd2NxxuyYRPHvC7msOKC2DbEvcX+itXQGycW2WzSWu/Kwuc5u3dL9d1s2Dha4E77rxnYW9WaExLU1O2wJo5hqAOBRKR4cBjQCTwvKr+qdLrMcC/gTOAXOBqVd3gvXYvcBNQCtymqtO89g3APq+9RFXTvfa/ABlAEbAWuEFV94hIJ2A5sNJ727mqOj44n9gHPS+DjeNg7pNwSn84LaP23rtwH+xcBTkrD3/tXOm6ltQbBtusnVtls7ZWlKxN6TfBkjfdRJ0pF558mB7cBbP/6n5h+7G0QiCc/2t4dqBbYG3ob/yuxoSQoIWNiEQCTwIXAlnAfBGZqqoVx4zeBOxW1S4iMgZ4GLhaRHoAY4CeQBLwiYh0VdXy4T9DVHVnpbf8GLhXVUtE5GHgXuB/vdfWqmrvYHzOkDDsIciaD29PcKOCWiYH9vgHcl2IVAyUnJWwt8K9PhHREN/FXUfqNRoSurmvVl3D57pDTUVEuBmqnz4XPrjLTSt0Mub8FYr2wQUhNC1NTVVcYK3fz6BpG78rMjWhCsUH3Q27ARbMM5t+wBpVXQcgIhOBUUDFsBkFPOA9ngw8ISLitU9U1UJgvYis8Y731dHeTFWnV3g6FxgdoM8R+qJi4Mp/wbOD4L/Xw03Ta/4LXhX2ZbubFnNWHRkuByvkenQjaJXihi+36uqFSndo0QkiowP4ocJEqxQ3PHnm793Q7tNGnNhxDk1Lcw206RHYGmvbkPtg2dtugbVL68HyGOFM1f2f3/A5bPwCNnzh7icb9WTA3yqYYdMO2FzheRZw1tG28c5I8oB4r31upX3Lb6FWYLqIKPCsqlZ1NfJG4I0Kz5NF5FtgL/ArVZ1TeQcRGQeMA+jYsWO1PmBIadEJRj0Fb1wL0+4/+n/yslLYs9EFSs6Kw91gO1dB4d7D28U2dyHS7WL3vfwspXmH8LuWEGzn3g5L33bLSHcacGLLNc98yE1LMzgEp6WpqYoLrPWfEPgzbXPiyspgx1IXKhs/h41fwsFc91qzdnDqYDe7eBCE0e3bhwxQ1S0i0hr4WERWqOrs8hdF5H6gBHjVa8oGOqpqroicAbwtIj1VdW/Fg3qh9RxAenp6eM4qeNoI6H+Lm9yyQz9ITK0QKN4ZS+5qKCk4vE+TNi5EUq+u0PXVzV1/CPX5ykJFZDSMfByeHwof/8Y9romtC90SCwN+GbrT0tTUoLth4evw6R/hB//0u5r6q6zUze6w8Qvv7OVLtwIwuOXFUy6CTufCKee6P1iD+H8+mGGzBehQ4Xl7r62qbbJEJApojhsocNR9VbX8+w4RmYLrXpsNICLXAyOAoapuPKrXFVfoPV4gImuBrkBmoD5oSLngAdj8tVtOuqK4ji5ETj3vcKAkdK0bQ5FDQbu+Lui/fNzdh1PdC/yqLqAatnRrxNQV5QusffGYO/NL7OV3RfVDaYmbU2/j5+7sZdNcKMxzr7VIdn+QnjLABUxc7fbgBDNs5gMpIpKMC4oxwDWVtpkKXIe7FjMamKmqKiJTgddE5G+4AQIpwDwRaQxEqOo+7/Ew4EE4NPLtbuA8VT10V5mIJAC7VLVURE71jrUuaJ/ab5HRcNUr7k7uuFO8YEkJygU/U8nge2H5uzD1Nrj5SzeX3fGsnQHrP3OzEoT6tDQ1NeAXritt5u/hmjeOv72pudJidy/bhs/d1+avochb5C8+BXpdfjhcmiX5WmrQwsa7BnMLMA039PlFVV0qIg8Cmao6FXgBeMUbALALF0h4203CDSYoASZ4YdEGmOLGEBAFvKaq5fO1PAHE4LrW4PAQ50HAgyJSDJQB41V1V7A+d0ho1tZdtDa1q0Ej14X2coZbPmHYceauK5+WpkWn8JiWpqYatnBnNTMedH9hdzzb74rCX0khbFlw+JrL5nlu9Bi4a6upV7vrhqecG3IjAUVt0aPvSU9P18zMutnLZmrB1NvcmeVPZ7qZAI5m4Wvw9s0w+kXo9YPaq682FR2Ax/tAy85wwwd2HbCmivMhK/PwNZes+Yevubbp5UKl/JpL41b+1gqIyILyex8rC8cBAsaEtgsfhFXT4J1bYdynVQ8JL873pqXpCz0ur/0aa0uDxjDoLvjgTrfAWkpwRjrVGUUH3NlK+TDkLZlQWgSIu4ct/SYXLh37Q6OWfldbIxY2xgRawzi3jPcb17oL5IPu/P42Xz/rboq9/Nm6P5S873VuhOQnv4POQ+v+562J4gJ3nWX9Z+7MZcsCKCtx8+61TXODLE4Z4LogT2RIfQixsDEmGE4b4VYi/ezP7nurlMOvHdzlVvtMuQiSB/pXY22JagBD7ncjJJe+5Ubr1VelJe6C/vrP3Nemr6G00IVL+YjGTgOh41nhPzt6JRY2xgTLxX+BdZ+5hdau/+DwX/SzH/GmpXnAz+pqV6/R7ixv5kMufOvLbBOqsGMZrJ/t/i1s/OLwzdNtToczf+JuRzjlnDoXLpVZ2BgTLE3bwEV/hHd+DgtedL9Ydm/wpqW5NvynpamJ8gXWXr/aDZ6oi6Pvyu1a7525zHZfB3Jce8tT3UCQ5EHuKwQu6NcmCxtjgqn3NfDdf90Q567D3V/2EVFu/rD6puICa6ljqncfUjjYt90Lllnue/lihk0SofP5kHyeC5e4Dsc8TF1nYWNMMIlAxqPwVH+YeI27u3vg//h+g50vyhdYe+lit27PgDv8rujE5O9x3WHrvOsuOStce2xzd73lnNtcuLTqakO9K7CwMSbYWnRyXUjT7oVG8e5Gx/qq4gJrZ1wfHtMlFee7m1LXf+YCJnuhW68pqqFbQyptrLvukpgKEZF+VxuyLGyMqQ1n/cwt29Dlgro3LU1NDf0NPDMAvnjcnemEmtJi2PKN1zX2mRuaXFrkuj/bn+nuG0o+D9qn1921moLAwsaY2hARCRmP+V1FaChfYO+rJ90ZQ1SM+4ps4H2PccOlo2IrtHnfq2qLjKniGN7+lduq6tYqn3a/4oixov0cupGy3zg39X7H/hDTpJZ/WHWHhY0xpvZd+CCUFbtVYIv2uzVVSovc3F8lhe7ek5Ii9720KHDvG1lFiBXkQb43XWJ8Fze/WPmIsTC7Sz+UWdgYY2pf83Zw1b+rt62qF0QFhwOopLCKtgqvVQ6sKtu8/aNj3dxiyefVnfWEQpCFjTEmtIkc7iYzYcsmKTLGGBN0FjbGGGOCzsLGGGNM0FnYGGOMCToLG2OMMUFnYWOMMSboLGyMMcYEnYWNMcaYoBNV9buGkCMiOcDGkzhEK2BngMoJd/azOJL9PA6zn8WR6sLP4xRVTajqBQubIBCRTFVN97uOUGA/iyPZz+Mw+1kcqa7/PKwbzRhjTNBZ2BhjjAk6C5vgeM7vAkKI/SyOZD+Pw+xncaQ6/fOwazbGGGOCzs5sjDHGBJ2FjTHGmKCzsAkgERkuIitFZI2I3ON3PX4SkQ4i8qmILBORpSJyu981+U1EIkXkWxF5z+9a/CYicSIyWURWiMhyEenvd01+EpE7vP8nS0TkdRGJ9bumQLOwCRARiQSeBC4GegBjRaSHv1X5qgT4H1XtAZwNTKjnPw+A24HlfhcRIh4DPlLV7kAa9fjnIiLtgNuAdFXtBUQCY/ytKvAsbAKnH7BGVdepahEwERjlc02+UdVsVf3Ge7wP98uk3i7wLiLtgUuB5/2uxW8i0hwYBLwAoKpFqrrH36p8FwU0FJEooBGw1ed6As7CJnDaAZsrPM+iHv9yrUhEOgF9gK/9rcRXjwJ3A2V+FxICkoEc4CWvW/F5EWnsd1F+UdUtwCPAJiAbyFPV6f5WFXgWNiaoRKQJ8CbwC1Xd63c9fhCREcAOVV3gdy0hIgroCzytqn2AA0C9vcYpIi1wvSDJQBLQWER+6G9VgWdhEzhbgA4Vnrf32uotEYnGBc2rqvqW3/X46FxgpIhswHWvni8i//G3JF9lAVmqWn6mOxkXPvXVBcB6Vc1R1WLgLeAcn2sKOAubwJkPpIhIsog0wF3gm+pzTb4REcH1yS9X1b/5XY+fVPVeVW2vqp1w/y5mqmqd+8u1ulR1G7BZRLp5TUOBZT6W5LdNwNki0sj7fzOUOjhgIsrvAuoKVS0RkVuAabjRJC+q6lKfy/LTucCPgO9EZKHXdp+qfuBjTSZ03Aq86v1htg64wed6fKOqX4vIZOAb3CjOb6mDU9fYdDXGGGOCzrrRjDHGBJ2FjTHGmKCzsDHGGBN0FjbGGGOCzsLGGGNM0FnYGOMTESkVkYUVvgJ2F72IdBKRJYE6njEny+6zMcY/+ara2+8ijKkNdmZjTIgRkQ0i8mcR+U5E5olIF6+9k4jMFJHFIjJDRDp67W1EZIqILPK+yqc6iRSRf3rrpEwXkYa+fShT71nYGOOfhpW60a6u8Fqeqp4OPIGbMRrgH8DLqpoKvAo87rU/Dnymqmm4OcbKZ65IAZ5U1Z7AHuAHQf48xhyVzSBgjE9EZL+qNqmifQNwvqqu8yYz3aaq8SKyE2irqsVee7aqthKRHKC9qhZWOEYn4GNVTfGe/y8QraoPBf+TGfN9dmZjTGjSozyuicIKj0uxa7TGRxY2xoSmqyt8/8p7/CWHlwu+FpjjPZ4B3AxueXJvJUxjQor9pWOMfxpWmBEb4CNVLR/+3EJEFuPOTsZ6bbfiVre8C7fSZflMybcDz4nITbgzmJtxKz4aEzLsmo0xIca7ZpOuqjv9rsWYQLFuNGOMMUFnZzbGGGOCzs5sjDHGBJ2FjTHGmKCzsDHGGBN0FjbGGGOCzsLGGGNM0P0/mfS8Eq6UhoUAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(train_losses, label='Train')\n",
        "ax.plot(val_losses, label='Val')\n",
        "#ax.plot(val_losses, label='Val')\n",
        "ax.set(xlabel='Epoch', ylabel='Loss')\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pVD7IpU8HWu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "3b7e9cb8e453d6cda0fe8c8dd13f891a1f09162f0e7c66ffeae7751a7aecf00d"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "242586dd62d544a39c122c7b1ed6efc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_78cb1778f71b45b88ffdb20bfe0cd66e",
              "IPY_MODEL_6d83c78727204f529982e56c3bfe32c1",
              "IPY_MODEL_55191ea6a8eb4574aff8a9bc21fdd55a"
            ],
            "layout": "IPY_MODEL_ae7f4e349b634abd8b36184774421f7a"
          }
        },
        "4146164713eb4c2db70cf52335398a3e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55191ea6a8eb4574aff8a9bc21fdd55a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b17d80f4da05457781be26c19a1dca17",
            "placeholder": "​",
            "style": "IPY_MODEL_8e403bce502349d28da4553899f80c33",
            "value": " 160M/160M [00:00&lt;00:00, 331MB/s]"
          }
        },
        "6d83c78727204f529982e56c3bfe32c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4146164713eb4c2db70cf52335398a3e",
            "max": 167502836,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e66226838c994913b2d58fd0db3f4282",
            "value": 167502836
          }
        },
        "78cb1778f71b45b88ffdb20bfe0cd66e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e012c54a3a9f48b38c500c17227f67b8",
            "placeholder": "​",
            "style": "IPY_MODEL_91eb94ac4f6347c6b7d860b73e3742da",
            "value": "100%"
          }
        },
        "8e403bce502349d28da4553899f80c33": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91eb94ac4f6347c6b7d860b73e3742da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae7f4e349b634abd8b36184774421f7a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b17d80f4da05457781be26c19a1dca17": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e012c54a3a9f48b38c500c17227f67b8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e66226838c994913b2d58fd0db3f4282": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
